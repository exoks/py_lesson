#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£§‚£¶‚£¥‚£∂‚£æ‚£ø‚£∂‚£∂‚£∂‚£∂‚£¶‚£§‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢†‚°∂‚†ª‚†õ‚†ü‚†ã‚†â‚†Ä‚†à‚†§‚†¥‚†∂‚†∂‚¢æ‚£ø‚£ø‚£ø‚£∑‚£¶‚†Ñ‚†Ä‚†Ä‚†Ä                  ìêì  Pandas ìêî           
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚†î‚†ã‚†Ä‚†Ä‚†§‚†í‚†í‚¢≤‚†Ä‚†Ä‚†Ä‚¢Ä‚£†‚£§‚£§‚£¨‚£Ω‚£ø‚£ø‚£ø‚£∑‚£Ñ‚†Ä‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚£Ä‚£é‚¢§‚£∂‚£æ‚†Ö‚†Ä‚†Ä‚¢Ä‚°§‚†è‚†Ä‚†Ä‚†Ä‚††‚£Ñ‚£à‚°ô‚†ª‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚†Ä       Dev: oezzaou <oussama.ezzaou@gmail.com> 
#  ‚¢Ä‚†î‚†â‚†Ä‚†ä‚†ø‚†ø‚£ø‚†Ç‚††‚†¢‚£§‚†§‚£§‚£º‚£ø‚£∂‚£∂‚£§‚£ù‚£ª‚£∑‚£¶‚£ç‚°ª‚£ø‚£ø‚£ø‚£ø‚°Ä                                              
#  ‚¢æ‚£æ‚£Ü‚£§‚£§‚£Ñ‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†â‚¢ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á                                              
#  ‚†Ä‚†à‚¢ã‚¢π‚†ã‚†â‚†ô‚¢¶‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£º‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á       Created: 2025/06/20 12:43:36 by oezzaou
#  ‚†Ä‚†Ä‚†Ä‚†ë‚†Ä‚†Ä‚†Ä‚†à‚°á‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†á       Updated: 2025/06/23 00:24:47 by oezzaou
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°á‚†Ä‚†Ä‚¢Ä‚£æ‚£ø‚£ø‚†ø‚†ü‚†õ‚†ã‚†õ‚¢ø‚£ø‚£ø‚†ª‚£ø‚£ø‚£ø‚£ø‚°ø‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚†á‚†Ä‚¢†‚£ø‚£ü‚£≠‚£§‚£∂‚£¶‚£Ñ‚°Ä‚†Ä‚†Ä‚†à‚†ª‚†Ä‚†ò‚£ø‚£ø‚£ø‚†á‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†±‚†§‚†ä‚†Ä‚¢Ä‚£ø‚°ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚£ø‚†è‚†Ä‚†Ä                             ìÜ©‚ôïìÜ™      
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°Ñ‚†Ä‚†Ä‚†Ä‚†ò‚¢ß‚°Ä‚†Ä‚†Ä‚†∏‚£ø‚£ø‚£ø‚†ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ê‚†ã‚†Ä‚†Ä‚†Ä                     ìÑÇ oussama ezzaouìÜÉ  
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚†Ñ‚£Ä‚°Ä‚†∏‚†ì‚†Ä‚†Ä‚†Ä‚††‚†ü‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä                                              

# ===[ General Information: ]===================================================
* In 'Data Science before training a model', The data yypically goes through
  'serveral key stages' in a pipeline, These steps are crucial for ensuring the
  model receives 'clean', 'structured', and 'meaningful data':

  1|> [ Data Collection: ] 
      * 'Gather' data from different sources: 
        - Databases(SQL, NOSQL)
        - APIs
        - CSV/Excel Files
        - Web scraping
        - Sensors, logs, etc

  2|> [ Data Cleaning: ] 
      * 'Fix or remove, corrupted, duplicate, or missing data': 
        - Handle 'missing' values (e.g, file, drop, interpolate)
        - Remove 'duplicates'
        - Correct insconsistent formatting
        - Handle 'outliers' or 'noise'
        - 'Normalize' inconsistent units 

  3|> [ Data Transformation: (Preprocessing) ]
      * 'Convert data' into a 'suitable format':
        - 'Encoding': categorical varibales (e.g, one-hot, label encoding) 
        - 'Feature Scaling': normalization or standardization 

  4|> [ Exploratory Data Analysis (EDA) ] 
      * Analyze and visualize data to understand its structure
        - Histograms, boxplots, scatter plots
        - Correlation matrix
        - Summary statistics (mean, median, std) 
        - find patterns, trends, anomalies
  
  5|> [ Feature Engineering: ]
      * 'Create' new features or 'transform' existing onces to improve model
        performance:
        - Polynomial features 
        - Interaction terms
        - Log transforms
        - Aggregation (e.g, mean, sum per group)

  6|> [ Data Splitting: ]
      * Split data into subsets: 
        - 'Training set': to train the model
        - 'Validation set': to tune parameters (optional)
        - 'Test set': to evaluate model performance

===[ Summary: ]===

    Raw Data 
       ‚Üì
  Data Collection 
       ‚Üì
  Data Cleaning 
       ‚Üì
Preprocessing (Transformation & Encoding)
       ‚Üì
Exploratory Data Analysis (EDA)
       ‚Üì
  Feature Engineering 
       ‚Üì
  Train-Test Split 
       ‚Üì
  Model Training
================================================================================

===[ Pandas: ]==================================================================
* 'Pandas' is library 'built on top of Numpy', Pandas is optimized for 'data'
  'manipulation and analysis', especially 'tabular (row/column)' or
  'labeled data', Pandas is used:
    - Data cleaning and preprocessing
    - Data transformation and reshaping
    - Exploratory Data Analysis (EDA) 
    - Time series analysis
    - Importing/exporting real-world data (CSV, Excel, SQL, etc) 
    # NOTE:---------------------------------------------------------------------
    # - Pandas is not 'replacement' for Numpy, It 'extends' it.
    # - It brings 'structure', 'labels', and 'data-aware operations' that are
    #   crucial for real-world data analysis 
    # - 'ndarray' is great for 'nemerical computation'; Pandas is optimized for
    #   'data manipulation', 'cleaning', and exploration. 
  - The term 'Pandas' is term derived from 'Panel Data', a term used in
    'econometrices' to describe 'multidimensional structured data' 
      . 'Panel Data' = data that invovles 'observations over multiple time'
        'periods' for the same individuals (e.g time series + cross-sectional
         data)
      . The 'library name' reflects its focus on providing powerful and flexible 
        'data structures' for working with such 'complex structured datasets'
  # CONCLUSION:-----------------------------------------------------------------
  # while 'Pandas' might sound like the animal üêº, it actually has a technical
  # origin rooted in 'data analysis'.

# QUESTION:[ Why learn Pandas ? ]===============================================
# 1|> Higher-level Abstraction: Pandas provides an intuitive way to work with
#     structured data, eliminating the need for complex loops and index tracking
# 2|> Faster Development: Cleaner and shorter code for common data analysis
#     tasks
# 3|> Real-World Data Ready: Most real-word data is 'tabular', with missing
#     values, labels, and mixed data types. (Pandas handles this gracefully)
# 4|> Seamless Integration: Works well with other data science tools, (like
#     Numpy, Matplotlib, Scikit-learn, etc) 
#
#>>> [ Numpy VS Pandas ]
# ______________________________________________________________________________ 
# | Aspect            | NumPy                 | Pandas                         |
# |-------------------|-----------------------|--------------------------------|
# | Focus             | Numerical computation | Data manipulation and analysis |
# | Data Type Support | Homogeneous arrays    | Heterogeneous tables           |
# | Primary Structure | ndarray               | Series and DataFrame           |
# | Use Case          | Scientific computing  | Real-world data analysis       |
# |-Learning Need     |-Essential for         |-Essential for high-level data  | 
# |                   | low-level operations  | analysis                       | 
# |___________________|_______________________|________________________________|
#
# TIP:==========================================================================
# - Use NumPy for efficient computation and Pandas for efficient data handling.
#   In most data science projects, you'll use both, but Pandas is your go-to
#   for structured, real-world datasets.

===[ The Core Motivation behind Pandas ]===
- The 'main goal' of 'creating pandas' was to add 'labels and metadata' to 
  to numerical data structures (like NumPy arrays), enabling more 'flexible' and
  'human-readable' data analysis workflows.
=> 'Labels' (especially column lables in a 'DataFrame') typically represent
   'input feature' in machine learning and data analysis.
=> [ Terminology: ]
  > 'Columns' -> represent 'features' (with column labels)
  > 'Rows'    -> represent 'records/observations/samples' (with row labels
    or index) 

# QUESTION:[ Why Labels Matter? ]-----------------------------------------------
# 1|> Contextual Understanding
#     - Labels like column names ("age", "price") and row indices ("user_001",
#       "2023-01-01") provide semantic meaning to raw data.
# 2|> Ease of Access & Manipulation
#     - Instead of arr[0, 1], you can do df.loc["user_001", "price"].
#     - Enables powerful slicing, filtering, and grouping based on names, not
#       just positions.
# 3|> Automatic Alignment
#     - Labeled operations (e.g., combining datasets with different indices) are
#       handled automatically.
#     - NumPy can't align data across mismatched indices ‚Äî Pandas can.
# 4|> Missing Data Support
#     - Real-world datasets often have incomplete entries ‚Äî Pandas uses NaN,
#       .isna(), .fillna(), etc., to manage this elegantly.
# QUESTION:[ How Pandas Built on Numpy ?]---------------------------------------
# - Uses 'NumPy (ndarray) under the hood' for fast computation 
# - Wraps it with: 
#   . `Index`: to assign labels
#   . `Series`: labeled 1D array 
#   . `DataFrame`: labeled 2D table 
# - Adds high-level data manipulation capabilities.
# ------------------------------------------------------------------------------
# CONCLUSION:===================================================================
# - multiple Series (each representing an input feature) can be combined into a
#   DataFrame ‚Äî a 2D data matrix ‚Äî to visualize and analyze both the features
#   (columns) and the cases (rows) easily.
# - DataFrame object represent my data matrix or matrix table that i have learnt 
#   in 'math_statistics'.

===[ Series ]===
* A Series is essentially a 'one-dimensional labeled array' than can hold any data 
  type

--> [ Internal Components: (Attribues) ]
    - 'values': Stores the actual data as a 'NumPy''ndarray' (or sometimes
                'ExtensionArray', like for 'pd.StringDtype', 'Categorical', etc)
    - 'index': Stores 'labels' for each element (row labels). It is an
               'instance' of the 'Index' class. 
    - 'dtype': Stores the data type of the elements 

--> [ Examples: ]
    Import pandas as pd

    s = pd.Series([10, 20, 30], index=["a", "b", "c"])
    # values -> array([10, 20, 30]) 
    # index  -> Index(['a', 'b', c]) => Labels

===[ DataFrame: ]===
* A 'DataFrame' is a 'two-dimensional labeled data structure' (like a table or
  spreadsheet).

--> [ Internal Components: (Attributes) ]
    - '_data': Under the hood, stores data in a 'block manager' which manages
               a dictionary of 'ndarray' or 'ExtensionArray' columns grouped by
               dtype. 
    - 'columns': Labels for columns (an 'Index' object) 
    - 'index': 'Labels' for 'rows' (also an 'Index' object (samples, observation
               cases <term in statistics>)
    - Each column is a 'Series' with the same index

--> [ Example: ]
    Import pandas as pd

    df = pd.DataFrame({
      "feature1": ["oussama", "ezzaou"]
      "feature2": [25, 30]
    }, index=["case1", "case2"])

    #>>> Internally:
    # name -> Series(['oussama', 'ezzaou', dtype=object])
    # age -> Series([25, 30], dtype=int)
    #>>> Data Matrix (Matrix Table)
    #         _______________________
    #         | Feature1 | Feature2 |
    # |-------|----------|----------|
    # | case1 | oussama  |  25      | 
    # |-------|----------|----------|
    # | case2 | ezzaou   |  30      |
    # |_______|__________|__________|
