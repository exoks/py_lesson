#  ⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣤⣦⣴⣶⣾⣿⣶⣶⣶⣶⣦⣤⣄⠀⠀⠀⠀⠀⠀⠀                                              
#  ⠀⠀⠀⠀⠀⠀⠀⢠⡶⠻⠛⠟⠋⠉⠀⠈⠤⠴⠶⠶⢾⣿⣿⣿⣷⣦⠄⠀⠀⠀                𓐓  ML_model 𓐔           
#  ⠀⠀⠀⠀⠀⢀⠔⠋⠀⠀⠤⠒⠒⢲⠀⠀⠀⢀⣠⣤⣤⣬⣽⣿⣿⣿⣷⣄⠀⠀                                              
#  ⠀⠀⠀⣀⣎⢤⣶⣾⠅⠀⠀⢀⡤⠏⠀⠀⠀⠠⣄⣈⡙⠻⢿⣿⣿⣿⣿⣿⣦⠀       Dev:  oezzaou  oussama.ezzaou@gmail.com 
#  ⢀⠔⠉⠀⠊⠿⠿⣿⠂⠠⠢⣤⠤⣤⣼⣿⣶⣶⣤⣝⣻⣷⣦⣍⡻⣿⣿⣿⣿⡀                                              
#  ⢾⣾⣆⣤⣤⣄⡀⠀⠀⠀⠀⠀⠀⠀⠉⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇                                              
#  ⠀⠈⢋⢹⠋⠉⠙⢦⠀⠀⠀⠀⠀⠀⢀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇       Created: 2025/05/12 08:13:45 by oezzaou
#  ⠀⠀⠀⠑⠀⠀⠀⠈⡇⠀⠀⠀⠀⣠⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠇       Updated: 2025/06/14 19:44:13 by oezzaou
#  ⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⢀⣾⣿⣿⠿⠟⠛⠋⠛⢿⣿⣿⠻⣿⣿⣿⣿⡿⠀                                              
#  ⠀⠀⠀⠀⠀⠀⠀⢀⠇⠀⢠⣿⣟⣭⣤⣶⣦⣄⡀⠀⠀⠈⠻⠀⠘⣿⣿⣿⠇⠀                                              
#  ⠀⠀⠀⠀⠀⠱⠤⠊⠀⢀⣿⡿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠘⣿⠏⠀⠀                             𓆩♕𓆪      
#  ⠀⠀⠀⠀⠀⡄⠀⠀⠀⠘⢧⡀⠀⠀⠸⣿⣿⣿⠟⠀⠀⠀⠀⠀⠀⠐⠋⠀⠀⠀                     𓄂 oussama ezzaou𓆃  
#  ⠀⠀⠀⠀⠀⠘⠄⣀⡀⠸⠓⠀⠀⠀⠠⠟⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀                                              

===[ Model: Linear Regression ]=================================================
* Linear regression: (easy) is used to 'model the relationship' between: 
  - One independent variable (push_swap: Size)
  - One dependent variable (push_swap: Range)
    > It fits a 'stright' line through the 'data that best predict the output'.

  > [ Model Formula: ]

      f(x) = ax + b

    - The formula is called 'model' also in ML.
    - f(x): (y) is the target (y-hat is the estimated/predicted value)
    - x: Input feature 

    # NOTE:=====================================================================
    # - Training set is the data used for training the model.                  |
    # - The generted fuction is called sometimes 'hypothesis'.                 |
    # ==========================================================================

  > [ Example: ]
    - Predection of salary based on the experionce:
    _______________________________
    | Experience (x) | Salary (y) |
    |----------------|------------|
    | 1              |  30k       |
    | 2              |  35k       |
    | 3              |  40k       |
    |_4______________|__45k_______|

    * The model my learn: 

      Salary = 5000 * Experience + 25000 

    * if input feature = 5
      Salary predected = 500 * 5 + 25000 = 50000

    > [ What it does ?: ]
      - It taks a data set of (x, y) data points.
      - Find the 'line for the function that minimize the error' between
        'predicted and actual value'.
      - After training it can predict a 'y-hat' for any input feature 'x' 

# TIP:========================================================================== 
# - Curve function gives sometimes more predictive input (The same case that   |
#   i have faced) in push swap Where a I got a curve function between size     |
#   and range and i have choosen to work with linear function instead to       |
#   simplify the calculation.                                                  |
# - Improving means find the function (model) that gives target inputs (y-hat) |
#   'predective output' closer to the 'actual output'.                         | 
# - Improving the model involves changing the value of 'a (slop)' and          | 
#   'b y-intercept'. That is the same what i have done to improve my function  |
#   in push_swap (a, b: are called parametters/coefficients/weights).          |
# - But how can we measure and make sure that the optimized function is the    |
#   suitable one ? (cost function).                                            | 
# ==============================================================================

  > [ Cost Function: Squared error cost function ]
  - In machine learning a cost function is 'mathematical formula' that measures
    how wrong model's prediction are. It simply tells the model how 'far off'
    from the true value (it measures the quality of fiting).
      * Low Cost: good predictions
      * High Cost: bad predictions

      (MSQ): stands for Mean Squared Error
               |-----------------------------------------------------|
      Formula: | J(a, b) = 1 / 2n * sum (i -> n) (Ypi - Yai)^2       |
               |-----------------------------------------------------|

      # TIP: DO NOT forget to prove it mathimaticaly. 

      - Yai: is the actual/True output at 'i' 
      - Ypi: is the predictive output estemated by the model at 'i'. 
      - (Ypi - Yai) is called the 'error' or 'residual', distance
        between [actual/True output, predictive output], residual can
        be 'positive/negative'.
      - n: number of trained examples. 

# QUESTION:[ Why devided by 2n ?]=============================================== 
# - Dividing 'n' gives the **average** squared error across all training       |
#   examples truning it into a [ mean squared error ].                         |
# - Dividing by '2' is done for 'mathematical convenience', when you perform   |
#   'gradient descent' to minimize the cost function, you will compute its     |
#   derivative, The derivative of a squared term (e.g., (h − y)^2) brings      |
#   down a factor of 2, which cancels out with the 1 / 2, 'This simplifies the |
#   gradient update formula'.                                                  | 
# CONCLUSION:                                                                  |
# - 1/n → Makes the cost function an average per training example.             |
# - 1/2 → Simplifies the math when taking derivatives.                         |
# ==============================================================================

===[ Descent Gradient: ]===
* This algorithm is used to find the lowest point in the function. There are
  a lot of algorithms used to find the lowest point.

  1|_ find the solution of the equation f'(x) = 0 (where the slop at Xl is 0) 
  2|_ descent algorithm
  3|_ ...

- The cost function depends on the linear regression function, which depends on
  the slop (a) and y-intercept (b) => Cost function depends on the slop and
  y-intercept.

       J(a, b) = 1/2n sum(1 -> n) (Yp - Ya)^2 

    => [ J(a, b) = 1/2n sum(1 ->n) (ax + b - Ya)^2 ]

- J(a, b) is a multi-variable function, We will use the partial dirivative to
  track the rate of change (slop of J(a,b) at in respect of 'a' or 'b').
  > Initial derivative of J(a, b) in respect of a | range of change cost J(a,b)
    according to slop (a). 
  > Initial derivative of J(a, b) in repsect of b | range of change cost J(a,b)
    according to y-intercept (b).

# INFO:[ Descent Gradient Analogy ]=============================================
# - It is like moving blind you just make a small step and feel the steepness  |
#   then you move on.                                                          |
# ==============================================================================
- let's take a = 0 and b = 0

  1|> Solution:
    - we need to find the dirivative on 'a' and dirivative on 'b'. 
    - Let's take a random point in the graph the slop(a) and cost function(a,b)

        J(a,b)
          |
          |                   /
          |                  /
          |___________ /\   /
          |           /| \_/
          |          / |
          |         /  |
          |------------|-----------> (a)
                    <- Ai ->

  1|> The idea is to choose a point 'slop a(i)' as an initial value. 
  2|> Partial derivative gives me the 'rate of change' at a specific '(ai)'
  3|> Start moving by a step 'alpha' based on this form: 

     /|-> A(new) = A(i) - alpha * change of range at point A(i) 
  --||
     \|-> B(new) = B(i) - alpha * change of rate at point B(i)
    
     /|-> A(new) = A(i) - LR * (Partial Dirived at A(i) Point)  
  --||
     \|-> B(new) = A(i) - LR * (Partial Dirived at B(i) Point)

     /|-> A(new) = A(i) - LR * &/&x J(a,b)  
  --||
     \|-> B(new) = A(i) - LR * &/&y J(a,b)

     /|-> A(new) = A(i) - LR *  &/&x (sum(i=0->m) (Y-hat(i) - Ya)^2 / 2m)
  --||
     \|-> B(new) = A(i) - LR * &/&y (sum(i=0->m) (Y-hat(i) - Ya)^2 / 2m)

     /|-> A(new) = A(i) - LR *  (sum(i=0->m) (Y-hat(xi) - Ya)^2 / m) * xi
  --||
     \|-> B(new) = A(i) - LR * (sum(i=0->m) (Y-hat(xi) - Ya)^2 / m)


# INFO:=========================================================================
# - What we do exactly here is to move on the x Axis to find the point that has|
#   the partial dirivative at xi = 6xi/6x|/6yi/6y = slop = 0.                  |
# CONCLUSION:===================================================================
# - The gradient descent algorithm is about finding the min of the function    |
#   min(f(x)) where the slop is 0 (It is one of the most poplular algorithms). |
# ==============================================================================

- When you use all the training data to calculate the min(J(w,b)), In simple
  term using all training data at each step of gradient descent is called 
  'batch gradient descent'.

# QUESTION:[Why gradient descent for the best fitting instead of Pearons's (r) ]
# - Pearson’s r is useful for interpreting simple linear relationships,
#   but gradient descent is a flexible optimization tool for learning the best
#   parameters in more complex and high-dimensional models, which is essential
#   in modern machine learning.
# - But you can still use Pearson's r cofficient in the case of simple linear 
#   regression to find the equation of the best line that fits the data.

===[ ML: Multiple linear regression ]===========================================
* This model is used to model the relationship between one 'dependent' variable
  and 'two or more independent variables'|also called 'predictors' or 'features'

  > [ model formula: ]
      - single linear regression formula is:
        f(x) = ax + b
      - multiple linear regression formula would look like: 
        f(x, y, ....) = ax + by + cz ... + b
        f(x1, x2, x3 ...) = a1 * x1 + a2 * x2 + ... + an * xn + b
      
      . moving a unit on x
      . moving b unit on y
      . moving c unit on z
        ...

      x-> = |x| = |x1|         a-> = |a| = |a1| 
            |y|   |x2|               |b|   |a2|
            |z|   |x3|               |c|   |a3|
            ...   ...                   ...    ...
      
      # NOTE: a-> is a row vector while x-> is a column vector. 

        [ f(x->) = a-> . x-> + b ] 

  # NOTE: a-> . x-> : is called dot product, It is the projection of a-> on x->
  # ERROR:
  # - what i have said is not true at all 
  # - row vector is not a ghraphical vector, i guess no projection is here. 

  > [ Feature Scaling: ]
    * Feature scaling is 'preprocessing technique' used to 'normalize' the 
      range of independent variables (features) in a dataset

    - Normalization (min-max scaling), Standardization (Z-score Scaling) are a
      'common methods of feature scaling'. 

    # NOTE:=====================================================================
    # - Instead of working by the original values, we can work by normalization,
    #   standerdization forms Instead. but Why ?
    # - To keep the input feature values in the same range, (their ranges must
    #   be close to each others) 
    # - Of course there is nothing can make this transformation than
    #   Normalization (max, mean ...), Standardization (Z-score ...), ... 

  # INFO:
   - The Z-score normalization or standardization is the most method famous 
   method to make 'feature scaling'.

  > [ Feature Transformation methods: ]

  |--------------------------------[ Normalization ]-------------------------|
  |                                       |                                  | 
[ range normalization ]         [  Z-score normalization   ]  [ Robust Scaling ] 
|- Math concept name: 'affine'  |- Math concept: 'standard'   |-Base on 'median' 
|  'transformation'.            |  'score transformation'     | and 'IQR', this
|- It maps data from its        |- It 'centers' the data      | technique is 
|  'original' range to 'new'    |  (mean = 0) and scales      | more 'resistant'
|  'range' usually '[0, 1]'     |  it to unit variance        | to 'outliers'
|  using a 'linear'             |  (std=1)                    |
|  'transformation'             |- Formula:                   |- Formula
|- Formula:                     |       X - mean              |      x - Median 
|         X - Xmin              | X' = ----------             | X' = ----------
|  X'  = ----------             |       std (&)               |         IQR 
|        Xmax - Xmin            |- std: average distance      |- Ref : 'Median'
|- Reference: 'Range'           |  of each x from the mean    | 
|                               |- 'sensetive' to 'outliers'  | 
|                               |- reference: 'Mean'          | 

  # QUESTION:[ When to use feature scaling ? ]==================================
  # - range: (-1 => 1), (-3 => 3), (-0.3 => 0.3) are acceptable ranges.
  #   |=> (no need for scaling) 
  # - range: (0 => 3), (-2 => 0.5): it's okey, can be rescaled also 
  # - range: (-100 => 100) too large => rescale 
  # - range: (-0.001 => 0.001) too small => rescale
  # ============================================================================

  # CONCLUSION:=================================================================
  # - Achno hiya Feature Transformation or scaling hiya bi basata tanrakeb
  #   wahed l mask li kol value, mask howa value wehda akhra li katmatel nafss
  #   l quentity fi range or scale wahed akhur swa kbar mno o la sghar mno.
  # ============================================================================

===[ Checking gradient Descent for Conversion ]===

  > [ Learning Curve: ]

# REVISION:=====================================================================
# - Gradient Descent algorithm is a method to find the min of cost function
#   (global minimum). 
#   J(a,b)
#
#   | A(new) = A(old) - LR * & J(a,b) / &A 
# =>|
#   | B(new) = B(old) - LR * & J(a,b) / &B 
#
# - We keep iterating (making steps) until conversion where A = Cste & B = Cste
# ==============================================================================

- 'Based on the revesion', we know that the 'cost function' changes at each 
  'iteration (step)' made using gradient descent algorithm, So let's represent 
  'the changing of cost by number of iterations' in scatter plot

J(a->, b)
  |
  | *
  | *
  | *
  |  *
  |   *
  |     *
  |       *
  |          * *   *   *
  |---------------------------------> Iterations

  - This curve is called 'Learning Curve', In this case Gradient descent is
    'working correctly'

> [ Examples: Case of divertion ]

J(a->, b)                                J(a->, b)
  |                                          |
  |                                          |                     * 
  |                                          |                    * 
  | *                                        |                   *
  |  *        *     * *                      |                 * 
  |   *      * *   *   *                     |               *
  |     *   *   * *      *    *              |             *
  |       *                *                 |          *
  |                                          | *  *  * 
  |---------------------------> Iters        |---------------------------> Iters
  - Learning rate is big or there is a       - learing rate is big but the most 
    problem in the code.                       famous problem in this case 
                                               the code.

  # NOTE:=======================================================================
  # - This is the graphical method to check if the gradient descent is
  #   working properly. 
  # - There is another method called Automatic covergence test.
  # ============================================================================
  
  > [ Automatic convergence test ]
    * Is a constant value 'epsilon'

      [ epsilon = 10^-3 = 0.001 ] 

    - If J(a->, b) decreases by <= epsilon in one iteration, declare
      'convergence'

  # NOTE:=======================================================================
  # - The learning curve is the best method to check for gradient descent if
  #   it is working properly. 
  # ============================================================================

===[ How to find the best Learning Rate: ]===
* Keep changing the 'learning rate' and visualize the 'learning curve'    

  - It is globaly known to start by: 

    Learning Rate = 0.001     | 
    Learing Rate = 0.1        | Keep multiplaying by x10
    Learing Rate = 1          | or x3 ...
    ...

  - Then choose the learning rate that gives the learning curve that
    'deacrease fast and continous'.

===[ Polynomial Regression ]====================================================
* 'Polynomial Regression' is a type of regression analysis where the 
  relationship between the 'independent variable(x)' and the 'dependent'
  'variable (y)' is modeled as an nth-degree polynomial

  > [ Formula: ]
    
    [ Y-hat = a*X + b*X^2 + c*X^3 + ... + alpha ] 

  - It still considered a 'linear model', not because of the curve but because
    it's "linear in the coefficients".

  # NOTE:=======================================================================
  # - Polynomial regression transforms the original input features into
  #   'polynomial terms' (That what is called feature engineering).
  #   * Original Feature: 'X'
  #   * Transformed features: X, X^2, X^3, ..., X^n
  # - These nea features are then used in standard linear regression model to
  #   find the best-fitting curve.
  # ============================================================================

  > [ Things to Watch out For ]
    - 'Overfitting': High-degree polynomials can fit training data too colsely,  
      hurting generalization.
    - 'Extrapolation Risk': Polynomial curves can behavewildly outside the data
       range 
    - 'Scaling': It is a good idea to normalize or standardize input features  
      when using high-degree terms.

  > [ Linear Regression VS Polynomial Regression ]
  _____________________________________________________________________________
  | Aspect                 | Linear Regression | Polynomial Regression        |
  |------------------------|-------------------|------------------------------|
  |-Equation form          |-Y = ax + b        |-Y = aX + cX^2 + ...+ b       |
  |-Fit shape              |-Straight line     |-Curved line (parabola,       |
  |                        |                   | cubic, etc.)                 |
  |-Handles non-linearity? |-❌ No             |-✅ Yes                       |
  |-Model complexity       |-Low               |- Higher (risk of overfitting |
  |                        |                   | if degree is too high)       |
  |________________________|___________________|______________________________|

# CONCLUSION:===================================================================
# - Polynomial regression is a powerful extension of linear regression that
#   capture non-linear relationships by transforming features into polynomial
#   terms, It offers flexibility but must be used carefully to avoid overfitting 
# ==============================================================================

===[ Feature Engineering: ]===
* Feature engineering  is the process of 'transforming raw data into meaningful'
  'inputs (features)' that improve the performance of a machine learning model.
  - It involves 'selecting', 'creating', 'modifying' , or 'encoding varibales' 
    (features) in a way that helps the model 'better understand the patterns'
    'in the data'.

  > [ Feature Engineering Technique ]
    1. Feature Selection: 
       - Choosing the most relevant feature. 
       - Removing redundant or irrelevant features.
    2. Feature Transformation:
       - 'Scaling': Normalize or standardize features 
       - 'Encoding': Convert categorical varibales => Numerical form  
    3. Feature Creation:
       - 'Combine existing' features (e.g S=weight * height) 
       - 'Extract information' (e.g extract "year" from a date)
    4. Handling Missing data:
       - Input missing values using 'mean', 'median', 'mode', or 'predictive'
         'models' 
    5. Dimensionality Reduction: 
       - 'Reduce the number of feature' while 'preserving' information (PCA, 
         t-SNE) 

# CONCLUSION:===================================================================
# - Feature engineering is the 'art and science' of turning raw data into useful
#   input for machine learning.
# - It can dramatically affect model performance and is often more impactful 
#   than tweaking algorithms themselves.
# - 'Data cleaning' prepares the data for modeling, 'Feature engineering' makes
#   the data useful for the model.
# ==============================================================================

===[ Linear Regression VS Classification ]======================================
# REVISION:
# - Regression is a type of supervised learning used to predict 'continuous' 
#   'values' by modeling the relationship between input features and output
#   * Predicting house prices based on size, location ...

> [ Classification ]
  * It is a type of 'supervised learning' used to 'predict categorical labels' 
    by learning patterns in the data that separate different 'classes'
    . Ex: Predicting whether an email is 'spam' or 'not spam'

  # NOTE:=======================================================================
  # Regression     => Predict Numbers (e.g price, revenue, ...) 
  # Classification => Predict categories (e.g pass/fail, ...) 
  # ============================================================================

# ERROR:========================================================================
# - While linear regression is meant for regression (predicting continous value) 
#   , It can be used for 'binary classification' as a simple baseline model, but 
#   It is generally not recommended.
# - You can use 'logistic regression' or 'other classification algorithms 
#   (e.g, decision trees, SVMS, random forests) instead
# ==============================================================================

===[ Logistic Regression: ]=====================================================
* It is a 'classification algorithm', not a regression algorithm (despite
  the name), It is used to 'predict the probability' that a given input belongs
  to a 'particular class', typically in 'binary classification' (e.g., yes/no,
  spam/not spam, 0/1). 

  > [ Formula: ]
    
    => [ Sigmoid (Z) = 1 / 1 + E^-Z ]

      - 0 < g(Z) < 1 
      - Z = f(a->, b) = a-> * x-> + b 
   
# INFO:
# - 'The sigmoid function' is a mathematical function that maps any real-valued
#   number into a range between '0 and 1'. It is commonly used in machine
#   learning, especially in classification tasks.
# > [ Formula: ]
#               1 
#     σ(x) = -------
#           1 + e^-x
#
#     - x: is the input
#     - e 

    => [ fa->, b(x->) = 1 / 1 + e^(a-> * x-> + b) ]
                      1-|
                        |           *   * * *
                        |        *
                        |     *
        -0.5-         * | *  
                  *     |
               *        |
             *          |
    * * * *             |
    --------------------|----------------------> x
                        0

    - 0 < Ouput value < 1, means the probality that 'output value' is 1,
      givin input 'x->', parameters 'a->, b'
    - Example: F(x->) = 0.7: 'probalitiy of 70% that y is 1'
    - Fa->, b(X->) = P(y = 1|x->:w->,b): 'the probality that y is 1 givin input'
      'x->, parameters a->, b'

  # NOTE:
  # e: is the natural language of calculas (is the math of rate of change and
  # growth and areas)

> [ Auler's Number: ]

    => f(x) = (1 + 1 / n)^n   <|>  e = 1 + 1/1! + 1/2! + 1/3! + 1/4! + ..., etc 

  [   lim(n -> inf) (1 + 1/n)^n = e   ]


===[ Decision Boundary: ]===
