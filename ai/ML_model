#  â €â €â €â €â €â €â €â €â €â¢€â£¤â£¦â£´â£¶â£¾â£¿â£¶â£¶â£¶â£¶â£¦â£¤â£„â €â €â €â €â €â €â €                                              
#  â €â €â €â €â €â €â €â¢ â¡¶â »â ›â Ÿâ ‹â ‰â €â ˆâ ¤â ´â ¶â ¶â¢¾â£¿â£¿â£¿â£·â£¦â „â €â €â €                ð““  ML_model ð“”           
#  â €â €â €â €â €â¢€â ”â ‹â €â €â ¤â ’â ’â¢²â €â €â €â¢€â£ â£¤â£¤â£¬â£½â£¿â£¿â£¿â£·â£„â €â €                                              
#  â €â €â €â£€â£Žâ¢¤â£¶â£¾â …â €â €â¢€â¡¤â â €â €â €â  â£„â£ˆâ¡™â »â¢¿â£¿â£¿â£¿â£¿â£¿â£¦â €       Dev:  oezzaou  oussama.ezzaou@gmail.com 
#  â¢€â ”â ‰â €â Šâ ¿â ¿â£¿â ‚â  â ¢â£¤â ¤â£¤â£¼â£¿â£¶â£¶â£¤â£â£»â£·â£¦â£â¡»â£¿â£¿â£¿â£¿â¡€                                              
#  â¢¾â£¾â£†â£¤â£¤â£„â¡€â €â €â €â €â €â €â €â ‰â¢»â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¡‡                                              
#  â €â ˆâ¢‹â¢¹â ‹â ‰â ™â¢¦â €â €â €â €â €â €â¢€â£¼â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¡‡       Created: 2025/05/12 08:13:45 by oezzaou
#  â €â €â €â ‘â €â €â €â ˆâ¡‡â €â €â €â €â£ â£¾â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â ‡       Updated: 2025/05/21 16:57:27 by oezzaou
#  â €â €â €â €â €â €â €â €â¡‡â €â €â¢€â£¾â£¿â£¿â ¿â Ÿâ ›â ‹â ›â¢¿â£¿â£¿â »â£¿â£¿â£¿â£¿â¡¿â €                                              
#  â €â €â €â €â €â €â €â¢€â ‡â €â¢ â£¿â£Ÿâ£­â£¤â£¶â£¦â£„â¡€â €â €â ˆâ »â €â ˜â£¿â£¿â£¿â ‡â €                                              
#  â €â €â €â €â €â ±â ¤â Šâ €â¢€â£¿â¡¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â €â €â €â €â €â €â ˜â£¿â â €â €                             ð“†©â™•ð“†ª      
#  â €â €â €â €â €â¡„â €â €â €â ˜â¢§â¡€â €â €â ¸â£¿â£¿â£¿â Ÿâ €â €â €â €â €â €â â ‹â €â €â €                     ð“„‚ oussama ezzaouð“†ƒ  
#  â €â €â €â €â €â ˜â „â£€â¡€â ¸â “â €â €â €â  â Ÿâ ‹â â €â €â €â €â €â €â €â €â €â €â €â €                                              

===[ Model: Linear Regression ]=================================================
* Linear regression: (easy) is used to 'model the relationship' between: 
  - One independent variable (push_swap: Size)
  - One independent variable (push_swap: Range)
    > It fits a 'stright' line through the 'data that best predict the output'.

  > [ Model Formula: ]

      f(x) = ax + b

    - The formula is called 'model' also in ML.
    - f(x): (y) is the target (y-hat is the estimated/predicted value)
    - x: Input feature 

    # NOTE:=====================================================================
    # - Training set is the data used for training the model.                  |
    # - The generted fuction is called sometimes 'hypothesis'.                 |
    # ==========================================================================

  > [ Example: ]
    - Predection of salary based on the experionce:
    _______________________________
    | Experience (x) | Salary (y) |
    |----------------|------------|
    | 1              |  30k      |
    | 2              |  35k      |
    | 3              |  40k      |
    |_4______________|__45k______|

    * The model my learn: 

      Salary = 5000 * Experience + 25000 

    * if input feature = 5
      Salary predected = 500 * 5 + 25000 = 50000

    > [ What it does ?: ]
      - It taks a data set of (x, y) data points.
      - Find the 'line for the function that minimize the error' between
        'predicted and actual value'.
      - After training it can predict a 'y-hat' for any input feature 'x' 

# TIP:========================================================================== 
# - Curve function gives sometimes more predictive input (The same case that   |
#   i have faced) in push swap Where a I got a curve function between size     |
#   and range and i have choosen to work with linear function instead to       |
#   simplify the calculation.                                                  |
# - Improving means find the function (model) that gives target inputs (y-hat) |
#   'predective output' closer to the 'actual output'.                         | 
# - Improving the model involves changing the value of 'a (slop)' and          | 
#   'b y-intercept'. That is the same what i have done to improve my function  |
#   in push_swap (a, b: are called parametters/coefficients/weights).          |
# - But how can we measure and make sure that the optimized function is the    |
#   suitable one ? (cost function).                                            | 
# ==============================================================================

  > [ Cost Function: Squared error cost function ]
  - In machine learning a cost function is 'mathematical formula' that measures
    how wrong model's prediction are. It simply tells the model how 'far off'
    from the true value (it measures the quality of fiting).
      * Low Cost: good predictions
      * High Cost: bad predictions

      (MSQ): stands for Mean Squared Error
               |-----------------------------------------------------|
      Formula: | J(a, b) = 1 / 2n * ensemble (i -> n) (Ypi - Yai)^2  | 
               |-----------------------------------------------------|

      # TIP: DO NOT forget to prove it mathimaticaly. 

      - Yai: is the actual/True output at 'i' 
      - Ypi: is the predictive output estemated by the model at 'i'. 
      - (Ypi - Yai) is called the 'error' or 'residual', distance
        between [actual/True output, predictive output], residual can
        be 'positive/negative'.
      - n: number of trained examples. 

# QUESTION:[ Why devided by 2n ?]=============================================== 
# - Dividing 'n' gives the **average** squared error across all training       |
#   examples truning it into a [ mean squared error ].                         |
# - Dividing by '2' is done for 'mathematical convenience', when you perform   |
#   'gradient descent' to minimize the cost function, you will compute its     |
#   derivative, The derivative of a squared term (e.g., (h âˆ’ y)^2) brings      |
#   down a factor of 2, which cancels out with the 1 / 2, 'This simplifies the |
#   gradient update formula'.                                                  | 
# CONCLUSION:                                                                  |
# - 1/n â†’ Makes the cost function an average per training example.             |
# - 1/2 â†’ Simplifies the math when taking derivatives.                         |
# ==============================================================================

===[ Descent Gradient: ]===
* This algorithm is used to find the lowest point in the function. There are
  a lot of algorithms used to find the lowest point.

  1|_ find the solution of the equation f'(x) = 0 (where the slop at Xl is 0) 
  2|_ descent algorithm
  3|_ ...

- The cost function depends on the linear regression function, which depends on
  the slop (a) and y-intercept (b) => Cost function depends on the slop and
  y-intercept.

       J(a, b) = 1/2n sum(1 -> n) (Yp - Ya)^2 

    => [ J(a, b) = 1/2n sum(1 ->n) (ax + b - Ya)^2 ]

- J(a, b) is a multi-variable function, We will use the partial dirivative to
  track the rate of change (slop of J(a,b) at in respect of 'a' or 'b').
  > Initial derivative of J(a, b) in respect of a | range of change cost J(a,b)
    according to slop (a). 
  > Initial derivative of J(a, b) in repsect of b | range of change cost J(a,b)
    according to y-intercept (b).

# INFO:[ Descent Gradient Analogy ]=============================================
# - It is like moving blind you just make a small step and feel the steepness  |
#   then you move on.                                                          |
# ==============================================================================
- let's take a = 0 and b = 0

  1|> Solution:
    - we need to find the dirivative on 'a' and dirivative on 'b'. 
    - Let's take a random point in the graph the slop(a) and cost function(a,b)

        J(a,b)
          |
          |                   /
          |                  /
          |___________ /\   /
          |           /| \_/
          |          / |
          |         /  |
          |------------|-----------> (a)
                    <- Ai ->

  1|> The idea is to choose a point 'slop a(i)' as an initial value. 
  2|> Partial derivative gives me the 'rate of change' at a specific '(ai)'
  3|> Start moving by a step 'alpha' based on this form: 

     /|-> A(new) = A(i) - alpha * change of range at point A(i) 
  --||
     \|-> B(new) = B(i) - alpha * change of rate at point B(i)
    
     /|-> A(new) = A(i) - LR * (Partial Dirived at A(i) Point)  
  --||
     \|-> B(new) = A(i) - LR * (Partial Dirived at B(i) Point)

     /|-> A(new) = A(i) - LR * &/&x J(a,b)  
  --||
     \|-> B(new) = A(i) - LR * &/&y J(a,b)

     /|-> A(new) = A(i) - LR *  &/&x (sum(i=0->m) (Y-hat(i) - Ya)^2 / 2m)
  --||
     \|-> B(new) = A(i) - LR * &/&y (sum(i=0->m) (Y-hat(i) - Ya)^2 / 2m)

     /|-> A(new) = A(i) - LR *  (sum(i=0->m) (Y-hat(xi) - Ya)^2 / m) * xi
  --||
     \|-> B(new) = A(i) - LR * (sum(i=0->m) (Y-hat(xi) - Ya)^2 / m)


# INFO:=========================================================================
# - What we do exactly here is to move on the x Axis to find the point that has|
#   the partial dirivative at xi = 6xi/6x|/6yi/6y = slop = 0.                  |
# CONCLUSION:===================================================================
# - The gradient descent algorithm is about finding the min of the function    |
#   min(f(x)) where the slop is 0 (It is one of the most poplular algorithms). |
# ==============================================================================

- When you use all the training data to calculate the min(J(w,b)), In simple
  term using all training data at each step of greadient descent is called 
  'batch gradient descent'.
