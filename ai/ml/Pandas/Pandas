#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£§‚£¶‚£¥‚£∂‚£æ‚£ø‚£∂‚£∂‚£∂‚£∂‚£¶‚£§‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢†‚°∂‚†ª‚†õ‚†ü‚†ã‚†â‚†Ä‚†à‚†§‚†¥‚†∂‚†∂‚¢æ‚£ø‚£ø‚£ø‚£∑‚£¶‚†Ñ‚†Ä‚†Ä‚†Ä                  ìêì  Pandas ìêî           
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚†î‚†ã‚†Ä‚†Ä‚†§‚†í‚†í‚¢≤‚†Ä‚†Ä‚†Ä‚¢Ä‚£†‚£§‚£§‚£¨‚£Ω‚£ø‚£ø‚£ø‚£∑‚£Ñ‚†Ä‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚£Ä‚£é‚¢§‚£∂‚£æ‚†Ö‚†Ä‚†Ä‚¢Ä‚°§‚†è‚†Ä‚†Ä‚†Ä‚††‚£Ñ‚£à‚°ô‚†ª‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚†Ä       Dev: oezzaou <oussama.ezzaou@gmail.com> 
#  ‚¢Ä‚†î‚†â‚†Ä‚†ä‚†ø‚†ø‚£ø‚†Ç‚††‚†¢‚£§‚†§‚£§‚£º‚£ø‚£∂‚£∂‚£§‚£ù‚£ª‚£∑‚£¶‚£ç‚°ª‚£ø‚£ø‚£ø‚£ø‚°Ä                                              
#  ‚¢æ‚£æ‚£Ü‚£§‚£§‚£Ñ‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†â‚¢ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á                                              
#  ‚†Ä‚†à‚¢ã‚¢π‚†ã‚†â‚†ô‚¢¶‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£º‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á       Created: 2025/06/20 12:43:36 by oezzaou
#  ‚†Ä‚†Ä‚†Ä‚†ë‚†Ä‚†Ä‚†Ä‚†à‚°á‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†á       Updated: 2025/06/30 22:24:46 by oezzaou
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°á‚†Ä‚†Ä‚¢Ä‚£æ‚£ø‚£ø‚†ø‚†ü‚†õ‚†ã‚†õ‚¢ø‚£ø‚£ø‚†ª‚£ø‚£ø‚£ø‚£ø‚°ø‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚†á‚†Ä‚¢†‚£ø‚£ü‚£≠‚£§‚£∂‚£¶‚£Ñ‚°Ä‚†Ä‚†Ä‚†à‚†ª‚†Ä‚†ò‚£ø‚£ø‚£ø‚†á‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†±‚†§‚†ä‚†Ä‚¢Ä‚£ø‚°ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚£ø‚†è‚†Ä‚†Ä                             ìÜ©‚ôïìÜ™      
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°Ñ‚†Ä‚†Ä‚†Ä‚†ò‚¢ß‚°Ä‚†Ä‚†Ä‚†∏‚£ø‚£ø‚£ø‚†ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ê‚†ã‚†Ä‚†Ä‚†Ä                     ìÑÇ oussama ezzaouìÜÉ  
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚†Ñ‚£Ä‚°Ä‚†∏‚†ì‚†Ä‚†Ä‚†Ä‚††‚†ü‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä                                              

# ===[ General Information: ]===================================================
* In 'Data Science before training a model', The data typically goes through
  'serveral key stages' in a pipeline, These steps are crucial for ensuring the
  model receives 'clean', 'structured', and 'meaningful data':

  1|> [ Data Collection: ] 
      * 'Gather' data from different sources: 
        - Databases(SQL, NOSQL)
        - APIs
        - CSV/Excel Files
        - Web scraping
        - Sensors, logs, etc

  2|> [ Data Cleaning: ] 
      * 'Fix or remove, corrupted, duplicate, or missing data': 
        - Handle 'missing' values (e.g, file, drop, interpolate)
        - Remove 'duplicates'
        - Correct insconsistent formatting
        - Handle 'outliers' or 'noise'
        - 'Normalize' inconsistent units 

  3|> [ Data Transformation: (Preprocessing) ]
      * 'Convert data' into a 'suitable format':
        - 'Encoding': categorical varibales (e.g, one-hot, label encoding) 
        - 'Feature Scaling': normalization or standardization 

  4|> [ Exploratory Data Analysis (EDA) ] 
      * Analyze and visualize data to understand its structure
        - Histograms, boxplots, scatter plots
        - Correlation matrix
        - Summary statistics (mean, median, std) 
        - find patterns, trends, anomalies
  
  5|> [ Feature Engineering: ]
      * 'Create' new features or 'transform' existing onces to improve model
        performance:
        - Polynomial features 
        - Interaction terms
        - Log transforms
        - Aggregation (e.g, mean, sum per group)

  6|> [ Data Splitting: ]
      * Split data into subsets: 
        - 'Training set': to train the model
        - 'Validation set': to tune parameters (optional)
        - 'Test set': to evaluate model performance

===[ Summary: ]===

    Raw Data 
       ‚Üì
  Data Collection 
       ‚Üì
  Data Cleaning 
       ‚Üì
Preprocessing (Transformation & Encoding)
       ‚Üì
Exploratory Data Analysis (EDA)
       ‚Üì
  Feature Engineering 
       ‚Üì
  Train-Test Split 
       ‚Üì
  Model Training
================================================================================

===[ Pandas: ]==================================================================
* 'Pandas' is library 'built on top of Numpy', Pandas is optimized for 'data'
  'manipulation and analysis', especially 'tabular (row/column)' or
  'labeled data', Pandas is used:
    - Data cleaning and preprocessing
    - Data transformation and reshaping
    - Exploratory Data Analysis (EDA) 
    - Time series analysis
    - Importing/exporting real-world data (CSV, Excel, SQL, etc) 
    # NOTE:---------------------------------------------------------------------
    # - Pandas is not a 'replacement' for Numpy, It 'extends' it.
    # - It brings 'structure', 'labels', and 'data-aware operations' that are
    #   crucial for real-world data analysis 
    # - 'ndarray' is great for 'nemerical computation'; Pandas is optimized for
    #   'data manipulation', 'cleaning', and exploration. 
  - The term 'Pandas' is term derived from 'Panel Data', a term used in
    'econometrices' to describe 'multidimensional structured data' 
      . 'Panel Data' = data that invovles 'observations over multiple time'
        'periods' for the same individuals (e.g time series + cross-sectional
         data)
      . The 'library name' reflects its focus on providing powerful and flexible 
        'data structures' for working with such 'complex structured datasets'
  # CONCLUSION:-----------------------------------------------------------------
  # while 'Pandas' might sound like the animal üêº, it actually has a technical
  # origin rooted in 'data analysis'.

# QUESTION:[ Why learn Pandas ? ]===============================================
# 1|> Higher-level Abstraction: Pandas provides an intuitive way to work with
#     structured data, eliminating the need for complex loops and index tracking
# 2|> Faster Development: Cleaner and shorter code for common data analysis
#     tasks
# 3|> Real-World Data Ready: Most real-word data is 'tabular', with missing
#     values, labels, and mixed data types. (Pandas handles this gracefully)
# 4|> Seamless Integration: Works well with other data science tools, (like
#     Numpy, Matplotlib, Scikit-learn, etc) 
#>>> [ Numpy VS Pandas ]
# ______________________________________________________________________________ 
# | Aspect            | NumPy                 | Pandas                         |
# |-------------------|-----------------------|--------------------------------|
# | Focus             | Numerical computation | Data manipulation and analysis |
# | Data Type Support | Homogeneous arrays    | Heterogeneous tables           |
# | Primary Structure | ndarray               | Series and DataFrame           |
# | Use Case          | Scientific computing  | Real-world data analysis       |
# |-Learning Need     |-Essential for         |-Essential for high-level data  | 
# |                   | low-level operations  | analysis                       | 
# |___________________|_______________________|________________________________|
#
# TIP:==========================================================================
# - Use NumPy for efficient computation and Pandas for efficient data handling.
#   In most data science projects, you'll use both, but Pandas is your go-to
#   for structured, real-world datasets.

===[ The Core Motivation behind Pandas ]===
- The 'main goal' of 'creating pandas' was to add 'labels and metadata' to 
  to numerical data structures (like NumPy arrays), enabling more 'flexible' and
  'human-readable' data analysis workflows.
=> 'Labels' (especially column lables in a 'DataFrame') typically represent
   'input feature' in machine learning and data analysis.
=> [ Terminology: ]
  > 'Columns' -> represent 'features' (with column labels)
  > 'Rows'    -> represent 'records/observations/samples' (with row labels
    or index) 

# QUESTION:[ Why Labels Matter? ]-----------------------------------------------
# 1|> Contextual Understanding
#     - Labels like column names ("age", "price") and row indices ("user_001",
#       "2023-01-01") provide semantic meaning to raw data.
# 2|> Ease of Access & Manipulation
#     - Instead of arr[0, 1], you can do df.loc["user_001", "price"].
#     - Enables powerful slicing, filtering, and grouping based on names, not
#       just positions.
# 3|> Automatic Alignment
#     - Labeled operations (e.g., combining datasets with different indices) are
#       handled automatically.
#     - NumPy can't align data across mismatched indices ‚Äî Pandas can.
# 4|> Missing Data Support
#     - Real-world datasets often have incomplete entries ‚Äî Pandas uses NaN,
#       .isna(), .fillna(), etc., to manage this elegantly.
# QUESTION:[ How Pandas Built on Numpy ?]---------------------------------------
# - Uses 'NumPy (ndarray) under the hood' for fast computation 
# - Wraps it with: 
#   . `Index`: to assign labels
#   . `Series`: labeled 1D array 
#   . `DataFrame`: labeled 2D table 
# - Adds high-level data manipulation capabilities.
# ------------------------------------------------------------------------------
# CONCLUSION:===================================================================
# - multiple Series (each representing an input feature) can be combined into a
#   DataFrame ‚Äî a 2D data matrix ‚Äî to visualize and analyze both the features
#   (columns) and the cases (rows) easily.
# - DataFrame object represent my data matrix or matrix table that i have learnt 
#   in 'math_statistics'.

===[ Series ]===
* A Series is essentially a 'one-dimensional labeled array' than can hold any data 
  type

--> [ Internal Components: (Attribues) ]
    - 'values': Stores the actual data as a 'NumPy''ndarray' (or sometimes
                'ExtensionArray', like for 'pd.StringDtype', 'Categorical', etc)
    - 'index': Stores 'labels' for each element (row labels). It is an
               'instance' of the 'Index' class. 
    - 'dtype': Stores the data type of the elements 

--> [ Examples: ]
    Import pandas as pd

    s = pd.Series([10, 20, 30], index=["a", "b", "c"])
    # values -> array([10, 20, 30]) 
    # index  -> Index(['a', 'b', c]) => Labels

===[ DataFrame: ]===
* A 'DataFrame' is a 'two-dimensional labeled data structure' (like a table or
  spreadsheet).

# INFO:[ Origin Of the Term 'DataFrame' ]=======================================
# - The name `DataFrame` comes from the 'R programming language' which used it
#   to describe a 'tabular data structure' that is: 
#   . 2-dimensional (like a matrix)
#   . Has 'Labeled axes' (rows and columns)
#   . Can store 'heterogeneous data types' (different types in different
#     columns) 
# - 'Wes Mckinney', the creator of Pandas, was inspired by R when he designed
#   Pandas, He browwed the name and concept of the 'DataFrame'
# QUESTION:[ Why Not `DataMatrix` name for example ? ]--------------------------
# - A 'matrix' is a mathematical concept that implies: 
#   ____________________________________________________________________________
#   | Matrix Characteristic | DataFrame Equivalent | Why Not Enough?           |
#   |-----------------------|----------------------|---------------------------|
#   |-Purely numerical      |-Columns can be, ints |-DataFrames can store      |
#   |                       | strings, etc         | mixed types               |
#   |-Only 2D and           |-DataFrames are 2D    |-Matrices are not flexible | 
#   | homogeneous           | but heterogeneous    | for real-world data       |
#   |-Indexed by position   |-DataFrames are       |-Real data often needs     | 
#   | only                  | labeled              | named columns/index       |
#   |_______________________|______________________|___________________________|
# - 'DataMatrix' would be misleading because:
#  . It suggest only 'numeric, homogeneous' data (like in NumPy)
#  . It ignores 'labels' which are contral to pandas 
#  . It does not reflect the 'flexibility' needed for real-world datasets
#    (think CSV files, Databases, etc)
# - 'DataFrame' make Sense because: 
#  > 'Frame' suggest a 'structure or container', just like a photo frame
#    contains multiple photos.
#  > 'Data' represent the 'contents', actual values, columns rows
#  > Together, 'DataFrame' means: 'A structured, labeled container for'
#    'your data'

--> [ Internal Components: (Attributes) ]
    - '_data': Under the hood, stores data in a 'block manager' which manages
               a dictionary of 'ndarray' or 'ExtensionArray' columns grouped by
               dtype. 
    - 'columns': Labels for columns (an 'Index' object) 
    - 'index': 'Labels' for 'rows' (also an 'Index' object (samples, observation
               cases <term in statistics>)
    - Each column is a 'Series' with the same index

--> [ Example: ]
    Import pandas as pd

    df = pd.DataFrame({
      "feature1": ["oussama", "ezzaou"]
      "feature2": [25, 30]
    }, index=["case1", "case2"])

    #>>> Internally:
    # name -> Series(['oussama', 'ezzaou', dtype=object])
    # age -> Series([25, 30], dtype=int)
    #>>> Data Matrix (Matrix Table)
    #         _______________________
    #         | Feature1 | Feature2 |
    # |-------|----------|----------|
    # | case1 | oussama  |  25      | 
    # |-------|----------|----------|
    # | case2 | ezzaou   |  30      |
    # |_______|__________|__________|

--> [ Pandas Core Class UML: ]
    +------------------------------------------------+
    |                DataFrame                       |
    +------------------------------------------------+
    | ‚Äì _data: BlockManager                          |
    | ‚Äì index: Index                                 |
    | ‚Äì columns: Index                               |
    +------------------------------------------------+
    | + __init__(...)                                |
    | + locate/iloc/select etc.                      |
    | + groupby(), merge(), pivot(), to_csv(), ...   |
    +------------------------------------------------+
                       ‚ñ≤         ‚ñ≤
                       |         |
         contains      |         | contains
                       |         |
        +----------------+   +----------------+
        |    Series      |   |     Index      |
        +----------------+   +----------------+
        | ‚Äì values: ndarray/ExtensionArray    |
        | ‚Äì index: Index                      |
        | ‚Äì name: Optional[str]               |
        +----------------+   +----------------+
        | + __init__(...)                     |
        | + arithmetic methods (add, mean, ..)|
        | + isna(), plot(), ‚Ä¶                 |
        +----------------+   +----------------+


===[ Reading in file: ]===
* As the first step made is to read a dataset from a file to dataframe
  (.csv, .text, .json)

  1|> "pd.read_csv(r'file_path')" this function is used to read in 'csv files' 
 
    . header=: (optional) It is used for input feature labels 
                      Ex: 'header=None' removes the labels and use index instead 
    . names=[inputfeature1, inputfeature2, ...], this function can be used also 

    # REVISION:----------------------------------------------------------------
    # - '\', ":",  are still special characthere until you till python to deal
    # with them as a row string
    # - What makes 'DataFrame' different is having labels 

    # TIP:---------------------------------------------------- 
    # - df is the standard name of the dataset red from a file

    . sep='separator': defines the separator between input features. 

  2|> 'pandas.read_table(data.txt)' this function is used to read a txt files 

  3|> 'pandas.read_json('data.json')': used to read json file into a DataFrame 

  4|> 'pandas.read_excel(data.xlsx, sheet_name='')': used for excel files


  # INFO:----------------------------------------------------------------------
  # - pd.set_option('display.max.rows', nbr/None): is used to display a nbr of
  #   rows
  # - pd.set_option('display.max_columns', nbr/None) is used to display a nbr
  #   of columns (None to display all columns)

===[ DataFrame Exploring:  ]===
    - 'df.info()': used to get a 'concis summary' of a 'Pandas DataFrame', It is
      very helpful for 'understanding the structure' of your dataset quickly.
    - 'df.head(nbr=5)': used to display the 'nbr' from the 'head' of 'DataFrame' 
    - 'df.tail(nbr=5)': used to display the 'nbr' from the 'tail (end)' of
                        'DataFrame' 
    - 'df.shape': shape of the 'DataFrame (dimensions)' 
    - "df['x']": used to get the a specific input feature by its name. 
    - 'df.loc'
    - 'df.iloc'

===[ Filtering and Ordering: ]===
 
> [ Filtering: ]
  - 1|> "df[df['x'] > y]": returns all the the 'cases/samples/observations' that
    have an input feature 'x' greater (`>`) than 'y'.

    # NOTE:---------------------------------------------------------------------
    # - '>' were used as an example, you CAN USE ANY OTHER COMPARISON operator.
    # - It is almost the same idea in 'NumPy' 
    # --------------------------------------------------------------------------
  
  2|> "df.isin()": is used to 'filter data' by checking whether each element in
    DataFrame or Series 'exist in a list or another set values'. It returns a
    'Boolean Mask', `True` if the element 'is in' the given list, else `False`

  3|> "df['x'].str.contains()": method is used to 'check if a string column' 
      'contains a certain substring or pattern' (using regex by default)

  4|> 'df.set_index()': is used to 'change the index' of a DataFrame to one or
      more existing columns. 

      > Ex:
          df.set_index('x'): 'x' is the input feature label 

  5|> 'df.filter()': is used to 'select specific rows or columns' based on
      'labels (names)', 'not by values'.

      df.filter(items=[label1, label2, label3, ...], like="regrex", axis=)
        . 'axis=': 0 represent the samples/observation labels 
                   axis=1: represetn the input feature labels
        . 'items=': get a list of items (row labels or column labels)
        . 'like=': like="x", brings all labels that contains 'x' as part from
                   the names. 
  6|> "df.loc['x']": is a powerful method for 'label-based selection' in pandas  
                   It allows you to 'access rows and columns by their labels',
                   not by position (use iloc for that)

      >>> [ df.loc[row_label, column_label] ] <<<

      . You can pass single labels, lists, slices or boolean masks
      . It includes both the 'start and end' of labels slices (unlike python's
        exclusive slicing)

        df.loc[start_row_lebale:end_row_label, start_col_label: end_col_label]

  7|> "df.iloc[]": is used for 'position-based indexing', It lets you select rows
                 and columns by 'integer location', just like you would with
                 python lists

      # INFO:------------------------------
      # - loc: stands for LOCation
      # - iloc: stands for Integer LOCation

      >>>[ df.iloc[start_row_idx: end_row_idx, start_col_idx: end_row_idx] ]<<<

  # NOTE:-------------------------------------
  # loc[] and iloc[] are Similar techniques to indexing and slicing in NumPy

    . You can pass integers, lists of integers, slices, or boolean masks.

> [ Ordering: ]
  8|> "df.sort_values(by=['label1', ...], ascending=[True, ...])", Is used to
    'sort a DataFrame or Series by one or more columns (or by its values)'

[df.sort_values(by, axis=0, ascending=True, inplace=False, na_position='last') ]
