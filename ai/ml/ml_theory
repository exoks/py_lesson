#  â €â €â €â €â €â €â €â €â €â¢€â£¤â£¦â£´â£¶â£¾â£¿â£¶â£¶â£¶â£¶â£¦â£¤â£„â €â €â €â €â €â €â €                                              
#  â €â €â €â €â €â €â €â¢ â¡¶â »â ›â Ÿâ ‹â ‰â €â ˆâ ¤â ´â ¶â ¶â¢¾â£¿â£¿â£¿â£·â£¦â „â €â €â €               ð““  ml_theory ð“”           
#  â €â €â €â €â €â¢€â ”â ‹â €â €â ¤â ’â ’â¢²â €â €â €â¢€â£ â£¤â£¤â£¬â£½â£¿â£¿â£¿â£·â£„â €â €                                              
#  â €â €â €â£€â£Žâ¢¤â£¶â£¾â …â €â €â¢€â¡¤â â €â €â €â  â£„â£ˆâ¡™â »â¢¿â£¿â£¿â£¿â£¿â£¿â£¦â €       Dev:  oezzaou  oussama.ezzaou@gmail.com 
#  â¢€â ”â ‰â €â Šâ ¿â ¿â£¿â ‚â  â ¢â£¤â ¤â£¤â£¼â£¿â£¶â£¶â£¤â£â£»â£·â£¦â£â¡»â£¿â£¿â£¿â£¿â¡€                                              
#  â¢¾â£¾â£†â£¤â£¤â£„â¡€â €â €â €â €â €â €â €â ‰â¢»â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¡‡                                              
#  â €â ˆâ¢‹â¢¹â ‹â ‰â ™â¢¦â €â €â €â €â €â €â¢€â£¼â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¡‡       Created: 2025/05/12 08:13:45 by oezzaou
#  â €â €â €â ‘â €â €â €â ˆâ¡‡â €â €â €â €â£ â£¾â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â ‡       Updated: 2025/11/09 18:01:56 by oezzaou
#  â €â €â €â €â €â €â €â €â¡‡â €â €â¢€â£¾â£¿â£¿â ¿â Ÿâ ›â ‹â ›â¢¿â£¿â£¿â »â£¿â£¿â£¿â£¿â¡¿â €                                              
#  â €â €â €â €â €â €â €â¢€â ‡â €â¢ â£¿â£Ÿâ£­â£¤â£¶â£¦â£„â¡€â €â €â ˆâ »â €â ˜â£¿â£¿â£¿â ‡â €                                              
#  â €â €â €â €â €â ±â ¤â Šâ €â¢€â£¿â¡¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â €â €â €â €â €â €â ˜â£¿â â €â €                             ð“†©â™•ð“†ª      
#  â €â €â €â €â €â¡„â €â €â €â ˜â¢§â¡€â €â €â ¸â£¿â£¿â£¿â Ÿâ €â €â €â €â €â €â â ‹â €â €â €                     ð“„‚ oussama ezzaouð“†ƒ  
#  â €â €â €â €â €â ˜â „â£€â¡€â ¸â “â €â €â €â  â Ÿâ ‹â â €â €â €â €â €â €â €â €â €â €â €â €                                              

===[ Index: ]===================================================================
1|> Linear Regression
    - cost function (Squared Error Cost function)
    - Gradient Descent 
2|> Multiple Linear Regression
    - Feature Scaling (range (min_max), z-score, robust)
    - checking gradient descent for convertions (learning curve & learning rate)
3|> Plynomial Regression 
    - Feature Engineering
4|> Regression VS Classification
5|> Logistic Regression
    - Decision Boundary 
    - Cost function 
6|> Overfitting & Underfitting
    - overfitting
      . Regularization 
      . Cost function with regularization
      . Gradient Descent with regularization


    |--------------[ Machine Learning Algorithms Types ]--------------|
    |                                |                                |
[ Parmateric Models ]        [ Tree-Based Models ]     [ Neural Network Models ]
- Linear Regression          - Random Forest           - CNNs, RNNs 
- Multi-linear regression    - Gradient Boosting
- Polynomial regression        (XGBoost, LightGBM) 
- Logistic regression


===[ Model: Linear Regression ]=================================================
* Linear regression: (easy) is used to 'model the relationship' between: 
  - One independent variable (push_swap: Size)
  - One dependent variable (push_swap: Range)
    > It fits a 'stright' line through the 'data that best predict the output'.

  > [ Model Formula: ]

      f(x) = ax + b

    - The formula is called 'model' also in ML.
    - f(x): (y) is the target (y-hat is the estimated/predicted value)
    - x: Input feature 

    # NOTE:=====================================================================
    # - Training set is the data used for training the model.                  |
    # - The generted fuction is called sometimes 'hypothesis'.                 |
    # ==========================================================================

  > [ Example: ]
    - Predection of salary based on the experionce:
    _______________________________
    | Experience (x) | Salary (y) |
    |----------------|------------|
    | 1              |  30k       |
    | 2              |  35k       |
    | 3              |  40k       |
    |_4______________|__45k_______|

    * The model my learn: 

      Salary = 5000 * Experience + 25000 

    * if input feature = 5
      Salary predected = 500 * 5 + 25000 = 50000

    > [ What it does ?: ]
      - It taks a data set of (x, y) data points.
      - Find the 'line for the function that minimize the error' between
        'predicted and actual value'.
      - After training it can predict a 'y-hat' for any input feature 'x' 

# TIP:========================================================================== 
# - Curve function gives sometimes more predictive input (The same case that   |
#   i have faced) in push swap Where a I got a curve function between size     |
#   and range and i have choosen to work with linear function instead to       |
#   simplify the calculation.                                                  |
# - Improving means find the function (model) that gives target inputs (y-hat) |
#   'predective output' closer to the 'actual output'.                         | 
# - Improving the model involves changing the value of 'a (slop)' and          | 
#   'b y-intercept'. That is the same what i have done to improve my function  |
#   in push_swap (a, b: are called parametters/coefficients/weights).          |
# - But how can we measure and make sure that the optimized function is the    |
#   suitable one ? (cost function).                                            | 
# ==============================================================================

===[ Cost Function: Squared error cost function ]===
  - In machine learning a cost function is 'mathematical formula' that measures
    how wrong model's prediction are. It simply tells the model how 'far off'
    from the true value (it measures the quality of fiting).
      * Low Cost: good predictions
      * High Cost: bad predictions

      (MSE): stands for Mean Squared Error
               |-----------------------------------------------------|
      Formula: | J(a, b) = 1 / 2n * sum (i -> n) (Ypi - Yai)^2       |
               |-----------------------------------------------------|

      # TIP: DO NOT forget to prove it mathimaticaly. 

      - Yai: is the actual/True output at 'i' 
      - Ypi: is the predictive output estemated by the model at 'i'. 
      - (Ypi - Yai) is called the 'error' or 'residual', distance
        between [actual/True output, predictive output], residual can
        be 'positive/negative'.
      - n: number of trained examples. 

# QUESTION:[ Why devided by 2n ?]=============================================== 
# - Dividing 'n' gives the average squared error across all training       |
#   examples truning it into a [ mean squared error ].                         |
# - Dividing by '2' is done for 'mathematical convenience', when you perform   |
#   'gradient descent' to minimize the cost function, you will compute its     |
#   derivative, The derivative of a squared term (e.g., (h âˆ’ y)^2) brings      |
#   down a factor of 2, which cancels out with the 1 / 2, 'This simplifies the |
#   gradient update formula'.                                                  | 
# CONCLUSION:                                                                  |
# - 1/n â†’ Makes the cost function an average per training example.             |
# - 1/2 â†’ Simplifies the math when taking derivatives.                         |
# ==============================================================================

===[ Gradient Descent: ]===
* This algorithm is used to find the lowest point in the function. There are
  a lot of algorithms used to find the lowest point.

  1|_ find the solution of the equation f'(x) = 0 (where the slop at Xl is 0) 
  2|_ descent algorithm
  3|_ ...

- The cost function depends on the linear regression function, which depends on
  the slop (a) and y-intercept (b) => Cost function depends on the slop and
  y-intercept.

       J(a, b) = 1/2n sum(1 -> n) (Yp - Ya)^2 

    => [ J(a, b) = 1/2n sum(1 ->n) (ax + b - Ya)^2 ]

- J(a, b) is a multi-variable function, We will use the partial dirivative to
  track the rate of change (slop of J(a,b) at in respect of 'a' or 'b').
  > Initial derivative of J(a, b) in respect of a | range of change cost J(a,b)
    according to slop (a). 
  > Initial derivative of J(a, b) in repsect of b | range of change cost J(a,b)
    according to y-intercept (b).

# INFO:[ Descent Gradient Analogy ]=============================================
# - It is like moving blind you just make a small step and feel the steepness  |
#   then you move on.                                                          |
# ==============================================================================
- let's take a = 0 and b = 0

  1|> Solution:
    - we need to find the dirivative on 'a' and dirivative on 'b'. 
    - Let's take a random point in the graph the slop(a) and cost function(a,b)

        J(a,b)
          |
          |                   /
          |                  /
          |___________ /\   /
          |           /| \_/
          |          / |
          |         /  |
          |------------|-----------> (a)
                    <- Ai ->

  1|> The idea is to choose a point 'slop a(i)' as an initial value. 
  2|> Partial derivative gives me the 'rate of change' at a specific '(ai)'
  3|> Start moving by a step 'alpha' based on this form: 

     /|-> A(new) = A(i) - alpha * change of range at point A(i) 
  --||
     \|-> B(new) = B(i) - alpha * change of rate at point B(i)
    
     /|-> A(new) = A(i) - LR * (Partial Dirived at A(i) Point)  
  --||
     \|-> B(new) = A(i) - LR * (Partial Dirived at B(i) Point)

     /|-> A(new) = A(i) - LR * &/&x J(a,b)  
  --||
     \|-> B(new) = A(i) - LR * &/&y J(a,b)

     /|-> A(new) = A(i) - LR *  &/&x (sum(i=0->m) (Y-hat(i) - Ya)^2 / 2m)
  --||
     \|-> B(new) = A(i) - LR * &/&y (sum(i=0->m) (Y-hat(i) - Ya)^2 / 2m)

     /|-> A(new) = A(i) - LR *  (sum(i=0->m) (Y-hat(xi) - Ya)^2 / m) * xi
  --||
     \|-> B(new) = A(i) - LR * (sum(i=0->m) (Y-hat(xi) - Ya)^2 / m)


# INFO:=========================================================================
# - What we do exactly here is to move on the x Axis to find the point that has|
#   the partial dirivative at xi = 6xi/6x|/6yi/6y = slop = 0.                  |
# CONCLUSION:===================================================================
# - The gradient descent algorithm is about finding the min of the function    |
#   min(f(x)) where the slop is 0 (It is one of the most poplular algorithms). |
# ==============================================================================

- When you use all the training data to calculate the min(J(w,b)), In simple
  term using all training data at each step of gradient descent is called 
  'batch gradient descent'.

# QUESTION:[Why gradient descent for the best fitting instead of Pearons's (r) ]
# - Pearsonâ€™s r is useful for interpreting simple linear relationships,
#   but gradient descent is a flexible optimization tool for learning the best
#   parameters in more complex and high-dimensional models, which is essential
#   in modern machine learning.
# - But you can still use Pearson's r cofficient in the case of simple linear 
#   regression to find the equation of the best line that fits the data.

===[ Model: Multiple linear regression ]========================================
* This model is used to model the relationship between one 'dependent' variable
  and 'two or more independent variables'|also called 'predictors' or 'features'

  > [ model formula: ]
      - single linear regression formula is:
        f(x) = ax + b
      - multiple linear regression formula would look like: 
        f(x, y, ....) = ax + by + cz ... + b
        f(x1, x2, x3 ...) = a1 * x1 + a2 * x2 + ... + an * xn + b
      
      . moving a unit on x
      . moving b unit on y
      . moving c unit on z
        ...

      x-> = |x| = |x1|         a-> = |a| = |a1| 
            |y|   |x2|               |b|   |a2|
            |z|   |x3|               |c|   |a3|
            ...   ...                   ...    ...
      
      # NOTE: a-> is a row vector while x-> is a column vector. 

        [ f(x->) = a-> . x-> + b ] 

  # NOTE: a-> . x-> : is called dot product, It is the projection of a-> on x->
  # ERROR:
  # - what i have said is not true at all 
  # - row vector is not a ghraphical vector, i guess no projection is here. 

===[ Feature Scaling: ]===
    * Feature scaling is 'preprocessing technique' used to 'normalize' the 
      range of independent variables (features) in a dataset

    - Normalization (min-max scaling), Standardization (Z-score Scaling) are a
      'common methods of feature scaling'. 

    # NOTE:=====================================================================
    # - Instead of working by the original values, we can work by normalization,
    #   standerdization forms Instead. but Why ?
    # - To keep the input feature values in the same range, (their ranges must
    #   be close to each others) 
    # - Of course there is nothing can make this transformation than
    #   Normalization (max, mean ...), Standardization (Z-score ...), ... 

  # INFO:
   - The Z-score normalization or standardization is the most method famous
     method to make 'feature scaling'.

  > [ Feature Transformation methods: ]

  |--------------------------------[ Normalization ]-------------------------|
  |                                       |                                  | 
[ range normalization ]         [  Z-score normalization   ]  [ Robust Scaling ] 
|- Math concept name: 'affine'  |- Math concept: 'standard'   |-Base on 'median' 
|  'transformation'.            |  'score transformation'     | and 'IQR', this
|- It maps data from its        |- It 'centers' the data      | technique is 
|  'original' range to 'new'    |  (mean = 0) and scales      | more 'resistant'
|  'range' usually '[0, 1]'     |  it to unit variance        | to 'outliers'
|  using a 'linear'             |  (std=1)                    |
|  'transformation'             |- Formula:                   |- Formula
|- Formula:                     |       X - mean              |      x - Median 
|         X - Xmin              | X' = ----------             | X' = ----------
|  X'  = ----------             |       std (&)               |         IQR 
|        Xmax - Xmin            |- std: average distance      |- Ref : 'Median'
|- Reference: 'Range'           |  of each x from the mean    | 
|                               |- 'sensetive' to 'outliers'  | 
|                               |- reference: 'Mean'          | 

  # QUESTION:[ When to use feature scaling ? ]==================================
  # - range: (-1 => 1), (-3 => 3), (-0.3 => 0.3) are acceptable ranges.
  #   |=> (no need for scaling) 
  # - range: (0 => 3), (-2 => 0.5): it's okey, can be rescaled also 
  # - range: (-100 => 100) too large => rescale 
  # - range: (-0.001 => 0.001) too small => rescale
  # ============================================================================

  # CONCLUSION:=================================================================
  # - Achno hiya Feature Transformation or scaling hiya bi basata tanrakeb
  #   wahed l mask li kol value, mask howa value wehda akhra li katmatel nafss
  #   l quentity fi range or scale wahed akhur swa kbar mno o la sghar mno.
  # ============================================================================

===[ Checking gradient Descent for Conversion ]===

  > [ Learning Curve: ]

# REVISION:=====================================================================
# - Gradient Descent algorithm is a method to find the min of cost function
#   (global minimum). 
#   J(a,b)
#
#   | A(new) = A(old) - LR * & J(a,b) / &A 
# =>|
#   | B(new) = B(old) - LR * & J(a,b) / &B 
#
# - We keep iterating (making steps) until conversion where A = Cste & B = Cste
# ==============================================================================

- 'Based on the revision', we know that the 'cost function' changes at each 
  'iteration (step)' made using gradient descent algorithm, So let's represent 
  'the changing of cost by number of iterations' in scatter plot

J(a->, b)
  |
  | *
  | *
  | *
  |  *
  |   *
  |     *
  |       *
  |          * *   *   *
  |---------------------------------> Iterations

  - This curve is called 'Learning Curve', In this case Gradient descent is
    'working correctly'

> [ Examples: Case of divertion ]

J(a->, b)                                J(a->, b)
  |                                          |
  |                                          |                     * 
  |                                          |                    * 
  | *                                        |                   *
  |  *        *     * *                      |                 * 
  |   *      * *   *   *                     |               *
  |     *   *   * *      *    *              |             *
  |       *                *                 |          *
  |                                          | *  *  * 
  |---------------------------> Iters        |---------------------------> Iters
  - Learning rate is big or there is a       - learing rate is big but the most 
    problem in the code.                       famous problem in this case 
                                               the code.

  # NOTE:=======================================================================
  # - This is the graphical method to check if the gradient descent is
  #   working properly. 
  # - There is another method called Automatic covergence test.
  # ============================================================================
  
  > [ Automatic convergence test ]
    * Is a constant value 'epsilon'

      [ epsilon = 10^-3 = 0.001 ] 

    - If J(a->, b) decreases by <= epsilon in one iteration, declare
      'convergence'

  # NOTE:=======================================================================
  # - The learning curve is the best method to check for gradient descent if
  #   it is working properly. 
  # ============================================================================

  >  [ How to find the best Learning Rate: ]===
     * Keep changing the 'learning rate' and visualize the 'learning curve'    

     - It is globaly known to start by: 

        Learning Rate = 0.001     | 
        Learing Rate = 0.1        | Keep multiplaying by x10
        Learing Rate = 1          | or x3 ...
        ...

     - Then choose the learning rate that gives the learning curve that
       'deacrease fast and continous'.

===[ Model: Polynomial Regression ]=============================================
* 'Polynomial Regression' is a type of regression analysis where the 
  relationship between the 'independent variable(x)' and the 'dependent'
  'variable (y)' is modeled as an nth-degree polynomial

  > [ Formula: ]
    
    [ Y-hat = a*X + b*X^2 + c*X^3 + ... + alpha ] 

  - It still considered a 'linear model', not because of the curve but because
    it's "linear in the coefficients".

  # NOTE:=======================================================================
  # - Polynomial regression transforms the original input features into
  #   'polynomial terms' (That what is called feature engineering).
  #   * Original Feature: 'X'
  #   * Transformed features: X, X^2, X^3, ..., X^n
  # - These nea features are then used in standard linear regression model to
  #   find the best-fitting curve.
  # ============================================================================

  > [ Things to Watch out For ]
    - 'Overfitting': High-degree polynomials can fit training data too colsely,  
      hurting generalization.
    - 'Extrapolation Risk': Polynomial curves can behave wildly outside the data
       range 
    - 'Scaling': It is a good idea to normalize or standardize input features  
      when using high-degree terms.

  > [ Linear Regression VS Polynomial Regression ]
  _____________________________________________________________________________
  | Aspect                 | Linear Regression | Polynomial Regression        |
  |------------------------|-------------------|------------------------------|
  |-Equation form          |-Y = ax + b        |-Y = aX + cX^2 + ...+ b       |
  |-Fit shape              |-Straight line     |-Curved line (parabola,       |
  |                        |                   | cubic, etc.)                 |
  |-Handles non-linearity? |-âŒ No             |-âœ… Yes                       |
  |-Model complexity       |-Low               |- Higher (risk of overfitting |
  |                        |                   | if degree is too high)       |
  |________________________|___________________|______________________________|

# CONCLUSION:===================================================================
# - Polynomial regression is a powerful extension of linear regression that
#   capture non-linear relationships by transforming features into polynomial
#   terms, It offers flexibility but must be used carefully to avoid overfitting 
# ==============================================================================

===[ Feature Engineering: ]===
* Feature engineering  is the process of 'transforming raw data into meaningful'
  'inputs (features)' that improve the performance of a machine learning model.
  - It involves 'selecting', 'creating', 'modifying' , or 'encoding varibales' 
    (features) in a way that helps the model 'better understand the patterns'
    'in the data'.

  > [ Feature Engineering Technique ]
    1. Feature Selection: (disavantage: ignoring of features) 
       - Choosing the most relevant feature. 
       - Removing redundant or irrelevant features.
    2. Feature Transformation:
       - 'Scaling': Normalize or standardize features 
       - 'Encoding': Convert categorical varibales => Numerical form 
    3. Feature Creation:
       - 'Combine existing' features (e.g S=weight * height) 
       - 'Extract information' (e.g extract "year" from a date)
    4. Handling Missing data:
       - Input missing values using 'mean', 'median', 'mode', or 'predictive'
         'models' 
    5. Dimensionality Reduction: 
       - 'Reduce the number of feature' while 'preserving' information (PCA, 
         t-SNE) 

# CONCLUSION:===================================================================
# - Feature engineering is the 'art and science' of turning raw data into useful
#   input for machine learning.
# - It can dramatically affect model performance and is often more impactful 
#   than tweaking algorithms themselves.
# - 'Data cleaning' prepares the data for modeling, 'Feature engineering' makes
#   the data useful for the model.
# ==============================================================================

===[ Linear Regression VS Classification ]======================================
# REVISION:
# - Regression is a type of supervised learning used to predict 'continuous' 
#   'values' by modeling the relationship between input features and output
#   * Predicting house prices based on size, location ...

> [ Classification ]
  * It is a type of 'supervised learning' used to 'predict categorical labels' 
    by learning patterns in the data that separate different 'classes'
    . Ex: Predicting whether an email is 'spam' or 'not spam'

  # NOTE:=======================================================================
  # Regression     => Predict Numbers (e.g price, revenue, ...) 
  # Classification => Predict categories (e.g pass/fail, ...) 
  # ============================================================================

# ERROR:========================================================================
# - While linear regression is meant for regression (predicting continous value) 
#   , It can be used for 'binary classification' as a simple baseline model, but 
#   It is generally not recommended.
# - You can use 'logistic regression' or 'other classification algorithms 
#   (e.g, decision trees, SVMS, random forests) instead
# ==============================================================================

===[ Logistic Regression: ]=====================================================
* It is a 'classification algorithm', not a regression algorithm (despite
  the name), It is used to 'predict the probability' that a given input belongs
  to a 'particular class', typically in 'binary classification' (e.g., yes/no,
  spam/not spam, 0/1). 

  > [ Formula: ]
    
    => [ Sigmoid (Z) = 1 / 1 + E^-Z ]

      - 0 < g(Z) < 1 
      - Z = f(a->, b) = a-> * x-> + b 
   
# INFO:=========================================================================
# - 'The sigmoid function' is a mathematical function that maps any real-valued
#   number into a range between '0 and 1'. It is commonly used in machine
#   learning, especially in classification tasks.
# > [ Formula: ]
#               1 
#     Ïƒ(x) = -------
#           1 + e^-x
#
#     - x: is the input
#     - e: Aulier's number 2.78 .. 

    => [ fa->, b(x->) = 1 / 1 + e^(a-> * x-> + b) ]
        g(fa->,b(x)) => g(a-> . x + b)  

      => g(...): is the sigmoid function 

                      1-|
                        |           *   * * *
                        |        *
                        |     *
        -0.5-         * | *  
                  *     |
               *        |
             *          |
    * * * *             |
    --------------------|----------------------> x
                        0

    - 0 < Ouput value < 1, means the probality that 'output value' is 1,
      givin input 'x->', parameters 'a->, b'
    - Example: F(x->) = 0.7: 'probalitiy of 70% that y is 1'
    - Fa->, b(X->) = P(y = 1|x->:w->,b): 'the probality that y is 1 givin input'
      'x->, parameters a->, b'

  # NOTE:=======================================================================
  # e: is the natural language of calculas (is the math of rate of change and
  # growth and areas)

> [ Auler's Number: ]

    => f(x) = (1 + 1 / n)^n   <|>  e = 1 + 1/1! + 1/2! + 1/3! + 1/4! + ..., etc 

  [   lim(n -> inf) (1 + 1/n)^n = e   ]


===[ Decision Boundary: ]===
* The 'decision boundary' is a concept used in 'classification problems' in 
  machine learning, it refers to the surface (or line in 2D) that 'separates'
  'different classes' predicted by a model.

                          [ Type of Decision Boundaries ]
                                        |
        |-------------------------------|-------------------------------|
        |                                                               |
  [ Linear classifiers ]                            [ Non-linear classifiers ]
* Produce 'straight lines' (or                    * Produce 'curved or'
  planes in higher dimensions)                      'irregular boundaries'
  . Logistic Regression
  . Linear SVM

# NOTE:=========================================================================
# - For logistic regression, decision boundary is where the predicted
#   probability is 0.5

> [ Why It matters: ]
  * Helps 'understand how your model makes predictions' 
  * Helps 'visualize and debug classification models'
  * Determines 'how well the model separates different classes' 
# NOTE:> Dataset are given in the form of the matrix (2D array table) 

- Since Logistic regression model = g(fa->, b(x->))  
                                  = 1 / 1 + e^-(a-> . x-> + b) 

- This rules are true:
  . g(x->) > 0.5 => 1 class is the probability | y-hat = 1 
  . g(x->) < 0.5 => 0 class is the probability | y-hat = 0 

  # NOTE: Y-hat is the predictted value that represent the classfication. 
  > [ Analysis: ]
    * Let is take the formula of logistic regression: 

        g(fa->,b(x->)) = 1 / 1 + e^-(a-> . x-> + b) 

    * At fa->,b(x->) = 0  ==> g(fa->, b(x->)) = 1 / 1 + e^0 = 0.5 
      - from this case we can simply tell:
      if (z = fa->,b(x->) > 0) 
          g(z) > 0.5 
      if (z = fa->,b(x->) > 0) 
          g(z) < 0.5

  # INFO:=======================================================================
  # - Descision boundary is where z = 0 
  #   |=> a-> . x-> + b = 0 (this teh threshold)
  #   |=> resuls a linear boundary that define my boundaries and split dataset
  #       into two classes.
  # - This type of classification is called linear decision boundaries. 


# NOTE:========================================================================= 
# - The sigmoid function transforms the output of the linear model into a bunded
#   range [0, 1]
# - sigmoid function does transform, map, wrap or squashes, (something like
#   normalization or feature scaling), This concept is called 'transforming into
#   a bounded range (in math)' 
# - fa->, b(x->) = 0: defines the 'decision boundary', because it is where the
#   probability is exactly 50% 
# ==============================================================================

> [ Analogy: ]
  * Think of it like this:
    - f(x) = a * x + b is like a ruler, measuring how far a point is
      from the dividing wall. 
    - f(x) = 0 is the wall itself 
    - On one side -> class1: on the other -> class 0

# INFO:
# - The equation f(x) = 0, represents the intercetion part 
# - Type of intersection parts:
#   . 2D => point
#   . 3D => line (intersection between two plans) in case of linear / non-linear
#           it can be curve or irregular function.

# NOTE:=========================================================================
# 1|_ understand f(x) gives a set of values that are the solution of f(x) = 0 
#     f(x) E |R => f(x) = 0 is the middle value f(x) > 0 or f(x) < 0, wich means
#     values above the line f(x) = 0 and values under the line f(x), this topic
#     has splited my space into to main categories one above the line and another
#     one under the line.
# 2|_ The boundaries equation just split the dataset into to main categories 
# 3|_ squatching those values under and above the the boundary equation 
# ==============================================================================
# 3|_ what is the quentity that help us measure the fitting of that line or
#     sigmoid function.
# 4|_ understand the quentity in a ghraphical way 

# QUESTION:[ If we take the same dataset, the fitting line in both cases
# linear regression and logistic classfication would be the same line ?]======== 
# - No, the are 'fitted using different objectives, which leads to different
#   lines' 
# ==============================================================================

===[ Cost function: ]===

# REVISION:=====================================================================
# - Convex function is the function that has the shape of 'bowl shape',
#   (e.g, cost function, x^2, ...) 
# - Logistic regression mode:
#                          1
#   g(fa->, b(x->)) = ------------ 
#                       1 + e^fa->,b(x->)
#   . a->: Controls the 'slop' or 'steepness' of the sigmoid curve. 
#   . b (bias): shifts the sigmoid 'left or right' and controls the 'threshold' 
#     or 'location' of the decision boundary.

* If we applied the 'mean squared error' on logistic regression model. The cost
  function won't be a "convex function" 

  > [ Example: ]

    |
    | *                            *
    |  *     *           *        * 
    |     *    *       *   *    * 
    | local min  *    *       * 
    |               *       local min 
    |           global min
    |--------------------------------

  - In this case the gradient desecnt won't be able to find the global min 
    if the local min is found.

  > [ Loss Function: ]
    * The loss function tells the model 'how far off' its prediction is:
      => lower is better. 
    - It simply tells the model how "wrong it is". 
      _______________________________________________________________
      | Model Type                | Loss Function                   |
      |---------------------------|---------------------------------|
      |-Regression                |-Mean Squared Error (MSE)        |
      |-Binary classification     |-Binary Cross-Entropy (Log Loss) |
      |-Multiclass classification |-Categorical Cross-Entropy       |
      |---------------------------|---------------------------------|

    * In the case of linear regression the loss function 
        [ L = sum(i->n)(yi-hat - yi)^2 ] 

    * In the case of logistic regression is called 'Binary Cross-Entropy'
      - It does use two functions on different intervals:

        /-----------------------------------------------
       | -log(fa->,b(x->))                  |=> if y = 1
    => |
       | -log(1 - fa->,b(x->))              |=> if y = 0
        \-----------------------------------------------

# QUESTION:[ Why applying or wrapping f(x->) by log(x) | function Composition? ] 
# * Function composition is to 'combine two or more functions' so that the ouput
#   of one becomes the input of another.
#   This allows you to build 'complex transformations' from simpler onces and
#   model 'multi-step' process mathematically. 
# * Applyting the logarithm function to another function expression is a common
#   technique in 'mathematics', 'statistics', and 'machine learning' and serves
#   several powerful purposes: 
# 1. Simplifies Multiplication into Addition: 
#     log(a * b) = log(a) + log(b)
# 2. Turns Exponentials grwoth into linear growth 
#    log(e^x) = x
# 3. Makes functions Easier to Optimize 
#    . Log transforms often 'smooth' curves and convexify loss function
# 4. Compress Scale (Handles Large Ranges) 
#    Ex: log(1) = 0, log(10) = 1, log(1000) = 3 
# 5. Interpretable Grwoth (int % terms)

- We already know that '0 < g(fa->, b(x->)) < 1', when we 'applied logarithm'
    on the g(z) probabilty value of class '1' givin by the 'sigmoid function'. 

> [ Analogy: -log(y-hat) <=> Think of logarithms as a "Surprize meter"]
  * It quantifies 'how surprised' the model is when the true label is 1.
  _______________________________________________________________________________
  | Prediction (y-hat)           | -log(y-hat) | Interpretation                 |
  |------------------------------|-------------|--------------------------------|
  | 0.99 (very confident, right) | â‰ˆ 0.01      | Not surprised â€” low penalty âœ… |
  | 0.8 (pretty confident)       | â‰ˆ 0.22      | Mildly surprised ðŸ˜            |
  | 0.5 (uncertain)              | â‰ˆ 0.69      | Not confident â€” medium penalty |
  | 0.1 (confident but wrong)    | â‰ˆ 2.30      | Very surprised â€” big penalty âŒ|
  | 0.01 (very wrong)            | â‰ˆ 4.60      | Shocked â€” huge penalty ðŸ”¥      |
  |______________________________|_____________|________________________________|
  - The scale that are used to make interpretation  
  _________________________________________________
  | Log Loss Value | Interpretation               |
  |----------------|------------------------------|
  | ~0.0 â€“ 0.2     | Very confident and correct âœ…|
  | ~0.3 â€“ 0.7     | Uncertain prediction ðŸ˜      |
  | ~1.0 â€“ 2.0     | Confident but wrong âŒ       |
  | > 3.0          | Very confident and wrong âŒðŸ”¥|
  |________________|______________________________|

> [ Loss function: simplied form ] 

1. coding style
  L(fa->,b(x->), y-hat) = (y == 1) * -log(f(x->)) + (y == 0) * -log(1 - f(x->)) 

2. math style
  [ L(fa->,b(x->), y-hat) = y * -log(f(x->)) + (y - 1) * -log(1 - f(x->)) ] 


  > [ Cost function: ]
    * Ja->, b = sum(i = 1|-> m)(L(f(x), y-hat)) / m 
    _________________________________________________________________
    [                                                               ]
    [   ja->, b = sum(-y*log(f(x->) - (y-1)*log(1 - f(x)) / m       ]
    [           = -(1/m) * sum(ylog(f(x->)) - (y-1)*log(1-f(x))))   ]
    [_______________________________________________________________]

    # NOTE:=======================================================================
    # - The cost function is simple know, like the one in linear regression model.
    # ============================================================================

===[ Overfitting & Underfitting ]===============================================

===[ Overfitting: ]===
* Overfitting occures when a machine 'learning model learns not only th'
  'underlaying patterns' in the training data but also the 'noise and outliers',
  this leads the model to perform very well on the training data but
  'poorly on new, unseen data'

> [ Causes of Overfitting: ]
    1. 'Too Complex models': using models with too many parameters.   
    2. 'Insufficient Training Data': Small datasets can cause the model to
      memorize rather than generalize. 
    3. 'Too Many Training Epochs': Training for too long can cause the model
      to adapt too closely to the training data.
    4. 'Noisy Data': If the training data contains a lot fo 'noise or errors' 
    , the model may learn those instead of general trands

> [ Consequences of Overfitting: ]
  * 'Poor Generalization': the model performs well on training data but fails 
      to predict accurately on validation or test data
    * 'High Variance': Small changes in the input lead to large changes in
      the output.  
    * 'Unreliable Predictions': The model becomes less useful in real-world
      scenarios. 

  > [ Preventing Overfittiong (high varience): ]
    1. 'Simplify the model': use fewer parameters or a less complex algorithm. 
       => You can minimize the number of input featuresfor example
          (feature selection, take a subset instead of the dataset).
    2. 'Collect more Training Data': increasing the dataset size helps the model 
      generalize better.
    2. 'Regularization': 
        . 'L1/L2 Regularization': Penalize large weights. 
        . 'Dropout': Randomly drop neurons during training (in neural networks) 
    4. 'Early Stopping': Stop training when the validation preformance starts
       to degrade. 
    5. 'Cross-Validation' 
    6. 'Data augmentation': creation in feature engineering

> [ Regularization: ]
  * Changing the paramters of input features without eleminating the features  
    we can simply say 'we change the weight of input feature'

" The goal of regularization is to 'penalize model complexity' by discouraging"
"overly complex models with large weights, By doing so, it helps the model to"
"generalize better to new, unseen data"

  # INFO:===================================================
  # - Penalize: (verb) to cause someone a disadvantage.
  #   > Ex: The presetn tax system penlizes poor people. 
  # ========================================================

> [ Modified cost function: ]

  - Loss function in case of L1 regularization (lasso).
      [ Loss = Original Loss + lambda * sum(wi^2) ]
              --------------   --------------------
                'MSE term'    'Regularization term'

      # INFO:===================================================================
      # - Î» (lambda) scales the penalty on the weights, and smaller weights tend 
      #   to produce smoother curves (especially in linear and polynomial models)
      # - â€œThe weights are scaled by lambda, is that why the curve stays smoothâ€
      # - Without Regularization (lambda = 0): 
      #   . The model will try to 'perfectly fit' the training data.
      #   . This can lead to 'large weights', especially in high-degree
      #     polynomials or complex models. 
      #   . Result: the curve 'wiggles' through every data point -> 'not smooth'
      #     -> 'overfittiong'
      # - Wit regularization (lambda > 0):
      #   . The model is 'penalized' for large weights. 
      #   . To 'minimize loss', it prefers 'smaller weights' even if the error  
      #     on training data is slightly higher.
      #   . Resul: the fitted curve is 'smoother', with
      #     'fewer wild oscillations'  

      - 'Mean squared Term': it does minimize the 'residuals' for the making 
                             the best fittiong.
      - 'Regularization term': try to keeps 'Wi' wieghts too small 
      - 'lambda': Is the 'regular parameter', the ai engineer is the one that
                  select its value > 0, It balances both goals.

# QUESTION:===================================================================== 
# * Why not just manually scale the weights to be smaller and then use the
#   original loss (e.g MSE) ? Why do we need to change the loss function 
#   itself ?
# SOLUTION:=====================================================================
# - You want the training process to learn both: 
#   . How to 'minimize prediction error'
#   . How to avoid complexity ()
#  > You do this by creating 'a single loss function' that combines both goals:
#    [ Loss = MSE + Î»â‹…Penalty ]
#   ... This way, the optimizer balances both during training, not as 
#       post-processing step.
#  > [ Analogy: ]
#   * Imagine teaching a robot to walk:
#     . Without regularization: just tell it to walk fast (minimize time) 
#     . With regularization: you also penalize steps that are too wide 
#       (risk of failing).
#   * If you don't 'build that penalty into its goal function', it won't
#     learn safe walking, It will just go fast and fall over.
# CONCLUSION:=================================================================== 
# - Wahed a, b ghadi i3tiwni min cost function, ms had l min ghadi i3tini
#   wahed l model li t3alem bzff, o hna fin endi l mochkil l model makay3tinich
#   'better generalization, key 3tini poor generalization' lhal howa dik l min
#   nziid elih wahed term li ghadi ykhalini newssal l nefss l min walakin
#   bi w-> and b kbar mn l fi lcase lowla.
# - o had lqadiya katkhali l model dyali t3alem walkin endi wahed marge dyal
#   error o hadchii howa li kaykhali l model dyali endo 'better generalization'
#   or unseen data.
# - 'Regularization slightly increase training' on purpose to achieve a much
#   'lower test MSE' making the model more reliable on unseen data
# QUESTION:[ Why can't we just add a fixed value 'lambda' to cost function to 
# increase the error margin and avoid overfittiong ?]===========================
# - We can't just add a constant to the loss Î» function because it has no 
#   effect on the optimization process, it does not change the model,
#   the weights, or the predictions.
#   (Constant value, it is derivative is 0, it will not effect the new weights
#   and bias in using the gradient descent algorithm)
# - Regularization must 'depend on the model  parameters' to actually influence
#   training and reduce overfitting.
# ==============================================================================

==[ Summary: ]==================================================================
  - Regularization is a core strategy for controlling model complexity, reducing
    overfittiong, and improving preformance on test data by modifying the loss
    function to penalize large or unnecessary weights.
  - Without regularization: 
    . You are finding the best fit to 'training data' (low bias, high variance)
  - With regularization: 
    . You are finding a balance, 'good fit' to training data, but with 'better' 
      'generalization' to unseen data (prediction).
  - Regularization "doesn't just increase the loss value, It changes how the"
    "loss is minimized", and that 'directly affects the final values of the'
    'weights and bias'
  > [ Analogy: ]
    * Imagine fitting a line to noisy (outlier) data.
      . Without regularization, you might bend the line too much to touch
        every point. 
      . With regularization, you allow some error but keep the line
        'reasonable and smooth', so it works better for future data. 
================================================================================

> [ Regularization With Gradient Descent: ]
  - Cost function with regularization:

    [----------------------------------------------------------------]
    [                  1                                             ]
    [  Jw->, b(x->) = ---- * (âˆ‘ (fw->,b(x->) - yi)^2 + Î» * âˆ‘ (Wi)^2) ]
    [                  2m                                            ]
    [----------------------------------------------------------------] 

  - Gradient Descent formual:

       /                   &
      | Wi = Wi - Alpha * ---- J(W->, b) 
   /|=|                    &Wi
   \|=|                  &
      | b = b - Alpha * ---- J(W->, b) 
       \                 &b

       /                     1                          lambda
      | Wi = Wi - Alpha * [ --- sum (f(x->) - y) * x] + ------ * wj ] 
   /|=|                      m                             m 
   \|=|                    1 
      | b = b - Alpha * [ --- sum(f(x->) - y) ]   => Still the same
       \                   m 

       /                        lambda              J(w->, b) 
      | Wi = Wi * (1 - alpha * -------) - ( alpha * ---------- ) 
   /|=|      ---------------------m---     -------------&w-------------- 
  |           Shrink 'wi' a little bit     The same case where lambda = 0 
   \|=|                    1 
      | b = b - Alpha * [ --- sum(f(x->) - y) ]   => Still the same
       \                   m 

===[ ML: Decision Trees ]=======================================================

===[ Index ]======================================
1|> Decision Trees 
2|> Random Forest
3|> Gradient Boosting
4|> Extreme Gradient Boosting

===[ XGboost Model ]===
# REVISION:---------------------------------------------------------------------
          |-------------------[ Time Series Model ]-------------------|
          |                                                           |
[ Time Series Traditional Model ]                           [ ML Models ]
          |                                                 - Regression with  
  |-------|-------------------------|                         lagged feature 
  |                                 |                       - Tree-based models 
[ Univariate Models ]           [ Multi-variate Model ]       (Random Forest, 
- ARIMA model                   - Vector Autoregression       (XGBoost, trained
- SARIMA (season ARIMA Model)     Model                       on time lagged 
  . It takes in the account                                   features.
    seasonality                                              - Neural Networks
- Prophet (suitable for people and passengers)               - Hybrid models  
- Neural Prophet (neural network version of 'prophet')        combine ARIMA +
                                                              ML for residual
                                                              modeling.

[1]|> ===[ Decision Trees ]===

- A 'Decision Tree' is just a 'series of `if-then` rules' that split data into
  smaller and smaller groups until each group is as `pure` (similar) as possible
  
  # INFO:[ Analogy ]------------------------------------------------------------
  # - Think of it like how a person makes decisions step by step:
  #   . "Is it raining?" -> is yes, take an umbrella
  #   . if no -> "Is it cloudy" -> Maybe bring a light jacket
  # -> That's exactly how a `Decision Tree` works, it keeps asking questions
  #   that best separate the data.

  - A Decision Tree looks like this:

               [Root Node: Is temperature > 25Â°C?]            | Root Node
                        /                     \
                     Yes                       No
            [Is humidity > 60%?]         [Is windy?]          | Decision Nodes
                /         \                 /      \
             Yes          No            Yes        No
           "No"          "Yes"        "No"        "Yes"       | Leafs Nodes

      - 'Root Node': The first and most important question 
      - 'Branches': possible answers ('yes', 'No', '< value', '> value') 
      - 'Leaves': final decisions or predictions

  # QUESTION:[ What the tree is trying to do? ]--------------------------------|
  # - At each step, the tree tries to ask the 'most informative question'      |
  #   possible, the one that 'best split' the data into groups that are more   | 
  #   consistent (or 'pure') in terms of the target variable.                  |
  # - Example:                                                                 |
  #   If we are predicting whether someone will 'buy ice cream', a good first  |
  #   question might be 'is the temperature > 25C?' because that seperates     | 
  #   'likely yes' vs 'likely no' quite well.                                  |
  # QUESTION:[ How the tree chooses questions? (splits) ]----------------------|
  # The algorithm test every feature (e.g: temerature, humidity, age, etc.) and|
  # finds the 'split that gives the biggest improvement' in purity.            |
  # - Fore Classification:                                                     |
  #   . [ Gini Impurity ] - measures how mixed a group is.                     |
  #   . [ Entropy (Information Gain) ] - measures disorder (borrowed from      |
  #     infromation theory).                                                   |
  # - For regression:                                                          |
  #   . Uses 'variance reduction' (how much the split reduces the spread of    |
  #     values).                                                               |
  # > The tree chooses the question that makes the children nodes more 'pure'  |
  #   than the parent node.                                                    |
  # QUESTION:[ How the tree grows? ]-------------------------------------------|
  # - 'It keeps splitting' the data recursively, each time choosing the best   | 
  #   question.                                                                |
  # - It 'stops' when:                                                         |
  #   . The group is pure enough (e.g: all `yes` or all `No`) or               |
  #   . The tree reaches a max depth, or                                       |
  #   . There's too little data left to split meaningfully.                    | 
  # ---------------------------------------------------------------------------|

# ===[ NOTES ]===
- Decision Tree is a 'supervised machine learning algorithm' used for both 
  'classification' and 'regression' tasks.
  - It works by 'splitting data into branches' based on 'conditions of the'
    'input features'
  > essentially forming a 'tree-like structure of if-then rules'.

> [ Linear Regression & Logistic regression VS Decision tree ]===
  _____________________________________________________________________________
  | Aspect        | Linear Regression | Logistic Regression | Decision Tree   |
  |---------------|-------------------|---------------------|-----------------|
  | Type of Model | Parametric        | Parametric          | Non-parametric  |
  |---------------|-------------------|---------------------|-----------------|
  | Prediction    | Linear mathematical Logistic (sigmoid)  | Hierarchical    |
  | Basis         | equation          | function            | ifâ€“then rules   |
  |---------------|-------------------|---------------------|-----------------|
  | Assumes       | Linear            | Linear in log-odds  | Non-linear and  |
  | Relationships | (straight-line)   |                     | flexible        | 
  | Are           |                   |                     |                 |
  |---------------|-------------------|---------------------|-----------------|
  | Interpret-    | Coefficients show | Coefficients show   | Very intuitive  |
  | ability       | feature impact    | impact on log-odds  | (visual,        | 
  |               |                   |                     |     rule-based) |
  |---------------|-------------------|---------------------|-----------------|
  | Data Handling | Requires scaling, | Similar to linear   | Handles both    | 
  |               |                   |                     | numeric &       | 
  |               | handles numeric   | regression          | categorical data| 
  |               | data well         |                     | easily          |
  |---------------|-------------------|---------------------|-----------------|
  | Overfitting   | Moderate          | Moderate            | High (unless    |
  |     Risk      |                   |                     | pruned or       |
  |               |                   |                     | regularized)    |
  |_______________|___________________|_____________________|_________________|

  - 'Regression models': (linear, logistic) learn 'mathematical equations' that
    fit data:
            [   y = w1 * x1 + w2 * x2 + ... + b   ]
    . They find the 'best-fitting line (or curve)' by optimizing coefficients 
      to minimize error.
  - 'Decision trees', on the other hand, do 'no equation-fitting', they
    repeatedly 'partition the data' based on feature thresholds that maximize
    '<purity>' (using metrics like 'Gini impurity' or 'information gain').
  >> The final prediction is based on 'majority vote (classification)' or
     'average value (regression)' of the samples in a leaf node
 
> [ Strenghts of Decision Trees ]
  - Easy To 'visualize and interpret' ('I can see the rules!')
  - Can handle 'numeric and categorical' data
  - Requires 'little preprocessing' (no scaling or normalization)
  - works for both 'classification and regression'.
  - can model 'nonlinear relationships' easily

# NOTE:------------------------------------------------------------------------
# - All boosting and bagging methods (including Random Forest and XGBoost) are
#   build on decision trees.

===[ Decision Tree Learning ]===
* There are 'two main questions' must be asked before building any decision tree:
1|> How to choose what feature to split on at each node (root/decision node) ?
      - The general rule says: "shoose the feature that maximize the purity"
        "or minimize the umpurity?"

  > [ Purity & Impurity ]
    - When a decision tree 'splits' data, it tries to create groups (branches)
      that are as 'pure as possible' meaning, the examples inside each group are
      'mostly of one kind'.
    
    # INFO:[ ANALOGY ]==========================================================
    # > Imagine you want to group similar items together, all apples in one
    #   basket, all oranges in another.
    #   - If a basket has only apples, it's 'pure'
    #   - If it has a mix of apples and orages, it's impure.
    # >> The goal of the decision tree is 'to find questions (splits)" that make
    #    these groups as pure as possible.
    
    - At each step, the decision tree asks:
        [ Which feature (question) best splits my data into groups that are more
          a like inside themselves? ]
    - If the split results in:
      . Groups with mostly one label -> 'high purity'
      . Groups with mixed lables 'low purity'

    > For example:
      - 'Question': is the fruit color = red ?
        . Yes -> 90% apples 
        . No  -> 80% oranges
        -> Great! That's a pretty pure split.

    # NOTE:[ VERY IMPORTANT ]===================================================
    # - Purity tells the decision tree 'how good its splits are'
    # - The tree wants each final leaf (end node) to be as pure as possible
    #   because:
    #   . Pure leaves -> clear, confident predictions
    #   . Impure leaves -> uncertain predictions (the tree is not sure) 
    # - In training, the tree measures impurity using math (like 'Gini impurity'
    #   or 'entropy'), but conceptually, it's all about 'how mixed' or 'cleanly'
    #   'separated' the data is.

2|> When do you stop splitting?
  - When a node is 100% one class
  - When splitting a node will result in the tree exceeding a maximum depth
  - When improvements in 'purity score' are 'below a threshold' 
  - When 'number of examples in' a node is below a 'threshold'

===[ Measuring Purity: Entropy & Information Gain ]============================
# REVISION:---------------------------------------------------------------
# - Purity is about how similar the items in a group are after a split 
#   . A 'pure' group means all items inside it belong to the same category
#   . An 'impure' group has a mix
# > So the goal from decision tree is to keep splitting the data until 
#   the groups are as pure as possible.


                |===[ How to Mesaure Purity ]===|
                |                               |
      [ Gini Impurity ]                     [ Entropy (measuring impurity: messness)]
- Think of it as the chance           - 'Entropy', Similar idea, but  
  of picking two different              it measures disorder 
  items from the same group             . All one type = low disorder(pure)

  # NOTE:=================================================================
  # - Don't worry about the math, both are just ways to score how mixed or 
  #   unmixed a group is. 
  # - When building a decistion tree, the algorithm looks for the question 
  #   (or 'split') that makes the resulting groups 'more pure' than before

===[ Entropy ]===
* 'Entropy' is just a way to 'measure how mixed or uncertain' a group of data is
  - If a group has 'only one kind of thing', its "pure -> low entropy (or zero)"
  - If it has 'a mix of different kinds', its 'impure -> high entropy'

  # INFO:[ Uncertain ]==========================================================
  # > `Uncertain`: (adj), not knowing what to do or believe, or not able to
  #                decide about something.
  # ============================================================================

# INFO:[ Analogy ]===============================
# * Think of entropy as a 'messiness' score:    |
#   . 'More mixed' = more messy = higher entropy|
#   . 'More uniform' = cleaner = lower entropy  |
# ===============================================

- The basic fomula for Entropy: 

    Entropy = - Sum (pi * log2(pi))

    . 'pi': the proportion (probability) of each class in the group.

> [ Why using log base 2: (short answer: get the peak of H(pi) == 1) ]
  - Entropy comes from information theory The idea of entropy comes from
    "Claude Shannon's infromation Theory" (1948) 
  - They wanted a way to measure how much 'information' or 'uncertainty' is in a
    message or dataset. 
  - In information theory, the 'amount of information' in a event depends on how
    surprising it is. 

    > if something is 'very predictable', it carries 'little information'.
      ex: "the sun rieses in the east" -> not surprising
    > if something is 'unlikely', it carries 'more information' 
      ex: "it will snow in the desert today" -> surprising!

  Shannon defined 'information' as:

    [ Information(x) = - log2(p(x)) ]

  - That's why "logarithms" come into play, they translate probabilities
    (like 0,5, 0,25, etc) into "information content".

# NOTE:[ VERY IMPORTANT ]=======================================================
# - log of base 2 is used to get the pick of 1 (100%) instead of another base 
#   . Suppose we used `ln (log of base 10)` we will get the same curve but
#     with another 'peak != 1'.
# ==============================================================================

# QUESTION:[ Why specifically log base 2 ]======================================
# - Because we measure information in 'bits', the basic unit of digital
#   information. 
#   . Each 'bit represents' a (yes/no | 1 or 0) question, like flipping a coin
#     or answering a binray question.
# - So using 'log base 2' means we are measuring:
#   "How many binary questions (yes/no decisions) would it take to identify"
#   "the correct class?"
# - For Example:
#   . if p = 0.5, then -log2(0.5) = 1bit -> one yes/no question
#   . if p = 0.25, then -log2(0.25) = 2 bits -> two questions (more uncertainty)
# > That's why 'base 2' makes sense, decision trees make binray decisions at
# each step (`yes` or `no`), just like bits!.
# INFO:[ EXAMPLE ]==============================================================
# - Imagine guessing what fruit someone picked:
# - If there are 2 equally likely fruits (apple or orange), it takes about 1
#   yes/no question (â€œIs it an apple?â€).
# - If there are 4 equally likely fruits (apple, orange, banana, mango), it
#   might take about 2 yes/no questions (â€œIs it an apple?â€ â†’ No â†’ â€œIs it an
#   orange?â€ â†’ etc.)
# - So the log base 2 gives a measure of how many yes/no questions are needed,
#   which fits perfectly with how decision trees split data!
# ____________________________________________________________________________
# | Concept     | Meaning                                                    |
# |-------------|------------------------------------------------------------|
# | Logarithm   | Turns probabilities into â€œinformation content.â€            |
# | Base 2      | Measures that information in bits (yes/no decisions).      |
# | Why use it? | Because decision trees are made of binary splits,          |
# |             | itâ€™s a natural match!                                      |
# |_____________|____________________________________________________________|
# - So, we use logâ‚‚ because entropy is all about measuring uncertainty in bits,
#   and decision trees reduce that uncertainty step by step â€” like answering a
#   series of yes/no questions.
# - Entropy measures how missness the group after splitting, the general formula 
#     [ entropy = -sum(log2(pi)) ]
# - log2(pi) measures the uncertainty in a group (or how many questions/decisions
#   of yes/no)
# ==============================================================================

> [ Information Gain: ]
  - 'Information Gain': measures 'how much impurity goes down' when we split the
    data based on a certain feature (question)

  > Example:
    - If i split the candies by color, how much cleaner or more organized did my
      boxes become?
      > If the split makes the boxes very pure (each box mostly one color)
        ===> [ We gain a lot of information ]
      > If the split does not help much (still mixed colors)
        ===> [ We gain little information ]
 
    # INFO:[ CONCLUSION ]=========================================================
    #  - INFORMATION GAIN = How much we improve the purity of our datat when we  |
    #    split it using a feature                                                |
    #  - INFORMATION GAIN = How much 'impurity' decreased 'after' we split       | 
    # ============================================================================

  > [ Exmaple ]
    - Suppose that we have a group of '5 cats' and '5 not cats'
    1. measuring the 'impurity' of this group

      H(5 / 10) = 1.0 

    2. suppose that we have 3 features to split the group and used as decision 
       node ('Ear Sahpe', 'Face Shape', 'Whiskers').

      - each features split the group into two groups, we can measure the impurity
        of each group
        [ h1, h1 ]

      - Calculate the wighted average:

      W(left) = (grp_count/10) * h1(left)  
      W(right) = (grp_count/10) * h1(right)

      W(splited) = W(left) + W(right) 

      - Calculating the information gain

    [ IG = (Impurity Before Split) - (Weighted Average impurity after split) ]

    [ IG = P1(root) - (W(left) * H(p1(left)) + W(right) * H(p1(right))) ]
      
      # INFO:-------------------------------------
      # Information Gain == Reduction of Entropy |
      # ------------------------------------------ 

# WARNING:[ You need to Try to build Decision Trees Manually ]==================
# DO NOT FORGET to build some simple Decision Trees using pen & paper  

===[ One Hot encoding of categorical features ]===
  * 'One-hot encoding': turns categorical features into 'binary (0/1) features'
    so algorithms, including decision trees can 'use them for split' and keep
    it a 'binary decision tree'.
    - If a 'categorical feature' can take on 'k values', create 'k binary'
      'features' with (0 or 1 values)
    - It is named 'one-hot encoding' because at 'any sample' you will find only
      'one feature' that has the value '1' ('only one feature that is hot').

  # INFO:[ VERY IMPORTANT ]=============================
  # One-Hot Encoding can be used with 'decision trees'
  # and neurtal networks also.

===[ Decision Trees: Continuous Feature ]===
# REVISION:[ Decision Making & Tree Splitting ]===================================
# - As we Already know choosing the decision node, depends on the value of
#   the Information Gain
#   . Categorical Feature: 'Is it '<feature>' ?'
#   . Numerical[Continuous] Features: 'Is it '<threshold>' ?'

  - 'First thing first': you have to sort all the 'quantitive features' and
    calculate the information gain of the 'threshold' between every 'two'
    'cases/sample' and choose the 'feature' that has the 'biggest information' 
    'Gain threshold'.

  > [ Example ]
    suppose that we have this feature

    x = [1, 5, 6, 7, 9, -1]
    
    1. calculate the information gain at threshold = 1 
    2. calculate the information gain at threshold = 5 
    3. calculate the information gain at threshold = 6 
    4. calculate the information gain at threshold = 7 
    5. calculate the information gain at threshold = 9 
    6. calculate the information gain at threshold = -1 

  - The decision node would be the question with the threshold that 
    has the biggest value of the ['information Gain']

    [ is <= y?  where   IG = H[p(root)] - [ W(pleft) + W(pright)] ]

  
===[ Decision Trees: Regression Trees ]===
* The final prediction is based:
  - [Classification]: 'majority vote' > of the samples in a leaf node
  - [Regression]: 'average value' > of the samples in a leaf node

-> 'Classification Trees' minimize the 'impurity' using entropy
    [ information gain = entropy(root) - Weighted average entropy ]

-> 'Regression Trees' minimize the 'variance' insted of using 'Entropy'
    we use the statistical quantity 'variance'. 
    [ information gain = entropy(root) - Weighted average variance ] 

===[ Ensemble Learning: Tree Ensemble ]===
* In 'Machine Learning', a 'tree ensemble' is a group of decision trees working
  together to 'make predictions', A 'decision tree' is a 'if-then model' that
  splits data into smaller and smaller subsets based on certain features.
  - One 'single decision tree' can sometimes be too simple and make errors, 
    'trees' are to 'sensitive to changes'.
    > Example: changing one 'sample in the training set' can changes the whole
               'decision tree design structure'. 
  -> This is where 'ensemble methods' comes in, they 'combine multiple decision' 
     'trees to make a stronger, more reliable prediction'.
     but, "How can create different decision trees structures?"
     "(sampling with replacement)"

    |---------------[ Types of Tree Ensemble algorithms ]---------------|
    |                   |                           |                   |
    |                   |                           |                   |
[ AdaBoost ]  [ Gradient Boosting ]           [  XGBoost ]    [ Random Forest ]
- Adaptive
  Boosting
- the first
  model

Decision Tree -> Bagging -> Random Forest -> Boosting -> Gradient Boosting -> XGBoost   

# QUESTION:[ Why Are Tree Ensemble Methods Important? ]=========================
# |> Better Accuracy:                                                          |
#    Tree ensembles tend to be much more accurate than a single decision tree. |
#    By combining the power of multiple trees, the model becomes much better at|
#    finding patterns and making predictions.                                  |
# |> Reduced Overfitting:                                                      |
#    A single decision tree might fit the data too perfectly (overfitting),    |
#    capturing random noise as if it were a real pattern. Since tree ensembles |
#    take multiple trees into account, they can generalize better and avoid    |
#    this issue.                                                               |
# |> Handles Complex Data:                                                     |
#    Tree ensembles can work with both numerical and categorical data, and they|
#    handle missing data well. Theyâ€™re also very flexible in modeling complex  |
#    relationships, making them useful for a wide range of real-world problems.|
# |> Feature Importance:                                                       |
#     Tree ensembles can also tell us which features (or attributes) are the   |
#     most important for making predictions. This can be really helpful when   |
#     trying to understand the data and make decisions based on it.            |
# |> Stability:                                                                |
#     With a variety of trees making predictions, tree ensembles are less      |
#     sensitive to small changes in the data. This makes them more stable and  |
#     reliable than a single tree.                                             |
# ==============================================================================


===[ Sampling with replacement: (statistical method) ]===
* 'Sampling with replacement' means that when you select a data point (an
  observation or row from your dataset) to be part of your sample, you put it
  back into the pool before selecting the next one.
  -> [ This allows the same data point to be selected multiple times ]

  > [ Example ]
    - Suppose that we have a 'feature' that 'contains 10 cases', we won't create
      a 'single decision tree' for this 'feature', instead we are going to use
      'tree ensembling' and create a 'group of dicition trees' using a
      'statistical method sampling && replacing'.
      . 'Statistically' and 'mathimatically' we have '10 power 10 different' 
        'cases'. [ should we create a decision tree for each case from this
        combinision? ]
      . 'Answer': Nop, 'because it would be large and slow to create a decision'
        'for each combinision which effect the performance of the model',
        instead we take between '64 and 228 tree (100 is recommeded)'.

                          [ ensemble learning ]
      |--------------------[ Tree ensembling ]--------------------|
      |                                                           |
      |                                                           |
[ Bagging-Based ensembles ]                     [ Boosting-Based ensembles ]
-> Parallel ensembling                          -> Sequential ensembling 
- Random Forest                                 - XGBoost, LightGBM, AdaBoost  
> Train trees independently                     > Train trees sequentially, 
  on random subsets of data                       each focusing on correcting
  , then aggregate (average                       the previous ones 'mistakes'
  or majority vote)

# INFO:[ CONCLUSION ]-----------------------------------------------------------
# tree emsembling can refer to any model that combines multiple trees, including
# random forests and boosting methods.

===[ Bagging: Random Forest algorithm ]===
* 'Bagging': bootstrap aggregation means ensembling a group of weak learner (
  decision trees) in parallel way, and final prediction is the result of 
  all prediction made by weak learner (classification -> voting, regression ->
  avereging)
  - Train the models independently

Random Forest ~= Bagging (Bootstrap <sampling with replacement> + Aggregation)
                                      +
                 random feature selection at each split 

# INFO:[ Boostrap Meaning ]---------------------------------------
# ________________________________________________________________
# | Term             | Meaning                                   |
# |------------------|-------------------------------------------|
# | Sampling         | Picking random examples from data         |
# | With replacement | You can pick the same item more than once |
# | Bootstrap sample | New dataset created this way              |
# |__________________|___________________________________________|

1|> [ But keep in mind: ]
    . <Bagging>: alone (without random features) -> creats an 'ensemble of'
      'decision trees' (called bagged trees).
    . Adding <random feature selection> -> creats the 'random forest' algorithm 

2|> [ 'Bagging' = 'Bootstrap AGGregatING' ]
    - So when you hear 'bagged trees', it literally means:
      [ "Decision Trees that have been trained using the bagging technique" ]
      [ "Bagged Trees = bootstrap sampling + aggregation"]

3|> [ What Bagging Does: ] 
    -  Bootstrap (sampling with replacement)
      . You randomly sample your training data with replacement to create
        multiple datasets
      . Each dataset (called a bootstrap sample) is used to train a separate 
        decision tree

    - Aggregation (averaging or voting)
      . For regression: take the 'average' of all trees predictions
      . For classification: take the 'majority vote'

===[ Boosting (general concept) ]===
* Boosting is a general ensemble learning framework â€” a family of algorithms
  that combine multiple weak learners (usually decision trees or stumps) into
  a single 'strong learner'.
  - 'The idea': 'train models sequentially', where each new model focuses on the
    mistakes of the previous ones.

    . Each weak learner is weighted based on its performance.
    . Final prediction is a weighted vote (classification) or weighted sum
      (regression).

-> There are many types of 'boosting algorithms', 'adaboost', 'Gradient boost'
   and 

===[ Boosting: AdaBoost Algorithm (adaptive boosting) ]=== 
* AdaBoost (short for Adaptive Boosting) is the first and most classic
  boosting algorithm, proposed by Freund and Schapire (1996).
  - Trees in 'adaptive boost' are useually 'one root node' and 'two leaves' it
    is called 'Stump'. 
  -> So this model is a 'forest of stumps' unlike 'random forest'
    '(forest of trees)'.
  -> 'Random forest' take advantage from all features to build decision trees. 

  # INFO:-----------------------------------------------------------------------
  # - Stump: (noun) the short part of something  that is left after most of it |
  #   has been removed.                                                        | 
  # ----------------------------------------------------------------------------
 
  - A 'Stump' use only 'one variable/feature' to make a decition tree, example:

                      [ sick ]
                      /      \
                    yes       no

  - 'Stumps' are technically a 'weak learners', however this is the way adaboost
    likes it, and it is one of the reasons they are so commonly combined.

  # INFO:[ AdaBoost VS Random Forest ]========================================== 
  # 1. Random forest use a forest of dicision trees that built using different |
  #    subsets of random features and samples, otheriwse AdaBoost use a forest | 
  #    of stumps, where each stump built using only one feature with makes it  |
  #    a weak learners.                                                        |
  # 2. In random forest the trees hase equal weight in voting otherwise in     |
  #    others adaboost some stumps get more say in the final classification    | 
  #    than the larger stumps get more say in the final classification than    |
  #    the smaller stumps.                                                     |
  # 3. In Random Forest Each dicision tree is made independently from the      |
  #    others, it does not matter wich one has been created first, A forest of | 
  #    stumps made by AdaBoost the order is important, the error made by the   |
  #    first stump influence how the second stump is made.                     | 
  # ============================================================================

  # QUESTION:[ How to define how much say a stump has in the calssification ]=== 
  # - We determine how much say a stump hsa in the final classification based on 
  #   how well it classified the samples. 

  # INFO:[ ANALOGY ]------------------------------------------------------------
  # Imagine a teacher giving more attention to students who failed to last     |
  # quiz each round, focusing on mistakes until everyone understands.          |
  # ----------------------------------------------------------------------------

===[ Boosting: Gradient Boosting Algorithm (GBM) ]===
* 'Gradient Boosting' is one of the most powerful ideas in modern machine
  learning, It is a 'tree ensemble method' trees (built in parallel),
  like random forest, but it builds trees in a 'very different way'.
  - 'Random Forest' = many 'independent' decision trees (built in prallel)
  - 'Gradient Boosting' = many 'dependent' trees (built on 'after another')
    -> Each new tree tries to 'fix the mistakes' made by the previous ones.

  - Sevenral Famous Algorithm are built on the top of Gradient boosting
    algorithm:

    |---------------------[ Gradient Boosting ]---------------------|
    | - XGboost (Extream Gradient Boosting)                         |
    | - LightGBM (Light Gradient Boosting Machine)                  | 
    | - CatBoost (Categorical Boosting)                             | 
    |_______________________________________________________________|

> [ The Core Idea (In Words) ]
  - Imagine you are trying to predict someone's "height" based on their "age" 
    1|> You start with a 'simple guess', maybe the average height of everyone
        -> you are probably wrong for many people

    2|> You look at the 'error (residulas)', the difference between your guess
        and the real values.
        -> For example, if someone is 180c m and your guess was 170 cm, the
           'error' is '+10'

    3|> You build a 'small tree' that tries to predict those 'errors'

    4|> You 'add' this correction to your old predictions
        -> Now you guesses are a bit closer to the truth
      
    5|> You repeat this process many times, each new tree fixes what's left
        wrong

    -> That is 'boosting', you keep 'boosting' your model's accuracy by "adding"
       "trees that correct errors".

  > The 'Gradient' part:
    - The model uses 'gradient descent' (like a neural networks) to minimize
      the loss function

    - Each new tree is trained to predict the 'negative gradient' of the loss
      function - which basically means: "What direction should i move to"
      "reduce my mistakes fastest?"

      . The 'gradient' tells you the direction of the biggest error
      . The 'boosting' means you move step by step in that direction using
        trees

> [ Components of Gradient Boosting: ]
  - 'Base Learner': Usually a shallow decision tree (called a weak learner)
  - 'Loss Function': Measures how wrong the model is (e.g., MSE, log loss)
  - 'Learning Rate (shrinkage)': Controls how much each tree contributes
                                 (small = safer, slower)
  - 'Number of Trees': More trees = more accurate, but risk of overfitting 

===[ Boosting: XGboost (Extreme Gradient Boosting Algorithm) ]===
- 'The university of washiton' that has developed the 'XGBoost algorithm'
  in 2016.

====[ Conclusion ]=============================================================
}> Bagging = parallel, independent learners (variance reduction).
}> Boosting = sequential, dependent learners (bias reduction).
}> Decision Tree -> Bagging -> Random Forest -> Boosting -> Gradient Boosting -> XGBoost   
