#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£§‚£¶‚£¥‚£∂‚£æ‚£ø‚£∂‚£∂‚£∂‚£∂‚£¶‚£§‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢†‚°∂‚†ª‚†õ‚†ü‚†ã‚†â‚†Ä‚†à‚†§‚†¥‚†∂‚†∂‚¢æ‚£ø‚£ø‚£ø‚£∑‚£¶‚†Ñ‚†Ä‚†Ä‚†Ä                  ìêì  Pandas ìêî           
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚†î‚†ã‚†Ä‚†Ä‚†§‚†í‚†í‚¢≤‚†Ä‚†Ä‚†Ä‚¢Ä‚£†‚£§‚£§‚£¨‚£Ω‚£ø‚£ø‚£ø‚£∑‚£Ñ‚†Ä‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚£Ä‚£é‚¢§‚£∂‚£æ‚†Ö‚†Ä‚†Ä‚¢Ä‚°§‚†è‚†Ä‚†Ä‚†Ä‚††‚£Ñ‚£à‚°ô‚†ª‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚†Ä       Dev: oezzaou <oussama.ezzaou@gmail.com> 
#  ‚¢Ä‚†î‚†â‚†Ä‚†ä‚†ø‚†ø‚£ø‚†Ç‚††‚†¢‚£§‚†§‚£§‚£º‚£ø‚£∂‚£∂‚£§‚£ù‚£ª‚£∑‚£¶‚£ç‚°ª‚£ø‚£ø‚£ø‚£ø‚°Ä                                              
#  ‚¢æ‚£æ‚£Ü‚£§‚£§‚£Ñ‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†â‚¢ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á                                              
#  ‚†Ä‚†à‚¢ã‚¢π‚†ã‚†â‚†ô‚¢¶‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£º‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á       Created: 2025/06/20 12:43:36 by oezzaou
#  ‚†Ä‚†Ä‚†Ä‚†ë‚†Ä‚†Ä‚†Ä‚†à‚°á‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†á       Updated: 2025/07/02 00:39:51 by oezzaou
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°á‚†Ä‚†Ä‚¢Ä‚£æ‚£ø‚£ø‚†ø‚†ü‚†õ‚†ã‚†õ‚¢ø‚£ø‚£ø‚†ª‚£ø‚£ø‚£ø‚£ø‚°ø‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚†á‚†Ä‚¢†‚£ø‚£ü‚£≠‚£§‚£∂‚£¶‚£Ñ‚°Ä‚†Ä‚†Ä‚†à‚†ª‚†Ä‚†ò‚£ø‚£ø‚£ø‚†á‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†±‚†§‚†ä‚†Ä‚¢Ä‚£ø‚°ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚£ø‚†è‚†Ä‚†Ä                             ìÜ©‚ôïìÜ™      
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°Ñ‚†Ä‚†Ä‚†Ä‚†ò‚¢ß‚°Ä‚†Ä‚†Ä‚†∏‚£ø‚£ø‚£ø‚†ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ê‚†ã‚†Ä‚†Ä‚†Ä                     ìÑÇ oussama ezzaouìÜÉ  
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚†Ñ‚£Ä‚°Ä‚†∏‚†ì‚†Ä‚†Ä‚†Ä‚††‚†ü‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä                                              

# ===[ General Information: ]===================================================
* In 'Data Science before training a model', The data typically goes through
  'serveral key stages' in a pipeline, These steps are crucial for ensuring the
  model receives 'clean', 'structured', and 'meaningful data':

  1|> [ Data Collection: ] 
      * 'Gather' data from different sources: 
        - Databases(SQL, NOSQL)
        - APIs
        - CSV/Excel Files
        - Web scraping
        - Sensors, logs, etc

  2|> [ Data Cleaning: ] 
      * 'Fix or remove, corrupted, duplicate, or missing data': 
        - Handle 'missing' values (e.g, file, drop, interpolate)
        - Remove 'duplicates'
        - Correct insconsistent formatting
        - Handle 'outliers' or 'noise'
        - 'Normalize' inconsistent units 

  3|> [ Data Transformation: (Preprocessing) ]
      * 'Convert data' into a 'suitable format':
        - 'Encoding': categorical varibales (e.g, one-hot, label encoding) 
        - 'Feature Scaling': normalization or standardization 

  4|> [ Exploratory Data Analysis (EDA) ] 
      * Analyze and visualize data to understand its structure
        - Histograms, boxplots, scatter plots
        - Correlation matrix
        - Summary statistics (mean, median, std) 
        - find patterns, trends, anomalies
  
  5|> [ Feature Engineering: ]
      * 'Create' new features or 'transform' existing onces to improve model
        performance:
        - Polynomial features 
        - Interaction terms
        - Log transforms
        - Aggregation (e.g, mean, sum per group)

  6|> [ Data Splitting: ]
      * Split data into subsets: 
        - 'Training set': to train the model
        - 'Validation set': to tune parameters (optional)
        - 'Test set': to evaluate model performance

===[ Summary: ]===

    Raw Data 
       ‚Üì
  Data Collection 
       ‚Üì
  Data Cleaning 
       ‚Üì
Preprocessing (Transformation & Encoding)
       ‚Üì
Exploratory Data Analysis (EDA)
       ‚Üì
  Feature Engineering 
       ‚Üì
  Train-Test Split 
       ‚Üì
  Model Training
================================================================================

===[ Pandas: ]==================================================================
* 'Pandas' is library 'built on top of Numpy', Pandas is optimized for 'data'
  'manipulation and analysis', especially 'tabular (row/column)' or
  'labeled data', Pandas is used:
    - Data cleaning and preprocessing
    - Data transformation and reshaping
    - Exploratory Data Analysis (EDA) 
    - Time series analysis
    - Importing/exporting real-world data (CSV, Excel, SQL, etc) 
    # NOTE:---------------------------------------------------------------------
    # - Pandas is not a 'replacement' for Numpy, It 'extends' it.
    # - It brings 'structure', 'labels', and 'data-aware operations' that are
    #   crucial for real-world data analysis 
    # - 'ndarray' is great for 'nemerical computation'; Pandas is optimized for
    #   'data manipulation', 'cleaning', and exploration. 
  - The term 'Pandas' is term derived from 'Panel Data', a term used in
    'econometrices' to describe 'multidimensional structured data' 
      . 'Panel Data' = data that involves 'observations over multiple time'
        'periods' for the same individuals (e.g time series + cross-sectional
         data)
      . The 'library name' reflects its focus on providing powerful and flexible 
        'data structures' for working with such 'complex structured datasets'
  # CONCLUSION:-----------------------------------------------------------------
  # while 'Pandas' might sound like the animal üêº, it actually has a technical
  # origin rooted in 'data analysis'.

# QUESTION:[ Why learn Pandas ? ]===============================================
# 1|> Higher-level Abstraction: Pandas provides an intuitive way to work with
#     structured data, eliminating the need for complex loops and index tracking
# 2|> Faster Development: Cleaner and shorter code for common data analysis
#     tasks
# 3|> Real-World Data Ready: Most real-word data is 'tabular', with missing
#     values, labels, and mixed data types. (Pandas handles this gracefully)
# 4|> Seamless Integration: Works well with other data science tools, (like
#     Numpy, Matplotlib, Scikit-learn, etc) 
#>>> [ Numpy VS Pandas ]
# ______________________________________________________________________________ 
# | Aspect            | NumPy                 | Pandas                         |
# |-------------------|-----------------------|--------------------------------|
# | Focus             | Numerical computation | Data manipulation and analysis |
# | Data Type Support | Homogeneous arrays    | Heterogeneous tables           |
# | Primary Structure | ndarray               | Series and DataFrame           |
# | Use Case          | Scientific computing  | Real-world data analysis       |
# |-Learning Need     |-Essential for         |-Essential for high-level data  | 
# |                   | low-level operations  | analysis                       | 
# |___________________|_______________________|________________________________|
#
# TIP:==========================================================================
# - Use NumPy for efficient computation and Pandas for efficient data handling.
#   In most data science projects, you'll use both, but Pandas is your go-to
#   for structured, real-world datasets.

===[ The Core Motivation behind Pandas ]===
- The 'main goal' of 'creating pandas' was to add 'labels and metadata' to 
  to numerical data structures (like NumPy arrays), enabling more 'flexible' and
  'human-readable' data analysis workflows.
=> 'Labels' (especially column lables in a 'DataFrame') typically represent
   'input feature' in machine learning and data analysis.
=> [ Terminology: ]
  > 'Columns' -> represent 'features' (with column labels)
  > 'Rows'    -> represent 'records/observations/samples' (with row labels
    or index) 

# QUESTION:[ Why Labels Matter? ]-----------------------------------------------
# 1|> Contextual Understanding
#     - Labels like column names ("age", "price") and row indices ("user_001",
#       "2023-01-01") provide semantic meaning to raw data.
# 2|> Ease of Access & Manipulation
#     - Instead of arr[0, 1], you can do df.loc["user_001", "price"].
#     - Enables powerful slicing, filtering, and grouping based on names, not
#       just positions.
# 3|> Automatic Alignment
#     - Labeled operations (e.g., combining datasets with different indices) are
#       handled automatically.
#     - NumPy can't align data across mismatched indices ‚Äî Pandas can.
# 4|> Missing Data Support
#     - Real-world datasets often have incomplete entries ‚Äî Pandas uses NaN,
#       .isna(), .fillna(), etc., to manage this elegantly.
# QUESTION:[ How Pandas Built on Numpy ?]---------------------------------------
# - Uses 'NumPy (ndarray) under the hood' for fast computation 
# - Wraps it with: 
#   . `Index`: to assign labels
#   . `Series`: labeled 1D array 
#   . `DataFrame`: labeled 2D table 
# - Adds high-level data manipulation capabilities.
# ------------------------------------------------------------------------------
# CONCLUSION:===================================================================
# - multiple Series (each representing an input feature) can be combined into a
#   DataFrame ‚Äî a 2D data matrix ‚Äî to visualize and analyze both the features
#   (columns) and the cases (rows) easily.
# - DataFrame object represent my matrix table that i have learnt in 
#   in 'math_statistics'.

===[ Series ]===
* A Series is essentially a 'one-dimensional labeled array' than can hold any
  data type

--> [ Internal Components: (Attribues) ]
    - 'values': Stores the actual data as a 'NumPy''ndarray' (or sometimes
                'ExtensionArray', like for 'pd.StringDtype', 'Categorical', etc)
    - 'index': Stores 'labels' for each element (row labels). It is an
               'instance' of the 'Index' class. 
    - 'dtype': Stores the data type of the elements 

--> [ Examples: ]
    Import pandas as pd

    s = pd.Series([10, 20, 30], index=["a", "b", "c"])
    # values -> array([10, 20, 30]) 
    # index  -> Index(['a', 'b', c]) => Labels

===[ DataFrame: ]===
* A 'DataFrame' is a 'two-dimensional labeled data structure' (like a table or
  spreadsheet).

# INFO:[ Origin Of the Term 'DataFrame' ]=======================================
# - The name `DataFrame` comes from the 'R programming language' which used it
#   to describe a 'tabular data structure' that is: 
#   . 2-dimensional (like a matrix)
#   . Has 'Labeled axes' (rows and columns)
#   . Can store 'heterogeneous data types' (different types in different
#     columns) 
# - 'Wes Mckinney', the creator of Pandas, was inspired by R when he designed
#   Pandas, He browwed the name and concept of the 'DataFrame'
# QUESTION:[ Why Not `DataMatrix` name for example ? ]--------------------------
# - A 'matrix' is a mathematical concept that implies: 
#   ____________________________________________________________________________
#   | Matrix Characteristic | DataFrame Equivalent | Why Not Enough?           |
#   |-----------------------|----------------------|---------------------------|
#   |-Purely numerical      |-Columns can be, ints |-DataFrames can store      |
#   |                       | strings, etc         | mixed types               |
#   |-Only 2D and           |-DataFrames are 2D    |-Matrices are not flexible | 
#   | homogeneous           | but heterogeneous    | for real-world data       |
#   |-Indexed by position   |-DataFrames are       |-Real data often needs     | 
#   | only                  | labeled              | named columns/index       |
#   |_______________________|______________________|___________________________|
# - 'DataMatrix' would be misleading because:
#  . It suggest only 'numeric, homogeneous' data (like in NumPy)
#  . It ignores 'labels' which are contral to pandas 
#  . It does not reflect the 'flexibility' needed for real-world datasets
#    (think CSV files, Databases, etc)
# - 'DataFrame' make Sense because: 
#  > 'Frame' suggest a 'structure or container', just like a photo frame
#    contains multiple photos.
#  > 'Data' represent the 'contents', actual values, columns rows
#  > Together, 'DataFrame' means: 'A structured, labeled container for'
#    'your data'

--> [ Internal Components: (Attributes) ]
    - '_data': Under the hood, stores data in a 'block manager' which manages
               a dictionary of 'ndarray' or 'ExtensionArray' columns grouped by
               dtype. 
    - 'columns': Labels for columns (an 'Index' object) 
    - 'index': 'Labels' for 'rows' (also an 'Index' object (samples, observation
               cases <term in statistics>)
    - Each column is a 'Series' with the same index

--> [ Example: ]
    Import pandas as pd

    df = pd.DataFrame({
      "feature1": ["oussama", "ezzaou"]
      "feature2": [25, 30]
    }, index=["case1", "case2"])

    #>>> Internally:
    # name -> Series(['oussama', 'ezzaou', dtype=object])
    # age -> Series([25, 30], dtype=int)
    #>>> Data Matrix (Matrix Table)
    #         _______________________
    #         | Feature1 | Feature2 |
    # |-------|----------|----------|
    # | case1 | oussama  |  25      | 
    # |-------|----------|----------|
    # | case2 | ezzaou   |  30      |
    # |_______|__________|__________|

--> [ Pandas Core Class UML: ]
    +------------------------------------------------+
    |                DataFrame                       |
    +------------------------------------------------+
    | ‚Äì _data: BlockManager                          |
    | ‚Äì index: Index                                 |
    | ‚Äì columns: Index                               |
    +------------------------------------------------+
    | + __init__(...)                                |
    | + locate/iloc/select etc.                      |
    | + groupby(), merge(), pivot(), to_csv(), ...   |
    +------------------------------------------------+
                       ‚ñ≤         ‚ñ≤
                       |         |
         contains      |         | contains
                       |         |
        +----------------+   +----------------+
        |    Series      |   |     Index      |
        +----------------+   +----------------+
        | ‚Äì values: ndarray/ExtensionArray    |
        | ‚Äì index: Index                      |
        | ‚Äì name: Optional[str]               |
        +----------------+   +----------------+
        | + __init__(...)                     |
        | + arithmetic methods (add, mean, ..)|
        | + isna(), plot(), ‚Ä¶                 |
        +----------------+   +----------------+


===[ Reading in file: ]=========================================================
* As the first step made is to read a dataset from a file to dataframe
  (.csv, .text, .json)

  1|> "pd.read_csv(r'file_path')" this function is used to read in 'csv files' 
 
    . header=: (optional) It is used for input feature labels 
                      Ex: 'header=None' removes the labels and use index instead 
    . names=[inputfeature1, inputfeature2, ...], this function can be used also 
    . index_col='x': you can define the index labels of samples/observations 
                     cases.

    # REVISION:----------------------------------------------------------------
    # - '\', ":",  are still special characthere until you till python to deal
    # with them as a row string
    # - What makes 'DataFrame' different is having labels 

    # TIP:----------------------------------------------------
    # - df is the standard name of the dataset red from a file

    . sep='separator': defines the separator between input features. 

  2|> 'pandas.read_table(data.txt)' this function is used to read a txt files 

  3|> 'pandas.read_json('data.json')': used to read json file into a DataFrame 

  4|> 'pandas.read_excel(data.xlsx, sheet_name='')': used for excel files


  # INFO:----------------------------------------------------------------------
  # - pd.set_option('display.max.rows', nbr/None): is used to display a nbr of
  #   rows
  # - pd.set_option('display.max_columns', nbr/None) is used to display a nbr
  #   of columns (None to display all columns)

===[ DataFrame Exploring:  ]====================================================
    - 'df.info()': used to get a 'concis summary' of a 'Pandas DataFrame', It is
      very helpful for 'understanding the structure' of your dataset quickly.
    - 'df.head(nbr=5)': used to display the 'nbr' from the 'head' of 'DataFrame' 
    - 'df.tail(nbr=5)': used to display the 'nbr' from the 'tail (end)' of
                        'DataFrame' 
    - 'df.shape': shape of the 'DataFrame (dimensions)' 

===[ Index Manipulation / Label Management ]====================================
  1|> 'df.set_index()': is used to 'change the index' of a DataFrame to one or
      more existing columns. 

      > Ex:
          df.set_index('x'): 'x' is the input feature label 

  2|> 'df.reset_index()' is used to 'reset the index' of a 'DataFrame' back to
      the default (0, 1, 2, ...) 
      > [ Use cases: ]
        . Your index is no longer meaningful (ex: after filtering or grouping)
        . You want to convert the index back into a column
        . You want a clean DataFrame with the default integer index.

        >>> [ df.reset_index(level=None, drop=False, inplace=False) ] <<<

        . 'level': (opt) specify index level(s) to reset for 'multiIndex'
        . 'drop': If 'True, do not insert the index' into DataFrame as a column 
          . Default is 'False'
        . 'inplace': If 'True', modify the DataFrame in-place. Default is
                     'False' > (simply define if you want to change the object
                     itself or not)

===[ Filtering and Ordering: ]=================================================
 
===[ Filtering: ]===
  - 1|> "df[df['x'] > y]": (boolean indexing) returns all the the 'cases/'
        'samples/observations' that have an input feature 'x' greater (`>`) than
        'y'.

    # NOTE:---------------------------------------------------------------------
    # - '>' were used as an example, you CAN USE ANY OTHER COMPARISON operator.
    # - It is almost the same idea in 'NumPy' 
    # - df['x'] > y: is called boolean indexing technique that generates a
    #   boolean mask. 
    # --------------------------------------------------------------------------
  
  2|> "df.isin()": is used to 'filter data' by checking whether each element in
      DataFrame or Series 'exist in a list or another set values'. It returns a
      'Boolean Mask', `True` if the element 'is in' the given list, else `False`

      # NOTE:----------------------------------------------------------
      # It does generate a boolean mask
    
  3|> "df['x'].str.contains()": method is used to 'check if a string column' 
      'contains a certain substring or pattern' (using regex by default)

      # NOTE:----------------------------------------------------------
      # It does generate a boolean mask

  4|> 'df.filter()': is used to 'select specific rows or columns' based on
      'labels (names)', 'not by values'.

      df.filter(items=[label1, label2, label3, ...], like="regrex", axis=)
        . 'axis=': 0 represent the samples/observation labels 
                   axis=1: represetn the input feature labels
        . 'items=': get a list of items (row labels or column labels)
        . 'like=': like="x", brings all labels that contains 'x' as part from
                   the names. 
   > [ Summary: ]
      _____________________________________________________________________
      | Method            | Returns | Uses     | Type of Filtering        |
      |                   | Boolean | Boolean  |                          |
      |                   | Mask    | Indexing |                          |
      |-------------------|---------|----------|--------------------------|
      | `.isin()`         | ‚úÖ Yes  | ‚úÖ Yes   | Value-based filtering    |
      | `.str.contains()` | ‚úÖ Yes  | ‚úÖ Yes   | String pattern filtering |
      | `.filter()`       | ‚ùå No   | ‚ùå No    | Label-based (column/row) |
      |                   |         |          | filtering                |
      |___________________|_________|__________|__________________________|

  # TIP:[ fd.loc[] & df.iloc[] as filters ]-------------------------------------
  # - fd.loc[] and df.iloc[] are indexer but they can be used as filters if you 
  #   combine it with boolean indexing
  #   > Ex:
  #     df.loc[ fd['x'] > y]
  #     
  #     # fd['x'] > y: is a boolean indexing technique that generates a boolean
  #     # mask that can be used with loc[] or iloc[]
  #
  #     # NOTE:> boolean mask tells the loc/iloc wich columns or rows to keep


===[ Ordering: ]===
  5|> "df.sort_values(by=['label1', ...], ascending=[True, ...])", Is used to
    'sort a DataFrame or Series by one or more columns (or by its values)'

[df.sort_values(by, axis=0, ascending=True, inplace=False, na_position='last') ]

  6|> 'df.sort_index()': is used to 'sort a DataFrame or Series by its index' 
      'labels, not by column values'


===[ Indexing: ]================================================================
* 'Index': is an object that stores all axis labels for all pandas objects 

  1|> "df.loc['x']": is a powerful method for 'label-based selection' in pandas  
                   It allows you to 'access rows and columns by their labels',
                   not by position (use iloc for that)

      >>> [ df.loc[row_label, column_label] ] <<<

      . You can pass single labels, lists, slices or boolean masks
        exclusive slicing)

        df.loc[start_row_lebale:end_row_label, start_col_label: end_col_label]

  2|> "df.iloc[]": is used for 'position-based indexing', It lets you select rows
                 and columns by 'integer location', just like you would with
                 python lists

    # NOTE:------------------------------------------------------------------
    # 'loc' and 'iloc' can be used for filtering, but only when combined with
    # 'boolean indexing'.

      # INFO:------------------------------
      # - loc: stands for LOCation
      # - iloc: stands for Integer LOCation

      >> [ df.iloc[start_row_idx: end_row_idx, start_col_idx: end_row_idx] ] <<

  # NOTE:-------------------------------------------------------------------
  # loc[] and iloc[] are Similar techniques to indexing and slicing in NumPy

  . You can pass 'integers', 'lists of integers', 'slices', or 'boolean masks'.

# QUESTION:[ What is the diff between indexing & filtering in pandas ?]========
# Filtering: Select rows based on 'condition (boolean)'
#   |=> result type: Filtered subset
# Indexing: Selects specific rows/columns by position or label
#   |=> Result Type: Subset of original 
# * [ Filtering: Which Rows to keep ]
#   - Filtering uses a 'boolean condition' to keep only certain rows.
#   > Ex:
#     df[df[x] > y]: filtering rows based on a condition 
# * [ Indexing: what to access ]
#  - Indexing is how you access specific parts of the data by 'label (loc)'
#    or 'position (iloc)'
#   > Ex:
#     df.loc['city'] 
#     df.iloc[1]
# CONCLUSION:-------------------------------------------------------------------
# * Indexing = direct 'selection' (by label or position)
# * Filtering = 'Conditional' selection (based on a rule)
# - 'loc' and 'iloc' are 'indexers', but they can perform 'filtering' when given
#   a 'boolean condition' for the 'row selector'.
# ______________________________________________________________________________
# | Feature    | Filtering                         | Indexing                  |
# |------------|-----------------------------------|---------------------------|
# | Definition | Selects rows based on a condition | Selects rows or           |
# |            |                                   | columns by label/position |
# |------------|-----------------------------------|---------------------------|
# | Syntax     | `df[condition]`                   | `df.loc[]`, `df.iloc[]`,  |
# |            |                                   | `df['col']`               |
# |------------|-----------------------------------|---------------------------|
# | Returns    | A filtered DataFrame              | A DataFrame, Series, or   | 
# |            | (subset of rows)                  | scalar                    |
# |------------|-----------------------------------|---------------------------|
# | Input      | Boolean Series (e.g.,             | Label(s), position(s),    | 
# |            | `df['age'] > 25`)                 | or slices                 |
# |------------|-----------------------------------|---------------------------|
# | Output     | Rows where condition is True      | Specific rows/columns by  |
# | Example    |                                   | name/position             | 
# |------------|-----------------------------------|---------------------------|
# | Purpose    | Keep only rows that match         | Access specific part(s)   |
# |            | condition                         | of data                   |
# |____________|___________________________________|___________________________|

===[ Group by <groupby()> && Aggregation<agg()>: ]==============================
* 'groupby()' function in Pandas is used to 'split data' into 'groups based' on
  'one or more keys (column values)', 'apply' a function (e.g 'sum', 'mean',
  'count') and then 'combine' the results into a 'new DataFrame', This process
  is known as 'the Split-Apply-Combine' strategy.

  > [ Aggregation methods: ]
    _________________________________________________
    | Method     | Description                      |
    |------------|----------------------------------|
    | `.sum()`   | Total per group                  |
    | `.mean()`  | Average per group                |
    | `.count()` | Count non-null entries           |
    | `.min()`   | Minimum per group                |
    | `.max()`   | Maximum per group                |
    | `.size()`  | Total number of rows (incl.NaN)  |
    | `.agg()`   | Custom or multiple aggregations  |
    |____________|__________________________________|

  |> 'pd.agg()': is used to apply 'one or more aggregation functions' to a
      'DataFrame or Series'

      >> [ df.agg(func, axis=0) ] <<
        
        . `func`: a function (or list/dict of functions) like 'mean', 'sum', etc
        . `axis=0`: (defautl) apply to 'columns'
        . `axis=1`: (defautl) apply to 'rows'

      > [ Example: ]

      import pandas as pd
      import numpy as np

      df = pd.DataFrame({
        'A': [1, 2, 3, 4],
        'B': [5, 6, 7, 8]
      })
      # Single aggregation on all columns
      df.agg('mean')
      # A    2.5
      # B    6.5
      # Multiple aggregations on all columns
      df.agg(['mean', 'sum'])
      # Different aggregations per column
      df.agg({
          'A': ['min', 'max'],
          'B': ['mean', 'std']
      })

  # INFO:[ Use cases: ]-------------------------------
  # - Cusom summary statistics
  # - aggregating different columns differently
  # - grouped aggregations (used with groupby() often)

  |> 'df.describe()': returns 'a quick overview of statistics' for numeric
     columns

  >> [ df.describe(include=None, exclude=None, percentiles=None) ] <<
    . include='all': include non-numeric columns
    . exclude='number': exclude numeric
    . percentiles: specify percentiles to show (e.g. [.25, .5, .75])

  > [ Exmaple ]
    # describing the distribution (center tendency (mode, median, mean),
    # dispersion <range, IQR, std variance, etc)
    df.describe()

    # return example
    #         | A    | B    |
    # |-------|------|------|
    # | count | 4.0  | 4.0  |
    # | mean  | 2.5  | 6.5  |
    # | std   | 1.29 | 1.29 |
    # | min   | 1.0  | 5.0  |
    # | 25%   | 1.75 | 5.75 |
    # | 50%   | 2.5  | 6.5  |
    # | 75%   | 3.25 | 7.25 |
    # | max   | 4.0  | 8.0  |

  # INFO:[ Use cases: ]-------------------------------
  # - Quick look at data distributions 
  # - Exploratory Data Analysis (EDA)

  # INFO:-----------------------------------------------------------------------
  # - Aggregation: (noun) the process of combining things or amounts into a
  #   single group or total:
  #   . ex: Some animals avoid being eaten by aggregation in groups such as
  #         schools or fish
  # - In Context of Pandas means applying a 'summary function' to groups of
  #   data to get a 'single value' per group (like 'sum', 'mean', 'min', 'max'
  #   or 'count')
  #
  # CONCLUSION:----------------------------------------------------------------
  # - Aggregation is a method to perform a group of statistical operations
  #   (like sum, mean, count, etc.) on a DataFrame or Series.
  # ----------------------------------------------------------------------------

# QUESTION:[ Boolean indexing VS boolean Mask ]---------------------------------
# - 'Boolean indexing' is a way to 'filter data' in a DataFrame or series by
#   using a 'boolean condition' to select rows or columns.
# - 'Boolean mask' is a 'Series or array of True/False values' used to filter 
#   data, it's called a 'mask' because it reveals only the elements where the 
#   condition is true
