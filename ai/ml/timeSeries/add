#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£§‚£¶‚£¥‚£∂‚£æ‚£ø‚£∂‚£∂‚£∂‚£∂‚£¶‚£§‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢†‚°∂‚†ª‚†õ‚†ü‚†ã‚†â‚†Ä‚†à‚†§‚†¥‚†∂‚†∂‚¢æ‚£ø‚£ø‚£ø‚£∑‚£¶‚†Ñ‚†Ä‚†Ä‚†Ä                     ìêì  add ìêî           
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚†î‚†ã‚†Ä‚†Ä‚†§‚†í‚†í‚¢≤‚†Ä‚†Ä‚†Ä‚¢Ä‚£†‚£§‚£§‚£¨‚£Ω‚£ø‚£ø‚£ø‚£∑‚£Ñ‚†Ä‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚£Ä‚£é‚¢§‚£∂‚£æ‚†Ö‚†Ä‚†Ä‚¢Ä‚°§‚†è‚†Ä‚†Ä‚†Ä‚††‚£Ñ‚£à‚°ô‚†ª‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚†Ä      Eng: oezzaou <oussama.ezzaou@gmail.com> 
#  ‚¢Ä‚†î‚†â‚†Ä‚†ä‚†ø‚†ø‚£ø‚†Ç‚††‚†¢‚£§‚†§‚£§‚£º‚£ø‚£∂‚£∂‚£§‚£ù‚£ª‚£∑‚£¶‚£ç‚°ª‚£ø‚£ø‚£ø‚£ø‚°Ä                                              
#  ‚¢æ‚£æ‚£Ü‚£§‚£§‚£Ñ‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†â‚¢ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á                                              
#  ‚†Ä‚†à‚¢ã‚¢π‚†ã‚†â‚†ô‚¢¶‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£º‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á       Created: 2025/11/01 15:46:52 by oezzaou
#  ‚†Ä‚†Ä‚†Ä‚†ë‚†Ä‚†Ä‚†Ä‚†à‚°á‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†á       Updated: 2025/11/01 18:24:35 by oezzaou
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°á‚†Ä‚†Ä‚¢Ä‚£æ‚£ø‚£ø‚†ø‚†ü‚†õ‚†ã‚†õ‚¢ø‚£ø‚£ø‚†ª‚£ø‚£ø‚£ø‚£ø‚°ø‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚†á‚†Ä‚¢†‚£ø‚£ü‚£≠‚£§‚£∂‚£¶‚£Ñ‚°Ä‚†Ä‚†Ä‚†à‚†ª‚†Ä‚†ò‚£ø‚£ø‚£ø‚†á‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†±‚†§‚†ä‚†Ä‚¢Ä‚£ø‚°ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚£ø‚†è‚†Ä‚†Ä                             ìÜ©‚ôïìÜ™      
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°Ñ‚†Ä‚†Ä‚†Ä‚†ò‚¢ß‚°Ä‚†Ä‚†Ä‚†∏‚£ø‚£ø‚£ø‚†ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ê‚†ã‚†Ä‚†Ä‚†Ä                     ìÑÇ oussama ezzaouìÜÉ  
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚†Ñ‚£Ä‚°Ä‚†∏‚†ì‚†Ä‚†Ä‚†Ä‚††‚†ü‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä                                              

===[ Measuring Purity: Entropy & Information Gain ]============================
# REVISION:---------------------------------------------------------------
# - Purity is about how similar the items in a group are after a split 
#   . A 'pure' group means all items inside it belong to the same category
#   . An 'impure' group has a mix
# > So the goal from decision tree is to keep splitting the data until 
#   the groups are as pure as possible.


                |===[ How to Mesaure Purity ]===|
                |                               |
      [ Gini Impurity ]                     [ Entropy ]
- Think of it as the chance           - 'Entropy', Similar idea, but  
  of picking two different              it measures disorder 
  items from the same group             . All one type = low disorder(pure)

  # NOTE:=================================================================
  # - Don't worry about the math, both are just ways to score how mixed or 
  #   unmixed a group is. 
  # - when building a decistion tree, the algorithm looks for the question 
  #   (or 'split') that makes the resulting groups 'more pure' than before


===[ Entropy ]===
* 'Entropy' is just a way to 'measure how mixed or uncertain' a group of data is
  - If a group has 'only one kind of thing', its "pure -> low entropy (or zero)"
  - If it has 'a mix of different kinds', its 'impure -> high entropy'

# INFO:[ Analogy ]===============================
# * Think of entropy as a 'messiness' score:    |
#   . 'More mixed' = more messy = higher entropy|
#   . 'More uniform' = cleaner = lower entropy  |
# ===============================================

- The basic fomula for Entropy: 

    Entropy = - Sum (pi * log2(pi))

    . 'pi': the proportion (probability) of each class in the group.
    . 

> [ Why using log base 2 ]
  - Entropy comes from infromation theory The idea of entropy comes from
    "Claude Shannon's infromation Theory" (1948) 
  - Whe wanted a way to measure how much 'information' or 'uncertainty' is in a
    message or dataset. 
  - In information theory, the 'amount of information' in a event depends on how
    surprising it is. 

    > if something is 'very predictable', it carries 'little information'.
      ex: "the sun rieses in the east" -> not surprising
    > if something is 'unlikely', it carries 'more information' 
      ex: "it will snow in the desert today" -> surprising!

  Shannon defined 'information' as:

      [ Information(x) = - log2(p(x)) ]

  That's why "logarithms" come into play, they translate probabilities (liek 0,5, 0,25, etc) into "information content".

# QUESTION:[ Why specifically log base 2 ]======================================
# - Because we measure information in 'bits', the basic unit of digital
#   information. 
#   . Each 'bit represents' a (yes/no | 1 or 0) question, like flipping a coin
#     or answering a binray question.
# - So using 'log base 2' means we are measuring:
#   "How many binary questions (yes/no decisions) would it take to identify"
#   "the correct class?"
# - For Example:
#   . if p = 0.5, then -log2(0.5) = 1bit -> one yes/no question
#   . if p = 0.25, then -log2(0.25) = 2 bits -> two questions (more uncertainty)
# > That's why 'base 2' makes sense, decision trees make binray decisions at
# each step (`yes` or `no`), just like bits!.
# INFO:[ EXAMPLE ]==============================================================
# - Imagine guessing what fruit someone picked:
# - If there are 2 equally likely fruits (apple or orange), it takes about 1
#   yes/no question (‚ÄúIs it an apple?‚Äù).
# - If there are 4 equally likely fruits (apple, orange, banana, mango), it
#   might take about 2 yes/no questions (‚ÄúIs it an apple?‚Äù ‚Üí No ‚Üí ‚ÄúIs it an
#   orange?‚Äù ‚Üí etc.)
# - So the log base 2 gives a measure of how many yes/no questions are needed,
#   which fits perfectly with how decision trees split data!
# ____________________________________________________________________________
# | Concept     | Meaning                                                    |
# |-------------|------------------------------------------------------------|
# | Logarithm   | Turns probabilities into ‚Äúinformation content.‚Äù            |
# | Base 2      | Measures that information in bits (yes/no decisions).      |
# | Why use it? | Because decision trees are made of binary splits,          |
# |             | it‚Äôs a natural match!                                      |
# |_____________|____________________________________________________________|
# - So, we use log‚ÇÇ because entropy is all about measuring uncertainty in bits,
#   and decision trees reduce that uncertainty step by step ‚Äî like answering a
#   series of yes/no questions.
# ==============================================================================
