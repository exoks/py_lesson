#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£§‚£¶‚£¥‚£∂‚£æ‚£ø‚£∂‚£∂‚£∂‚£∂‚£¶‚£§‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢†‚°∂‚†ª‚†õ‚†ü‚†ã‚†â‚†Ä‚†à‚†§‚†¥‚†∂‚†∂‚¢æ‚£ø‚£ø‚£ø‚£∑‚£¶‚†Ñ‚†Ä‚†Ä‚†Ä                ìêì  revision ìêî           
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚†î‚†ã‚†Ä‚†Ä‚†§‚†í‚†í‚¢≤‚†Ä‚†Ä‚†Ä‚¢Ä‚£†‚£§‚£§‚£¨‚£Ω‚£ø‚£ø‚£ø‚£∑‚£Ñ‚†Ä‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚£Ä‚£é‚¢§‚£∂‚£æ‚†Ö‚†Ä‚†Ä‚¢Ä‚°§‚†è‚†Ä‚†Ä‚†Ä‚††‚£Ñ‚£à‚°ô‚†ª‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚†Ä      Eng: oezzaou <oussama.ezzaou@gmail.com> 
#  ‚¢Ä‚†î‚†â‚†Ä‚†ä‚†ø‚†ø‚£ø‚†Ç‚††‚†¢‚£§‚†§‚£§‚£º‚£ø‚£∂‚£∂‚£§‚£ù‚£ª‚£∑‚£¶‚£ç‚°ª‚£ø‚£ø‚£ø‚£ø‚°Ä                                              
#  ‚¢æ‚£æ‚£Ü‚£§‚£§‚£Ñ‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†â‚¢ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á                                              
#  ‚†Ä‚†à‚¢ã‚¢π‚†ã‚†â‚†ô‚¢¶‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£º‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á       Created: 2025/08/21 11:40:18 by oezzaou
#  ‚†Ä‚†Ä‚†Ä‚†ë‚†Ä‚†Ä‚†Ä‚†à‚°á‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†á       Updated: 2025/09/28 15:29:53 by oezzaou
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°á‚†Ä‚†Ä‚¢Ä‚£æ‚£ø‚£ø‚†ø‚†ü‚†õ‚†ã‚†õ‚¢ø‚£ø‚£ø‚†ª‚£ø‚£ø‚£ø‚£ø‚°ø‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚†á‚†Ä‚¢†‚£ø‚£ü‚£≠‚£§‚£∂‚£¶‚£Ñ‚°Ä‚†Ä‚†Ä‚†à‚†ª‚†Ä‚†ò‚£ø‚£ø‚£ø‚†á‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†±‚†§‚†ä‚†Ä‚¢Ä‚£ø‚°ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚£ø‚†è‚†Ä‚†Ä                             ìÜ©‚ôïìÜ™      
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°Ñ‚†Ä‚†Ä‚†Ä‚†ò‚¢ß‚°Ä‚†Ä‚†Ä‚†∏‚£ø‚£ø‚£ø‚†ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ê‚†ã‚†Ä‚†Ä‚†Ä                     ìÑÇ oussama ezzaouìÜÉ  
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚†Ñ‚£Ä‚°Ä‚†∏‚†ì‚†Ä‚†Ä‚†Ä‚††‚†ü‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä                                              

- Time series are the data structure of a dataset, it is an ordered-time data,
  this form is a bit different from the normal data (or non-ordered dataset) 
  , in this structure the data is recorded over a fixed time interval. 

- the concpet of time series leads us to time 'series analysis' to summarize and 
  understand the time series dataset.

- time series analysis concept: 
  > Stationarity: (matimatically constant mean (trend) + constant variance)
      - Differencing (to de-trend a time series), 
      - log/Box-Cox Transformation (to remove the variance, <make it constant>) 
  > Seasonality: predicted changes that happens periodically in fixed 
                 periods.
      - There are two types of seasonality
        1. Additive: Seasonal Cycles are constant in value
        2. multiplicative: Seasonal Cycles are proportional to the trend
      - How to identify seasonality type?: infortunatily there is no statistical
        method to identify seasonality type, but we have some 2 options:
        1. Option 1: Data Visualization
           > You can use some plots like `month_plot`, `` to visualize
             'seasonality'.
             . `month_plot`: for monthly seasonality
             # NOTE:> resample the data 'montly' and take the mean 
             . `quarter_plot`: for quarter seasonality 
             # NOTE:> resample the data 'quarterly' and take the mean  

        2. Option 2: Model performance, try (additive & multiplicative) models
                    to see wich one that fits best
                    |=> (option 2 is generaly preferd) 

  > Decomposition: (the concept of breaking up time series model into Trend +
                    Seasonaliy + Residuals) and visualize each part on each own 
      - Additive Model:           Y(t) = T(t) + S(t) + R(t)
      - Multiplicative Model:     Y(t) = T(t) * S(t) * R(t) 

      > Seasonality:
        . 24 for hourly
        . 7 or 365 for daily, but 7 is preferred for modeling
          # NOTE:> 7 is commonly used for modeling (ARIMA, SARIMA, SARIMAX)
        . 52 for weekly
        . 12 for montly
        . 4 for quarterly
        . 5 for weekdays 

      - seasonal_decomposition in python 

        from statsmodels.tsa.seasonal import seasonal_decompose
        # Seasonal Decompostion in Python
        seasonal_decompose(x, method="mut/add", period=)

        - When in the index is date (in case of time series), we can simply
          create another feature to help us improve our model prefermance
          these feature are kind of related to seasonality concept 

          . df['day'] = df.index.day
          . df['month'] = df.index.month
          . df['year'] = df.index.year
          . df['weekday']. = df.index.day_name() # to get the name of the day  
                                                 # instead of numerical number 
          . df['is_weekend'] = df.index.weekday > 4 
          # INFO:---------------------------------------------------------------
          # - Weekday starts from 0 -> 6 
          # --------------------------------------------------------------------
        - When it comes to Feature Engineering - Lagged values (easy)
          . df['feature_lag1'] = df['feature'].shift(1) 
          . df['feature_lag2'] = df['feature'].shift(2)

    > Auto-correlation
      - This concept simply tell us if there is information in the past that
        will help us predict values in the future.
        (just correlating time series values with its 'lagged (past)' values)
        . Its like totoal correlation between a series and its lagged values.

        . Suppose that we want to understand the correlation between coffe 
          assumption of today and all last past days.

          > ACF: "how was my caffe asumption today related to the past days?"  
              - This is the basic formual to caluclate autocorrelation 

                        Cov(y(t), y(t - 1)) 
                P(k) = -------------------- 
                        Var(y(t))

              - pacf_plot may use  the 'unbiased/sample autocorrelation'

                       sum(t=k+1-> N)[(y(t) - y-bar) * (y(t - k) - y-bar) ]
                p(k) = --------------------------------------------------- 
                        sum(t=1->N)[ (y(t) - y-bar)^2 ] 

          > PACF: "how does my coffe assumption relate specifically to 3 days"
                  "ignoring the days in between?"
                - There are two main ways to compute PACF: 
                  . Using regression (direct method) => residuals 
                  . Using Yule-Walker equations (AR coefficients)  
            Y(t) = B1 * Y(t - 1) + B2 * Y(t - 2) + ... + Bk * y(t - k) + e(t)

          > [ Analogy ]
            - ACF: gives overall picture (it is simple)
            - PACF: Zooms in on specific
      > [ Application ]
        - let's take a "fish sales average in differnet months", 
          . S(t) = Avg sales in this month              (March) 
          . S(t - 1) = Avg sales in the last month      (Fabruary) 
          . S(t - 2) = Avg sales 2 months Ago           (January)

        > Effects
          - relation 1: 'Sales average in January' absuloty has 'effect' on
            'Fabruary sales average', same idea between 'fabruary' and 'march'.
          - relation 2: 'Sales average in January' has a 'direct effect' on 'March' 

                        X  | Y
                       ----|----
                        J  | M
                        F  | A
                        M  | May

          - IN 'PACF' we do not care about the effect 'through time periods', we
            only care about 'direct effect'.
            [   S(t - k) -> S(t)    ]
            - 'Indirect effects' effects the process, it may be no correlation 
              between two time periods but the 'intermediate effect' show a 
              height correlation.
            > does s(t - k) is a good predictor for S(t) ?
              . suppose that k = 2 (lags) 
                [ S(t) = W1 * S(t - 1) + W2 * S(t - 2)  + e ]

                . `e`: error term
                . `W1, W2`: are the direct effect of s(t - 1) and s(t - 2) 
                  > `W2` is the direct effect of S(t - 2) on S(t)
              - in case of k = 3
                [ S(t) = W1 * S(t - 1) + W2 * S(t - 2) + W3 * (t - 3) + e ]
                . `W3` is the direct effect `PACF` of s(t - 3) on S(t) 
          # INFO:[ CONCLUSION ]-------------------------------------------------
          # - PACF tell us the coeficient tell us the effect of that past time
          #   serie value on the time series values.

              
        # INFO:[ They key concept in time series ]------------------------------
        - mesurment of some values at a time periode depends one the measurment 
          of that value on the previous time period.

      # NOTE:-------------------------------------------------------------------
      # - We expect lower correlation as the past values holds less information
      # - Lag K means you are comparing the series with itself shifted
      #   'K steps back' 

  > Basic Forecasting Models
      - Average Model: new forecasts takes the average value of the older values 
                       over time.
                                 sum(i:1|-> m) (Y(i)) 
                      Y(t + h) = ---------------------
                                          m
          . Which lead us to a constant forecasts 

      - Naive Model: This model is (mkalkh), new forecasts takes the last
                     observed time serie value. 

                     [ Y(t) = Y(t - 1) ] 

      - Seasonal Naive Model: This model takes seasonality into consideration 
                              , new forecasts takes the last observed time 
                              series values in the same sesonality. 

                   [ Y(t) = Y(t - 1 - m) ]

      - Drift Model: this model takes trend into consideration 

                   [ Y(t) = Y(t - 1) + (h * W) ] 

                        Y(t) - Y(t=1)
          . W = slope = ------------ : general 'growth', 'trend'.
                           t - 1

===[ Exponential Smoothing Family Models ]=== 
  > Simple Exponential Smoothing Model
    . This model exponentialy smoothing the last observed values, by putting
      more weights on last values, and less weights one the older ones.
      `W`: weight parameter   0 <= W <= 1 
      . `W`:= 1: if 'W=1' the model becomes a naive model. 

        Y(t + h) = W * Y(t) + W * (1 - W) * Y(t - 1) 
        Y-hat(t + h) = L(t) 

  Current level|baseline at (t) = alpha * (last observation) + (prev level) 

  > Holt's Linear Trend Model 
    . This model exponential smooth both, 'level' and 'trend', following the 
      same concept where we put more weight on the last values, and less
      weight to the historical ones

      Y(t + h) = L(t) +  h * B(t) : Linear equation form 

  L(t): level equation: L(t) = W * Y(t) + (W - 1) * [L(t - 1) + b(t)] 
  B(t): trend equation: b(t) = beta * (L(t) - L(t - 1)) + (1 - beta) * b(t - 1) 

  > Holt winters Model
    . This model triple exponenially smooth the all three time series componenets
      'level', 'trend' and 'Seasonality' (Triple Smoothing)

    - Additive Model (time series are Stationary)
      Y(t + h) = L(t) + h * B(t) + S(t + h - m)  

    - Multiplicative Model (time series variance changes with trend)
      Y(t + h) = [ L(t) + h * B(t) ] * S(t + h - m) 

  > [ Exponential Smoothing models In Python ]
    from statsmodels.tsa.holtwinters SimpleExpSmoothing, holt, ExponentialSmoothing 

  - This 'model' is the favirate in the world of 'forecasting', because It is 
    'simple'.

  > [ Pros]
    - Simple (good in the case of non-complex problems with 'trend' and
      'seasonality')
    - Simple Implementation
    - Intuitive Model (no need to go for smoothing factors (alpha, beta, gama))
    - Adaptable to changes 

  > [ Cons ]
    - Only one Seasonal Components (these models struggle when we have complex
      time series) 
    - External factors can not be used to refine the forecast (No room for
      regressors) => like wheater, covide, ..., these staff can't be included
      since the model 'relies' on 'past values'. 

===[ Residual Analysis ]===
===[ Cross-Validation ]===

===[ ARIMA family Models ]===
  1|> [ AutoRegression (AR) Model ]
      - It is a model that forecast the future based on a specific number 'p'
        of past observations. 

      [   Y(t + h) = y(t) + O1 * y(t - 1) + ... + Op * y(t - p)  ]


===[ The Big challenge in Forecasting ]===
* 'Differentiating' between 'trends & seasonality' is quite simple
  in time series forecasting, but what makes this a 'complicated task' ? 
  - The main challenge is in modeling and interpreting errors. (explaining
    and integrating relevant factors, or regressor).
  
  - The data Engineering problem > Time horizon of data is very important
    > because past 'may not' represent present or future.

    > Example:
      - let s take a '6 year' of data, the older data can cause 'noise' in our
        model because 'past conditions may not reflect present or future'
        scenarios. (you must decide wheter to include or exclude those values)

  # INFO:======================================================================
  # Stock data forecasting is a complex art that involves considering factors | 
  # such as:                                                                  |
  # - Data relevance                                                          |
  # - Error modeling                                                          | 
  # - External influences                                                     | 
  # ===========================================================================

===[ When Forecasting Gone Wrong? ]===
- A lot of models failed to forecast future no matter how they are complex,
  these stores remind us that the world full of surprises, No model in the 
  world no matter how it is can predict 100% correct.

===[ Juypeter Notebook: ]=======================================================
* Juypeter Notebook has been build on the top of `ipython`, ipython has been
  developed as an extension of python to make it more interactive.
  - Jupyter Notebook has two main type of cells:

          |--------------------[ Cell Type ]---------------------|
          |                                                      |
    [ Markdown Cell ]                                      [ Code Cell ] 

  # INFO:-----------------------------------------------------------------------
  # - IPython = Interactive Python: is at the heart of jupyter and knowing what
  #   it does makes a lot of the ecosystem clearer.
  # - Think of it as Python's 'power user' command-line interface.
  # - It provides:
  #   > A 'richer interactive REPL' (read-eval-print loop) than the default
  #     `python` shell 
  #   > Useful 'syntax enhancements' (like '!' to run shell command, '%' magics
  #     for special commands)
  #   > History, tab-completion, object introspection (`obj?`, shows docs,
  #     `obj??` shows source) 
  #   > Integrastion with 'data science workflows', plotting, debugging, etc.
  # - Running `ipython` you will get a prompt `In [1]:` instead of the normal
  #   `>>>` 
  # -> Inside Jupyter Notebooks: Every notebook kernel is actually an 'IPython'
  #    'Kernel' which executes python code and communicates with the Notebook
  #    frontend.
  # -> Behind the scenes in tools: like JupyterLab, Spyder, VS Code notebooks,
  #    etc.
  # -> [ Key Features: ]
  # - Magic commands: (`%` and `%%`)  
  #     . %time my_func()         # time execution
  #     . %lsmagic                # list all magics
  #     . %pip install requests   # install into current kernel
  #     . %%timeit
  #     [x**2 for x in range(1000)]
  # - Shell integration:
  #     . !ls                     # run shell command
  #     . files = !ls             # capture shell output into a Python list
  # - Object inspection
  #     . import math 
  #     math.sqrt?                # show docstring
  #     math.sqrt?                # show source if available
  # - Rich display system -> handles images, HTML, LaTex, plots (when combined
  #   with matplotlib, pandas, etc)
  # -> [ Why It is Important ]
  # - It makes 'interactive exploration' smooth for data science, ML, and
  #   research
  # - It is the 'engine' powering Jupyter Notebooks (so without IPython,
  #   Jupyter, wouldn't run Python code)
  # - It brings Python with the shell, making workflows faster and more flexible
  # ----------------------------------------------------------------------------   

===[ Anomalies VS Ouliers ]=====================================================
Great question ‚Äî they look similar at first but they are not exactly the same
thing. Here‚Äôs the difference in the context of time series and data analysis:

===[ Outliers ]===
    Definition: Data points that deviate significantly from the general
    distribution of the data.
  - Context: Usually statistical ‚Äî they lie far from the mean, median, or
    expected range.

  > [ Examples: ]
  - In daily export traffic, if the typical volume is ~700 and one day has 2000,
    that‚Äôs an outlier.
  - A sensor records a temperature of 120¬∞C when the usual range is 20‚Äì40¬∞C ‚Üí
    outlier.
  - Cause: Can be due to errors (data entry mistakes, faulty sensors) or rare
    but valid events.

===[ Anomalies ]===
  - Definition: Patterns or behaviors in the data that don‚Äôt conform to expected
    temporal trends, seasonality, or structure.
  - Context: Time series‚Äìfocused ‚Äî anomalies consider the sequence and context,
    not just individual points.

  > [ Examples: ]
  - Sudden drop in traffic during weekdays (where we normally expect high
    traffic) ‚Üí anomaly.
  - Unexpected dip/spike in exports right in the middle of a seasonal holiday
    pattern.
  - Cause: Often indicate meaningful changes (fraud, system failure, sudden
    demand shock).

===[ ‚úÖ Key Difference ]===
  - Outlier ‚Üí purely a statistical deviation (point-wise).
  - Anomaly ‚Üí context-aware deviation (sequence/time-wise).
  -> All anomalies are ‚Äúoutliers in context,‚Äù but not all outliers are anomalies

===[ ‚ö° In time series EDA, you usually: ]===
- Detect outliers with statistical methods (IQR, Z-score, etc.).
- Detect anomalies with time series models (ARIMA residuals, STL decomposition,
  machine learning methods like isolation forests, LSTMs, etc.).
