#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£§‚£¶‚£¥‚£∂‚£æ‚£ø‚£∂‚£∂‚£∂‚£∂‚£¶‚£§‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢†‚°∂‚†ª‚†õ‚†ü‚†ã‚†â‚†Ä‚†à‚†§‚†¥‚†∂‚†∂‚¢æ‚£ø‚£ø‚£ø‚£∑‚£¶‚†Ñ‚†Ä‚†Ä‚†Ä             ìêì  time_series ìêî           
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚†î‚†ã‚†Ä‚†Ä‚†§‚†í‚†í‚¢≤‚†Ä‚†Ä‚†Ä‚¢Ä‚£†‚£§‚£§‚£¨‚£Ω‚£ø‚£ø‚£ø‚£∑‚£Ñ‚†Ä‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚£Ä‚£é‚¢§‚£∂‚£æ‚†Ö‚†Ä‚†Ä‚¢Ä‚°§‚†è‚†Ä‚†Ä‚†Ä‚††‚£Ñ‚£à‚°ô‚†ª‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚†Ä       Eng: oezzaou <OussamaEzzaou@gmail.com> 
#  ‚¢Ä‚†î‚†â‚†Ä‚†ä‚†ø‚†ø‚£ø‚†Ç‚††‚†¢‚£§‚†§‚£§‚£º‚£ø‚£∂‚£∂‚£§‚£ù‚£ª‚£∑‚£¶‚£ç‚°ª‚£ø‚£ø‚£ø‚£ø‚°Ä                                              
#  ‚¢æ‚£æ‚£Ü‚£§‚£§‚£Ñ‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†â‚¢ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á                                              
#  ‚†Ä‚†à‚¢ã‚¢π‚†ã‚†â‚†ô‚¢¶‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£º‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á       Created: 2025/08/18 10:21:13 by oezzaou
#  ‚†Ä‚†Ä‚†Ä‚†ë‚†Ä‚†Ä‚†Ä‚†à‚°á‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†á       Updated: 2025/09/10 10:45:04 by oezzaou
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°á‚†Ä‚†Ä‚¢Ä‚£æ‚£ø‚£ø‚†ø‚†ü‚†õ‚†ã‚†õ‚¢ø‚£ø‚£ø‚†ª‚£ø‚£ø‚£ø‚£ø‚°ø‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚†á‚†Ä‚¢†‚£ø‚£ü‚£≠‚£§‚£∂‚£¶‚£Ñ‚°Ä‚†Ä‚†Ä‚†à‚†ª‚†Ä‚†ò‚£ø‚£ø‚£ø‚†á‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†±‚†§‚†ä‚†Ä‚¢Ä‚£ø‚°ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚£ø‚†è‚†Ä‚†Ä                             ìÜ©‚ôïìÜ™      
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°Ñ‚†Ä‚†Ä‚†Ä‚†ò‚¢ß‚°Ä‚†Ä‚†Ä‚†∏‚£ø‚£ø‚£ø‚†ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ê‚†ã‚†Ä‚†Ä‚†Ä                     ìÑÇ oussama ezzaouìÜÉ  
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚†Ñ‚£Ä‚°Ä‚†∏‚†ì‚†Ä‚†Ä‚†Ä‚††‚†ü‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä                                              

===[ Index ]====================================================================
1|> Forcasting
2|> Time Series 
3|> Time Series Analysis 
    - Stationarity 
    - Decomposition 
    - Seasonality 
4|> Naive Models
    - Average Model
    - Naive Model
    - Seasonal Naive Model 
    - Drift (trend) model
5|> Exponential Smoothing Models 
    - Simple Exponential Smoothing Model (single exponential method) 
    - Holt's Linear Trend Model (double exponential method)
    - Holts Winters Model (triple exponenetial method) 
6|> ARIMA Models 
    - Auto-Regression Model (AR)
    - Moving Average Model (MA) 
    - Auto-Regression Integrated Moving Average Model (ARIMA) 
    - Seasonal Auto-Regression Integrated Moving Average Model (SARIMA)
    - SARIMA Xogonous variables


# QUESTION:[ ESTIMATION VS PREDICTION VS FORECASTING ]==========================
# - ESTIMATION: Focues on 'current or past values' not future onces. 
# - PREDICTION: can be future or unseen inputs, but does not necessarily account
#               for time order.
# - FORCASTING: Always 'time-dependent', often handles trends, seasonality and
#   cycles.
# _____________________________________________________________________________
# | Concept     | Time-     | Focus               | Example                   |
# |             |dependent? |                     |                           |
# |-------------|-----------|---------------------|---------------------------|
# | Estimation  | No        | Current/past values | Average sales last month  |
# | Prediction  | Maybe     | Specific outcome    | Will it rain tomorrow?    |
# | Forecasting | Yes       | Future over time    | Next month‚Äôs bakery sales |
# |_____________|___________|_____________________|___________________________|
# - Forcasting = making predictions about 'future or unknown values' based on
#   'patterns in past or known data'.
# NOTE:[ IMPORTANT NOTES ]------------------------------------------------------
# - The data does not have to be tied to time
# - Time Series Forcasting (Special Case)
# - When data is not tied to time, forecasting looks very similar to
#   'regression' or 'predictions tasks' in ML. 
#   . Forecasting 'house prices' based on features (size, location, rooms)
#   . Forecasting 'product dement' from product attributes, not necessarily from 
#     time.
# - All time series forcasting is 'forcasting', But not all forcasting are time 
#   series.
# - [ Time series analysis = statistical field (older than ML) ]
# - [ Time series forecasting = can use ML methods             ]
# INFO:[ Time Series in Machine Learning ]-------------------------------------- 
# - ML models can be applied to time series, but ML itself is 'not required' for
#   time series analysis.
# - Modern ML methods for time series:
#   . Regression models using 'lagged features'.
#   . Random Forest/XGBoost on time-based feature. 
#   . Neural networks (RNN, LSTM, Temporal CNN) 
# - Here, 'time series becomes a supervised ML problem': past -> future mapping 
# CONCLUSION:-------------------------------------------------------------------
# - Forcasting: "What will happen in the future?"
# - Predection: "What is the target value given these inputs?" 
# - Estimation: "What are the hidden parameters that explain my data?" 

===[ Forecasting: ]=============================================================
* 'Forcasting' means 'making an informed prediction' about the future based on
  'past and present' data
  - Forecasting works because:
    . 'History influence the future'.
    . Data often follows 'patterns' (daily, weekly, seasonal, economic cycles) 

  # NOTE:-----------------------------------------------------------------------
  # - Forecasting is perhaps the most common application of machine learning
  #   in the real world.

  # INFO:-------------------------------------------------------------
  # - forcasting (noun): the job or activity of judging what is likely
  #   to happen in the future, based on the information you have now:

# CONCLUSION:-------------------------------------------------------------------
# - Once data is related to 'time', the concept changes because observations are
#   no longer "independent": they become 'sequentially dependent', so you need
#   'time series forcasting' instead of regular prediction.
# - Forcasting = using past + present information to make predictions about the 
#   future, usually in situations where time plays a role.
# - In short Forcasting is a type of predection 'specifically for future values
#   over time' using historical data patterns.
# - Data are recorded is recorded at regular intervals.

  > [ Simple Example: ]
    * Imagine you run a small bakery:
      - You sold 50 bread on Monday
      - 55 on Tuesday 
      - 53 on Wednesday
      - 60 on Thursday.

  - If you want to know 'how many bread to bake on Friday', you don't just guess
    randomly, you look at the 'past sales trend' ->> around 50-60 per day
  |-> That "educated guess" for Friday's sales is a "forecast".

===[ Forcasting Time Series Models ]============================================
* 'Time Series Analysis' Concept is a concept that data scientist tackle in
  their job
  - These are different approach to make prediction in case of time series 
    . ARIMA               |                               | Univariate model 
    . Prophet             | try to use this model first   | Univariate model
                           (handle missing values better)
    . Neural Prophet      | try to use this model second  | Univariate Model

    . Vector Autoregression (VAR)   | Multi-variate model

    |--------------------[ Time Series Components ]--------------------|
    |                   |                          |                   |
    |                   |                          |                   |
[ TREND ]         [ Seasanality ]             [ Cycle ]            [ VAR ]

# QUESTION:[ time series are always related the concept of forcasting, Why? ]===
# * A 'time series' is simply a "sequence of data points collected over time 
#   usually at equal intervals".
#   - Examples
#     . Daily freight volume at a port
#     . Montly sales
#     . Hourly electricity demand 
#     . Yearly population counts
#   formaly it is:
#     [ X = {x1, x2, x4, ..., xt} ]
#     where `xt` is the value observed at time `t`
# * Forcasting is the 'process' of using historical data to predict future
#   values.
#   > If the data is a 'time series' type, then we call it
#     |==> time series forcasting <===|
#   # NOTE:>--------------------------------------------------------------------
#   # - forecasting can also apply to 'non-time' data
# CONCLUSION:[ Relationship between 'time series' and 'forcasting' ]============
# - Time Series -> the type of data (ordered in time)   | data structure
# - Forcasting  -> the task (predecting future values)  | the predection task
# - Time Series Forcasting -> A specific type of forcasting where the input data 
#   is 'time-dependent (time series)'.
# - in simple word Predection of future points (values) in a time-ordered
#   dataset (time series) 
# REVISION:> Forcasting is predection technique as 'Estimation' and 'Prediction'

  # NOTE:>------------------------------------------------------
  # These are the list of models that i can use for the freight
  # predection project 


          |-------------------[ Time Series Model ]-------------------|
          |                                                           |
[ Time Series Traditional Model ]                           [ ML Models ]
          |                                                 - Regression with  
  |-------|-------------------------|                         lagged feature 
  |                                 |                       - Tree-based models 
[ Univariate Models ]           [ Multi-variate Model ]       (Random Forest, 
- ARIMA model                   - Vector Autoregression       (XGBoost, trained
- SARIMA (season ARIMA Model)     Model                       on time lagged 
  . It takes in the account                                   features.
    seasanlity                                              - Neural Networks
- Prophet                                                   - Hybrid models  
- Neural Prophet (neural network version of 'prophet')        combine ARIMA +
                                                              ML for residual
                                                              moderling.
# INFO:--------------------------------------------         --------------------
# + These models are 'Recursive', which means               > Make predctions
#   if we want to predict for 10 days in the future           directly depending
#   the model will prodect first day and use it to            one the horizon
#   predect the second one until the teenth day.            - Tough to Extend
# + Easy to Extend                                          > Training data 
# - Tough to get right                                        increases linearly  
# - Can't add time varying features varying features          as we have more
#   (mostly true for the univariate models)                   horisons to predict

  # INFO:-----------------------------------------------------------------------
  # - Uni-variate model means that the model deals with single time series 
  #   variable. 
  # - Multi-Variate model can handle multi-time series variable.
  # - Uni/Multi-variate model use depends on number of values we are predecting. 

  # NOTE:[ VERY IMPORTANT NOTE ]------------------------------------------ 
  # - All This model and approach that we have mentiend so far are treated
  #   as time series problem. this one can be converted as traditional  
  #   machine learning regression model. 

===[ ARIMA Model ]===
* 'ARIMA' stands for 'Autoregressive Integrated Moving Average' it is a
  forecasting technique used to predict future data based on past data, especially 
  for things that change over time (like sales)
  - The model contains three parts:
    1|> `AR` (AutoRegressive) 
    2|> `I`  (Integrated) 
    3|> `MA` (Moving Average)
  - In simple ARIMA combines patterns (AR), trends(I), and corrections for
    surprises (MA) to make your `X` predictions as accurate as possible.

===[ SARIMA Model ]===
* 'SARIMA' stands  for 'Seasonal AutoRegressive Integrated Moving Average' is
  extension of ARIMA that adds the ability to handle 'seasonality' (recurring
  patterns like weekly, monthly, yearly cycles).
  - Good choise when:
    . Your data show 'trend' (increasing/decreasing pattern)
    . Your data shows 'seasonality (repeated cycles): daily, weekly, yearly' 
    # NOTE:[ THE Last ONE IS VERY VERY IMPORTANT INFORMATION ]==================
    . You want an interpretable, classical model before moving to complex ML/DL 
      models

    - Advantages of SARIMA:
      . Handles 'trend + seasonality' explicitly.
      . Works well for 'small to medium datasets'. 
      . Provides 'statistical interpretability' (you can explain coefficients) 
      . Often a 'Strong baseline' before trying ML/DL models 

  > [ Using in Python ]

    pip install statsmodels

# NOTE:[ VERY IMPORTANT NOTES ]-------------------------------------------------
# - Regular EDA often ignores order and focuses on distributions, correlations,
#   and missing values.


===[ Time Series Analysis ]=====================================================

===[ Stationarity ]===
* A 'stationary time series' has statistical properties that 'do not change'
  'over time', mathematically means:
  - 'Constant mean' over time ('constant (no-long term trend)') 
  - 'Constant variance' over time

  # QUESTION:[ But Why does it matter ?]-------------------------------------
  # - Many classical time series models (AR, MA, ARMA, ARIMA), <assume
  #   stationarity> because they relay on stable patterns to make predections
  # - If the series is <non-stationary> (trends, changing variance), these
  #   models my give 'biased or meaningless forecasts'.

                      [ Stationarity  Types ]
                                 |
          |----------------------|--------------------| 
          |                                           |
  [ Strict stationarity ]                     [ Weak / Covariance stationarity ]
- the 'entire distrubution' series           - Only 'mean', 'variance', and  
  is unchanged over time.                      'autocorrelation' are constant
  (rare in practice)                           (commonly used in practice)

  # CONCLUSION:>----------------------------------------------------------------
  # Stationarity = series has stable 'statistical properties over time', which |
  # is essential for many time series models to work properly.                 |
  # ----------------------------------------------------------------------------

[ 'Variance' Transformation: Log && Box-Cox ]===
1|> [ Log Transformation]
  * you can simply use natural 'np.log' () transform the variance 

  # NOTE:> Make sure to stationize 'variance' first and then 'mean'. 

  - np.log(data['VOL_TRAFIC']) => another transformed dataframe

2|> [ Box-Cox transformation ]
  Box-Cox (Box Clock) transformation is a statistical technique used in
  'time series analysis (and general ML/Stats)', to make data more 
  'normal-like' and often more 'stationary'.

  from scipy.stats import boxcox

  data['TRAFIC_VOL_BOX_COX'], lam = boxcox(data['TRAFFIC_VOL'])

[ Differencing: Make Transformation to mean ('trend') ]===
* To make 'trend' mean constant
                  
          [       d(t) = y(t) - y(t - 1)      ]

  - in pandas use function .diff() | data['TRAFFIC_VOL'].diff() 

[ Stationarity test: READ ABOUT THIS Subject ]===
????

===[ Seasonality: ]===
* 'seasonality' is just as fundamental in time series as 'trend' or
  'stationarity'.
  - A time series is 'seasonal' if it shows predictable changes that recure
    every fixed period (day, week, month, year...).
  - Examples:
    . Weekly: shopping patterns (weekend spikes)
    . Daily: electricity demand (higher at night/daytime peaks) 

  # QUESTION:[ Why Seasonality Important? ]-------------------------------------
  # 1. Model Assumption: many forecasting models (like ARIMA) assume
  #    stationarity, 'Strong seasonality makes the series' non-stationary. 
  # 2. Bias in Forecast: if seasonality is ignored, forecasts may systematically  
  #    over/understimate during seasonal peaks and throughs.
  # 3. Business Insight: Seasonality explains why something happens (ex, sales
  #    in December not because of trend, but because of holidays) 
  # > [ Example ]
  # - Imagine monthly sales data:
  #   . Trend = increasing over years
  #   . Seasonality = spike every December (holidays)
  #   . Residual = random noise
  # - If your model without seasonality, the model might think December spikes
  #   are "trend," leading to poor predictions in other months.
  # - If you model with seasonality, it correctly adjusts for the December jump.

[ Handling Seasonality: ]===
1|> Transformation
    - Removing => [ d(t) = y(t) - y(t - m) ] => use 'pandas.diff(periods=nbr)'
    - Decomposition 
2|> Using Models that handle seasonality (e.g, SARIMA) 

===[ Decomposition ]===
* 'decomposition' means 'breaking down' a 'complex series' into simpler,
  interpretable 'componenents'. 
  # NOTE:>-------------------------------------------------------------
  # This help us understand underlying patterns rather than just seeing
  # a noisy curve
  - A time Series Y(t) is usually modeled as:

    [   Yt = T(t) + S(t) + R(t)   ] -> 'Additive model' 
    - When 'seasonal variations' are 'constant' over time. 
                OR
    [   Yt = T(t) * S(t) * R(t)   ] -> 'Multiplicative model'  
    - When 'seasonal variations' 'grow/shrink' with the 'level' of the
      series.

    1. 'Trend': T(t): the long-term direction of data (upward, downward, flat)
        example: sales increasing over years.
    2. 'Seasonality': S(t): repeating, predictable patterns at fixed intervals.  
    3. 'Residual/Noise': R(t) random 'fluctuations' left after removing trend
                         and seasonality.
                         
    # INFO:-------------------------------------------------------------
    # - Fluctuation: (noun) ÿßŸÑÿ™ŸÇŸÑÿ®, a change of the process of changing,
    #   especially continuously between one level or thing and another. 

  > [ Transformation from multiplicative to addictive ]
    - Use log or box-cox transformation to transform and stabilize the
      'variance', once the variance is stabilized the model becomes 'additive'.

# CONCLUSION:[ Decomposition Goal ]---------------------------------------------
# - To analyze each part separately (trend forecasting, seasonality 
#   understanding, noise detection)
# - To prepare data for forecasting models like `SARIMA`, `Prophet`, etc. 

  > [ How Decomposition is done in practice? ]
  - There are multiple algorithms and methods to decompose the time series into
    the three componenets, i want to go over the classic one, 
    
    1. Compute the trand componenet, 'T', using a 'moving/rolling' average. and
       then 'De-trend (remove)' the series, 'Y-T' for 'additive' model, 'Y/T'
       for 'multiplicative' model.
    2. Compute the Seasonal componenet 'S', by taking the average of the
       'de-trended' series for each season. 
    3. 'The Residual componenet R', is calculated as 'R=Y-T-S' for additive
       model and 'R=Y/(TS)' for multiplicative model. 
    # NOTE:> Calculate components and then remove it, and then calculate and
    #        remove it ... 
   
    - In python:
        # Import the seasonal decompose function 
        from statsmodels.tsa.seasonal import seasonal_decompose

        plot_multi = seasonal_decompose(data['seasonal'],
                                  model='additive/multiplicative') 
        # It will show all different componenets (T), (S), (R) in different
        # subplots 
        plot_multi.plot()
        plt.show()

# QUESTION:[ Additive Model VS Multiplicative Model ]=========================== 
# 1|> Additive model:
#     The parts of the time series simply add together:
#     [ Time¬†Series=Trend + Seasonality + Noise ]
#
#     > Example: Suppose sales grow by +100 units per year (trend) and there‚Äôs a
#       +30 spike every December (seasonality). No matter how high sales go, the
#       seasonal effect always adds 30 units.
#
# 2|> Multiplicative model:
#     The parts of the time series multiply:
#     [ Time¬†Series=Trend √ó Seasonality √ó Noise ]
#
#     > Example: Sales grow by 10% per year (trend) and December sales are 20%
#       higher (seasonality). Now, as the trend grows, the seasonal ups and
#       downs also get bigger in proportion.
#
# üëâ [ In short: ]
#    - Additive = seasonal/trend effects are constant in size.
#    - Multiplicative = seasonal/trend effects are relative (proportional).

# INFO:>---------------------------------------------------------------
# THERE ARE SEVERAL OTHER METHODS AVAILABLE FOR DECOMPOSITION such as 
# STL, X11, and SEATS, These are advanced methods and add to the basic
# approach from the classical method and improve upon its shortcomings.

===[ Autocorrelation (ACF): Autocorrelation Function ]===
* 'Autocorelation' is one of the most used 'tools' in time 'series analysis'
  and the 'core of time series analysis'.
  - 'Autocorrelation' (is also called 'serial correlation') 'measures' how   
    'related' a time series is with its 'own past values'.
  - In simple words: if todays value is strongly related to yesterday (or
    last week s, last month s) the series has 'autocorrelation'
  - It is just 'correlation', but instead of comparing two different variables,
    you compare a variable with its 'lagged versions'.

    [            Cov (Yt, Yt-k)           ]
    [    pk =   ------------------        ]
    [               simga^2               ]

  # NOTE:> IT is very simple and close to PEARSON'R correlation concept
  # - Pk is the correlation
  #   +1 -> perfect postive correlation (past and present move together)
  #   < +1 -> strong positive correlation 
  #   -1 -> perfect negative correlation (past up, present down)
  #   > -1 -> strong negative correlation 
  #   0  -> no correlation

  # QUESTION:[ What is a 'lag' in time series? ]---------------------------------
  # - A 'lag' is just a shift backward in time. It means "how many steps back in
  #   the past we look at to compare with the present"
  #   . 'Lag 1': yesterday compared to today
  #   . 'Lag 2': two days ago compared to today
  #   . 'Lag 12': 12 monts ago compared to this month
  # - Example:
  #   ___________________
  #   | Day | Temp (¬∞C) |
  #   |-----| ----------|
  #   | 1   | 22        |
  #   | 2   | 24        |
  #   | 3   | 23        |
  #   | 4   | 25        |
  #   |_____|___________|
  # - At lag 1 -> compare day2 with day1, day3 with day2, day4 with day3 
  # - At lag 2 -> compare day3 with da1, day4 with day2
  # So, lag="the time gap between the two values we are comparing"
  #üîπ Why it matters
  #   . If a series has high correlation at lag 1, it means yesterday strongly
  #     influences today (good for autoregressive models).
  #   . If there‚Äôs a spike at lag 7, maybe the data has weekly seasonality.
  # CONCLUSION:[ Analogy ]------------------------------------------------------
  # - A 'lag' is like a memory -- how much the past influences the present, with
  #   the number telling you how far back you look.

# CONCLUSION:[ PURPOSE ]--------------------------------------------------------
# - Detects 'patterns & dependencies' in data  
# - Helps idenfity 'seasonality' (e.g. high correlation at lag=12 months, yearly
#   seasonality) 
# - Used in model Building (AR, ARIMA, SARIMA rely heavily on autocorrelation)
#   detect how many lags to use to build the models.
# - In general 'correlation' is just a 'normalized covariance', always between
#   -1 and 1
#               Cov(x, y) 
#   p(x, y) = ------------
#             std(x) * std(y) 

[ Autocorelation In Python ]===
    from statsmodels.graphics.tsaplots import plot_acf

    plot_acf(data['TRAFFIC_VOLM'], lags=nbr)

    plt.show() 

# INFO:[ TOOLS ]----------------------------------------------------------------
# - ACF (AutoCorrelation Function): shows correlations at different lags. 
# - PACF (Partial AutoCorrelation Function): shows direct effect of a lag, 
#   removing intermediate effects.
# > [ Exmaple Intuition ]
#   . If ice cream sales today are highly correlated with sales yesterday
#     (lag 1), we‚Äôll see a strong autocorrelation at lag 1.
#   . If sales follow a 'weekly pattern', lags at 7, 14, 21‚Ä¶ will show peaks in
#     the autocorrelation function. 
# üëâ These plots are standard for diagnosing which ARIMA/SARIMA parameters to use

===[ Partial Autocorrelation (PACF) ]===
- 'Autocorrelation': Measures how current values are related to past values
  (lags)
- 'Partial Autocorrelation (PACF)': Measures the correlation between a value
  and its lag 'after removing the effect of shorter lags'.
  > Example:
    . Autocorrelation at lag 5 includes effects from lags 1-4
    . Partial autocorrelation at lag 5 removes those and shows the 
      'direct relationship' between `Yt` and `Yt - 5`

# QUESTION:[ Why it is important? ]---------------------------------------------
# - In 'ARIMA/SARIMA modeling', PACF helps choose the 'order of the AR
#   (AutoRegressive) term'.
#   . If PACF cuts off after lag 'p', it suggests AR(p) might be appropriate
# - ACF (Autocorrelation Function): on the other hand, helps choose the 'MA'
#   (Moving Average) term
# > So:
#   . ACF  -> MA terms, it help us identify the 'q' parameter in ARIMA (MA part)
#   . PACF -> AR terms, it help us identify the 'p' parameter in ARIMA (AR part)

[ Partial Autocorelation In Python ]===
    from statsmodels.graphics.tsaplots import plot_pacf

    plot_acf(data['TRAFFIC_VOLM'], method='')

    plt.show() 

# CONCLUSION:-------------------------------------------------------------------
# - Correlating a time series with itself, ignoring the effect of intermediate
#   lags, PACF is not really used that often but it's mainly used in determining
#   number of auto regressor componenets when an ARIMA model, so it is gold
#   standard when fitting forecasting model.  


===[ Forecasting Metrices: To evaluate a Time series Model ]===
1|> Mean Absolute Error (MAE) (most classic one) 
          sum(1->n) (Yi - Fi)             |- Yi: is the actual value at `i`
    MAE = -------------------             |- Fi: is the forcasted value at `i` 
                  n                       |- n: number of samples

    > adv:
      - Intuitive
      - Error in same units as forecast
    > dadv:
      - It does not penalize outliers
      - Scale dependent 

2|> Mean Squared Error (MSE) 
    - It is known for me and it is used in the 'Cost function', instead of 
      abs value, we use the '^2' 

    > adv:
      - Penalise outliers 
    > dadv:
      - Less interpretable
      - Scale dependent

3|> Root Mean Squared Error (RMSE)
    - sqrt(MSE)

    > adv:
      - Penalise outliers
      - Error in forecast units
      - Best of both worlds of MSE and MAE  
    > disadv:
      - Less interpretable 
      - Scale dependent 

4|> Mean Absolute Percentage Error (MAPE) (percentage difference) 
            sum(1->n) ( 100 * (|Ai - Fi| / Ai) )
    MAPE = -------------------------------------
                n

    # NOTE:--------------------------------------------------------------
    # - This is the base line matric used when building forecasting model
    
    > adv:
      - Easy to interpret as precentage
      - Scale independent
    > dadv:
      - Infinite error if the actual value is near zero 
      - Biased to under-forecast 

5|> Symmetric Mean Absolute Peercentage Error (SMAPE)
6|> Mean Sqauared Logarithm Error (MSLE) 

# INFO:---------------------------------------------------
# - No Metric is Best than the other.
# - Metric used depends on the context 
# - advice: good to use more than one metric for you model

===[ Basic Forecasting Methods for Time Series Analysis ]===
* forecasting is a wide domain iwth numerous applications in almost every
  industry, Due to this, the range of forecasting models is also very large
  with each model having its own adv and dadv.
  - In this toturial we are go voer some basic and simple forecasting models,
    despite their cimplicity, these models can offer good resutls in practie
    and provide a good basis to iterate from.

  1|> Average Forecast
      - had l module sahel bzff, it assumes all the future values are equal to
        the 'mean' of the all the previous values., that is why is called
        'average Model'.
        _________________________________________________
        [                                               ]
        [                     sum(t:1->T) Y(t)          ]
        [ Y-Hat = Y(t + h) = ---------------------      ]
        [                             T                 ]
        [_______________________________________________]

  2|> Naive (mkalkh) Forecast
      - This model sets the future forecast equat to the latest observed value
        ___________________________________________
        [                                         ]
        [           Y(t + h) = Y(t)               ]

  3|> Seasonal Naive Forecasting 
      - This method is an extension of the naive model, but this time the
        forecast is equal to the most recent observed value in the same 
        season, Ex: the forecast for the next quarter one is equal to the
        previous years quater one value. This model is useful when we have
        a clean and large seasonal variation in our time series.
        ___________________________________________
        [                                         ]
        [       Y(t + h) =  Y(T + h - m)          ]

        . `m` is the seasonality of my data, for monthly data with a yearly
          seasonality m=12, quarterly data would have m=4 and weekly data  
          would have m=52

  4|> Drift (or trend) Model
      - This also an extension of the naive forecast whwere we let the
        predection either linearly incrase or decrease through time as a
        function of time step, h, scaled by the average historical trend 

        ___________________________________________________
        [                                                 ]
        [       Y(t + h) = Y(t) + h * trend_growth        ]

                          Y(t) - Y(t=1)
        . trend_growth = --------------- = slope 
                              t - 1

        # NOTE:>----------------------------------------------------------------
        # - THIS Model makes more sense to me actually
        # - But this model does not capture the seasonality, It is just a line

# CONCLUSION:>------------------------------------------------------------------
# - These basic models are not that good
# - They provide a good baseline
# - Use them as comparison models.

===[ Exponential Smoothing Forecasting Models ]=================================

===[ Simple Exponential Smoothing ]===
* The idea of exponential something originated in the 1950s and is basically a
  fancy way of stating that we will put more weight on recet observations, older
  observations will receive less weight at an exponentially decaying rate, hence 
  it is called 'exponential something'. (in short just exponentialy something
  the data by adding more weights to previous and less weight to last)

  [                                                                           ]
  [   Y(t + 1) = W * Y(t) + W(1 - W) * Y(t - 1) + W(1 - W)^2 * Y(t - 2) + ... ]
  [                                                                           ]

    . `W` is the smothing parameters   0 <= W <= 1
    . The smoothing parameter is the main parameter we need to estimate
      when fitting this model. The parameter is often calculated by the method
      of 'least squares' which is fortunately done for us by most computing
      packages.
    . The higher the value of `W`, the more weight is put on recent observations
      and vice versa for lower values of `W`, if `W=1` then we recover the naive
      forecast model where all forecasts are equal to the most recent 
      observation.

  - Like all time series exponential smoothing methods are often broken into
    their components such as 'level', 'trend' and 'seasonality', As simple
    exponential smoothing does not take into account 'trend' of 'seasonality',
    it is therefore only consists of the 'level component'. 

    Overall Equation: [ Y(t + h) = l(t) ]           => `l`: level
      . [ L(t) = W * y(t) + (1 - W) * l(t - 1) ] 

    > The formula says: ‚ÄúThe forecast for any future point steps ahead is just"
                        " equal to the current level at time ‚Äù

  - 'Simple Exponential Smoothing' This is the most basic forcasting model in
    the 'exponential something family', The reason this model is simple is
    because it does not take into account 'trend' or 'seasonality'.
  - In general this simple model is best when your data does not have any
    'trend' or 'Seasonality', Alternatively, you can transform  your data by
    carrying out 'differencing' and the 'Box-Cox transform' to make it
    "stationary", hence removing its trend and seasonality.

  # INFO:[ Deep Understand of 'Level' ]========================================
  # - The 'level' is just the 'average or baseline value' around which your
  #   series fluctuates "when there's no trend or seasonality".
  # - Think of it as:
  #   . The "center line" of the data
  # > [ Example: ]
  #   - suppose your daily sales are:
  #     98, 102, 100, 99, 101
  #     . They bounce up and down but hover around around ~100 
  #     . That "~100" is the level.
  # - Why Simple Expnential Smoothing cares about level
  #   . this model assumes the series has 'no growth (trend)' and 'no repeated'
  #     'cycles (seasonality)'
  #   . So the only thing it needs to model is the 'current baseline (level)'
  #   . forecasts then extend this baseline forward.
  # QUESTION:[ But why using this concpet instead of using mean? ]==============
  # - If the series is 'perfectly stable' (no changes over time) then yes
  #   the mean would work fine as the 'level'.
  # - But in real time series, the 'baseline shifts' as new data comes in.
  #   . Example: sales hover around 100 for a while, then around 120 later
  #   . A fixed mean won't adapt
  # - SES does not take a simple mean, Instead if updates the 'level'
  #   'dynamically' with each new observation:
  #   [ L(t) = alpha * Y(t) + (1 - alpha) L(t - 1) ]
  #   . L(t) = estimated level at time (t)
  #   . y(t) =  actual observation 
  #   . alpha = smoothing factor  
  #   => This formula is like a 'moving, weighted mean' where recent values
  #      count more.
  # - In shorts:
  #   . The mean = "one-time average of everything"
  #   . The SES level = "continuously updated average, with memory that fades"
  #     "for older data".
  #   > This is why SES adapts if the series drifts upward or downward
  # CONCLUSION:=================================================================
  # - In short in Simple Exponentail smoothing is the current "average baseline" 
  #   (level) of the series
  # - SES formula is just a smart way of 'tracking a moving baseline' that
  #   adjusts as new data arrives.
  # ============================================================================

  > [ How to find Alpha value? (Œ±) ]
    - In practice, we usually fit by minimizing a forecast error metric:
      [ Œ± = arg min (SSE(Œ±))  ]

      . where:
        [   SSE = sum(t=1:->T)(y(t) - Y-hat(t, Œ±)^2   ]

  > [ Simple Exponentail Smoothing in Python ] 

    from statsmodels.tsa.holtwinters import SimpleExpSmoothing

    model = SimpleExpSmoothing(data['feature'])
    model.fit(optimized=True)
    forcasts = model.forecast(test) 

    # plot forecasts to see the resuls
    # look at a summary about the model
    model.summary() # It is a useful function try to use it.

# CONCLUSION:-------------------------------------------------------------------
# - Simple Exponential something model is the basic model for forecasting      | 
#   1. Simple as does not take into account 'seasonality' or 'trend'           |
#   2. Weights recent observations more than older ones                        | 
#   3. Not very good because its so simple                                     |
#   4. Good baseline model => flat forecast (all forecasts will be same and    |
#      equal to the most recently observed value)                              |
# ------------------------------------------------------------------------------

===[ Holt's Linear Trend Model ]===
# REVISION:---------------------------------------------------------------------
# - The gist of exponential smoothing is to put more weight on recent
#   observations and less weight, exponentially on more historical ones.
# - The previous model is simple, because does not take 'trend' or 'seasonality' 
#   into consideration, it does only forecast level, This part leads us to this
#   particular model.
* Holt s linear method (also known as 'double' exponential smoothing), which
  like its name suggests, 'adds' a '(linear) trend component' to the 'simple'
  'exponential smoothing model'.

      [ Y(t + h) = L(t)  + H * B(t) ]

      # NOTE:-------------------------------------------------------------------
      # b + a * x: linear form that is why called Holt's Linear Trend Model. 

    Smoothed Level = alpha * Recent_actual + (1 - alpha) * (Prev level - Prev Trend) 

      . level equation: L(t) = W * Y(t) + (1 - W) * (L(t - 1) + B(t - 1)) 
                              --- * ---  + ----- *  ---------------------  
            smoothing parameter:   Y(t)   weight     Y(t - 1)

      . Trend Equation: B(t) = beta * (L(t) - L(t - 1)) + (1 - beta) * B(t - 1)  
                               -----  ---------------- +  ---------- * ------- 
              trend smoothing factor        B(t)          Weigths      B(t - 1)

                    Y(t) - Y(0)
      where: b(0) = -----------  => trend is gonna change over time
                        T
      . 'h': is the time step to forecast in the future. 

      # NOTE:>------------------------------------------------------------------
      # - As you can see the Trend Equation does put more weights on recent
      #   trend (which is difference between last two levels), that is way
      #   called double smoothing.
      #
        . 'beta': is the 'trend smoothing factor' 0 <= beta <= 1 
      
      - one issue with this 'current formulation is that the forecasts will'
        'increase or decrease arbitrarily into the future', in reality nothing
        grows nor decays indefinetely, therefore, there is often a dapening term
        `Q` added to curtail the forecasts at long horizons.


      . Overall Equation: Y(t + h) = L(t) + (Q + Q^2 + ... + Q^h) * b(t) 
      . Level Equation: L(t) = W * Y(t) + (1 - W) * (L(t - 1) + Q * b(t - 1)) 
  . Trend Equation: b(t) = beta * (L(t) - L(t - 1)) + (1 - beta) * Q * b(t - 1) 

      . where `0 < Q < 1`, the reason it cannot be 0 or 1 is to ensure some
        dampening indeed occurs. if Q=1 then the model would just be the
        vanilla holt's linear trend model (method).


      # INFO:------------------------------------------------------------------
      # - horizon: (noun) the line in the distance where they sky and the land 
      #   or sea seem to meet ( ÿ£ŸèŸÅŸèŸÇ ) 

  > [ Hotl's Linear Model In Python ]

    from statsmodels.tsa.holtwinters import SimpleExpSmoothing, holt

    # 1|> Using simple exponential model 
    model = SimpleExpSmoothing(data['feature'])
    # simple exponential smoothing model (does not handle 'trend' or
    # 'seasonality') 
    fited_model = model.fit(optimized=True)  
    simple_forecasts = fited_model.forecast(len(test))

    # 2|> Using holt's linear model (double smoothing model)
    model_holt = holt(data['feature'], damped_true=True) 
    holt_fited_model = model_holt.fit()

    # forecasts using holt's linear model
    holt_forcasts = holt_fited_model.forecast(len(test))

    # this function to understand what is happening under the hood
    model_holt.summary() 


# CONCLUSION:-------------------------------------------------------------------
# - holt's linear model is much better, it handles both 'level (~mean)' and
#   'trend'. we are one step away from gitting the best time series forecasting
#   model. the (seasonal componenet is missing).

===[ Holt Winters Method (Model) ]===
# REVISION:--------------------------------------------------------------------- 
# - we have bee discussting a very well known family of forecasting models,
#   exponential smoothing.
# - The fundamental principle of exponential smoothing is to put more weight on 
#   recent observations and less on historical observations as a means to
#   forecast the time series.
# - The most basic exponential smoothing model (is called also single exponentail
#   smoothing model), this model just forecast the 'level' of time series and 
#   does not take into account 'trend' or 'seasonality'
# - The next step from thsi simple model is holt's linear trend method, which is
#   also known as 'double exponential smoothing model', this model incorporates
#   the 'trend' as well as the 'level'.
# - Finally, the next step from Holt's method is to find a way to include
#   seasonality in the exponential smoothing model, this is WHERE 'Holt Winters'
#   (triple exponential smoothing) comes in.
* As stated above, the holt Winters model further 'extends' 'Holts linear trend' 
  'method' by adding 'seasonality' to the forecast. The addition of
  'seasonality' gives rise to two different Holt winters model, 'additive' and
  'multiplicative'.
  - The difference between the two models is the size of the 'seasonality'
    'fluctuations' are mostly 'constant', However, for 'multiplicative model'
    'the fluctuations' are 'proportional to the value of the time series at' 
    'that given time'.

  > In general the Holt's winter models consists severla components: 
    [ Next Forecast = Current Level + alpha * (Recent Acutal - Current Level) ] 

    - Current Level: (baseline)
    - Recent Actual: Recent Observation  
    - alpha: fine-tuner

    ===[ Example: ]===
      - let's take for example 
        . current level = 100 cups sold (baseline)
        . yesterday level = 120 cups sold 
        . let's imagen 'alpha=0.2': is usually selected 
        => tomorrow forecast: 100 + 0.2 * (120 - 100)) = 104 
 
  > [ Additive Model: ]
      
  Overall Equation: [ Y(t + h) = L(t) + h * b(t) + S(t + h - m) ] 
  Level Equation: L(t) = W * (Y(t) - S(t - m)) + (1 - W) * (L(t - 1) + b(t - 1)) 
  Trend Equation: b(t) = beta*(L(t) - L(t - 1)) + (1 - beta) * b(t - 1)  
  Seasonality Equation: S(t) = gama*(Y(t) - L(t - 1) - b(t-1))+(1 - gama)*S(t-m)  
                                    --------------------------
                      time series at (t) - trend - level = Seasonality
  # NOTE:----------------------------------------------------------------------
  # - The concept here is simple , we take the trend and level, so the
  #   seasonality is left, and we do some exponential smoothing by adding 
  #   'gama' as the smoothing factor for 'seasonality componenet'
  #
  . 'm' is the seasonality of the time series, s(t) is the seasonal forecast 
    componenet, s(t - m) is the forecast for the previous season and 'gama' is 
    the seasonal componenet factor `0 <= gama <= 1 - W`

  > [ Multiplicative Model: ]
  Overall Equation: Y(t + h) = (L(t) + h * b(t)) * S(t + h - m)  
  Level Equation: L(t) = W * (Y(t) / S(t - m)) + (1 - W) * (L(t - 1) + b(t - 1))
  Trend Equation: b(t) = beta*(L(t) - L(t - 1)) + (1 - beta) * b(t - 1) 
  Seasonality Equation: 
    [ S(t) = gama * (Y(t) / (L(t - 1) + b(t - 1))) + (1 - gama) * S(t - m) ] 

  > [ Holt Winters Model In Python ]
    # imports different exponential smoothing models
    from statsmodels.tsa.holtwinters import SimpleExpSmoothing, holt, ExponentialSmoothing 

    # fit simple model and get forecasts
    model_simple = SimpleExpSmoothing(train['feature']).fit(optimized=True) 
    forecasts_simple = model_simple.forecast(len(test)) 

    # fit Holt's model and get forecasts
    model_holt = holt(train['feature'], damped_trend=True).fit(optimized=True)
    holt_forecasts = model_holt.forecast(len(test))

    # fit  Holt winters model and get forecasts
    model_holt_winters = ExponentialSmoothing(train['feature'],
                                        trend='mul', # multiplicative
                                        seasonal='mul' # multiplicative
                                        seasonal_periods=12).fit(optmized=True) 
    forecasts_holt_winters = model_hotl_winters.forecast(len(test))

# CONCLUSION:-------------------------------------------------------------------
# - Holt winter model is the final model in exponential smoothing model family,
#   it is the best model so far, it takes 'level', 'trend' and 'seasonality'
#   into consideration (all three componenets are included).

===[ Residuals Analysis ]===
*  Being able to analyse your timie series model is essential to diagnose it 
   'performance', One such way to do this is through the 'residuals' of the
   'fitted model', It time series 'residuals r' are the difference between
   the 'fitted value  `y-hat`' and 'actual value `y`'
        [ r = Y(i) - Y-hat(i) ] 
  > Purpose: useing residuals Analysis (diagonise) to improve our forecasting
             methods.

  > [ Residual Analysis ] 
    - We can use the residuals to analyse how well our model has captured the 
      characteristics of the data. In general residuals should:
      . show 'very little' or 'no autocorrelation' or 'partial autocorrelation', 
        if they 'have' the 'model has missed some information' that is in the
        data. We can use 'Ljung-Box statistical test' and a 'correlogram' to
        determine if the 'residuals' are 'indeed correlated'.
      . the 'mean' of the residuals should be 'zero', otherwise the forecast 
        will be biased, In reality this is quite easy to aadjust for by simply
        'adding' or 'substracting' the bias from the forecasts. if mean!=0
        |=> under, over forecasting exist.

  > [ Difference Between 'Error' and 'Residual' ] 
    - The 'error' is the difference between the 'actual' and 'forecasted' values
    - the 'residuals' are the 'observed difference' between 'actual' and 'fitted'
      values from your model.
    > Ex:
      . 'Error' = (True future temp - your forecast), But you only know it once
        the 'REAL FUTURE ARRIVES'. (theoretical: involves the unobserved 'true'
        model).
      . 'Residual': (Observed temp in your dataset - models' fitted value), you
        always have residuals for the training data because both actual and 
        fitted are known. (observed: computed from data)

  > [ Residuals In Python ]
    # Getting fitted values by simply using fittedvalues attr 
    fitted = model.fittedvalues 
    # Getting Residualas by simplyg using resid attr
    resid = model.resid

    # NOTE:[ Video 13: rewatch this video ] 
      
# CONCLUSION:-------------------------------------------------------------------
# - Residuals are the difference between actual values and fited values, It is  
#   used to determine where our model may be struggling for example residuals
#   can identify things such autocorrelation and bias in the forecasting model 
#   and therefore we can correct this when we are rebuilding our forecasting
#   model. This is really useful it allows you to understand what the model's
#   doing and better diagnose it and so you can improve this forecast in next
#   iteration. This analysis is easy to do, so it's recommended to be done. 

===[ Coss-Validation For Time Series Forecasting ]===
- corss-validation is good technique to check model performance on all time 
  periods.
- We have to check for the model in different time periods, (e.g. last time
  week, and week before last week, etc)

  # NOTE:-----------------------------------------------------------------------
  # - It is very important for time series to test the model in different
  #   seasonal periods.
  # - if the data is not used in the final model, it should be used for
  #   validation. 

    |---------------------[ Cross-validation types ]---------------------|
    |                                                                    |
[ Rolling Forecasts ]                                       [ Sliding Forecasts] 
- Rolling CV extends                                      - Sliding CV keeps the   
  the training data                                         same train length


  > [ Cross-Validation in python ]
    # import libraries
    from pmdarima import auto_arima, ARIMA, model_selection

    cv = model_selection.RollingForecastCV(h=1, step=1, initial=None)  
    
      . 'h': (opt) is the forecast horizons
      . 'step': (opt) The size of step taken to increase the training sample
                size 
      . 'initial': (opt), the initial training size, if 'None', will use 1/3 
                   the length of the time series

    cv_score = model_selection.corss_val_score(model,
                                               y=data['revenue'],
                                               X=Exogonous_set,
                                               scoring='mean_squared_error',
                                               cv=cv,
                                               verbose=1,
                                               error_score=10000000000000000)

===[ ARIMA Models Family ]======================================================

===[ AutoRegressive Model (AR) ]===
# REVISION:---------------------------------------------------------------------
# - In previous we covered some basic forecasting models like 'exponential'
#   'smoothing' models family, In this post we will discover another family of 
#   (ARIMA family) forecasting models begining with autoregression.
  - 'Autoregression' is one of the most 'applied' and 'common models', It is not
  necessarily the best, but it really good inial building block towards our
  'ARIMA' which is kind of like the gold standard for models nowadays.
  - Autoregression is when you forecast a time series using some linear weighted
    combination of the previous values (lags) of that time series, As we are
    'regression' a 'target value' against 'itself'. It is called auto-regression
    , Mathematically, we can write autoregression as:

  # INFO:[ Simple And Friendly Explanation ]------------------------------------
  # - Imagine you want to guess today's temperature
  #   . You don't check the weather app
  #   . Instead, you just look at yesterday's temperature and say: 
  #     "Today will probably be about the same as yesterday, maybe a little"
  #     "higher or lower"
  # - That is the core idea of 'Autoregressive model': it predicts something 
  #   'based on its own past values'.
  #   . If you use only "yesterday's value", that is an `AR(1)` model 
  #   . If you use the past "two day's values", that is an `AR(2)` model
  #     and so on.
  #     # NOTE:> It is like saying: "THE PAST EXPLAIN THE PRESENT"

  * An 'autoregression (AR) model' is a time series model where the current
    value of the series is expressed as a 'linear combination ot its past'
    'values' plus some random error. 
      
      # NOTE:> It is in the form of multi-linear regression model.
      [   Y(t) = C + Q1 * Y(t-1) + Q2 * Y(t - 2) + Qp * Y(t - p) + e(t)   ]

  . `Y`: is the time series we are forecasting at various time steps.
  . `Q`: are the fitted coefficients of the lags fore the time series.
  . `e`: is the error term (typically normaly distributed)
  . `p`: is the number of lagged componenets included in the model, this is also
         known as the order.

      > If we have no coefficients or they are all zero, this is just 'white noise'. 
      > If we only have Q1=1 and the other coefficients are zero, then this is a
        'random walk'.

  > [ Requirements: ]
    - It is recommended to have 'stationary' time series, We need stationarity
      to ensure the statistical properties of the time series is consistent
      through time. (rendering it easier to model)
      # REVISION:>--------------------------------------------------------------
      # Differencing & box-cox or lag transformation to ensure stationarity.
      # (stabilize 'variance' and 'mean')

      # NOTE:-------------------------------------------------------------------
      # - Data must belongs to the same distribution 
    - Using all time series forecasting may cause model overfitting, so we need
      to select some those, we have to find optimal value `p` and basically the 
      way we can do this is through 'plotting the PACF method (correlation'
      'between lags ignoring the middle effect)'
      - We need to find the optimal number `p` of lags we can use for the model 

  # QUESTION:[ Why this model is called Auto-Regression? ]---------------------
  # 1|> 'regression' in statistics means we try to explain or predict a
  #     variable using other variables 
  #     - Ex: Predicting someones's weight using their height -> that is a 
  #       regression model.
  # 2|> 'Auto', means self, So instead of predecting a variable using
  #     'different variables (like height to predict weight)', here we predict
  #     a variable 'using its own past values'.
  # 3|> Putting it together
  #     - 'Regression': -> prediction using explanatory variables
  #     - 'Autoregression': -> prediction using the 'variable's own past values'
  #       as the explanatory variables.
  #     => "A regression of the variable on itself (with a time lag)"

  > [ Walkthrough In Python ]
    1|> plot the time series values 
    2|> make sure that time series is 'stationary'
        - Use differencing to 'de-trend' the time serie
        - Use log or box-cox transform to make 'variance' constant
    3|> Finding the number of lags that we need `p` 
        - Using 'PACF' ploting method
          from statsmodels.graphics.tsaplots import plot_pacf 
    # INFO:---------------------------------------------------------------------
    # - When you build an 'autoregression model' the big question is:
    #   "How many past steps should i look at to predict the present?"
    # - PACF is the tool that tells you how far back in time you should look
    #   when building an 'AR' model.
    # - PACF: direct correlation between today and a past value, ignoring
    #   everything in between
    #   > If the 'direct correlation is high' -> the past value really matters 
    #     to predict today
    #   > if the 'direct correlation is near zero' -> the past value does help
    #     predict today.
    # - So when choosing 'how many past values (lags) to use In (AR) MODEL'
    #   . Keep the lags that have 'high PACF' 
    #   . Stop at the first lag where to PACF is basically zero -> no need to 
    #     include further past values.
    #   > Example:
    #     - lag 1 correlation = 0.7 -> to day depends a lot on yesterday
    #     - lag 2 correlation = 0.2 -> some effect, maybe include it
    #     - lag 3 correlation = 0.0 -> today 'does not depend' on the value 3
    #       days ago -> ignore it.
    # - Basically: Hight PACF -> include, near zero -> ignore

    4|> Build the model
        from statsmodels.tsa.ar_model import AutoReg, ar_select_order
        # Build AR model
        # Try from 1 -> 15 lags to see with ones gives the best results
        # INFO:-----------------------------------------------------------------
        # - `ar_slect_order` is a helper function to 'choose the best lag order' 
        #   for an AR model.
        # - It tries different values of (p) lags and compares them using  
        #   information criteria (AIC, BIC, HQIC)
        selector = ar_select_order(train['feature'], maxlag=15)
        # Buid The Model
        model = AutoReg(train['feature'], lags=selector.ar_lags).fit()
    5|> Use Inverse Box-Cox to the 'forecasted values' 
        from scipy.special import inv_boxcox

        forecasts = inv_boxcox()

# CONCLUSION:-------------------------------------------------------------------
# AutoRegression is where you forecast a time series using some linear weighted
# combination of the previous values (lags) of that time series, As we are
# regressing a target value against itself, it is called auto-regression.

===[ Moving Average Model (MA) ]===
* The 'MA' model is away to "predict today's value using the 'error' (random"
  "chocks) from revious days" 
  - This model does not look 'directly at past values like (AR) model': it looks 
    at how much yesterday (or the past few days) 'differed from what exprected'
  - think of it as:
    [ "What happened unexpectedly in the past few days will influence today" ]

    [ X(t) = U + e(t) + O1 * e(t - 1) + O2 * e(t - 2) + ... + Oq * e(t - q) ]
                        -------------
                        an error term

    . U: Average of all observed past value of the series (like baseline)
    . e(t): random chock (error) today
    . e(t-1), e(t-2), .. = past errors (random choks)
    . O1, O2, ... = weights for each past error
    . q = number of past error included 

  - A big Question we should ask (order selection): 
    "How to get the number of past error included `q`"

    - We simply use 'ACF' technique to define `q` number of past error included
      . If the a lag has high correlation, then it is 'influential'. 
      . otherwise if it is not 'highly correlated', the it is not 'influential'. 

  > Friendly Interpretaion:
    . "Today's value" = 'baseline (average past observations)' + 
                        'weighted errors from previous observations'


  > [ Estimation: (Fitting terms coefficients {O1, O2, ...Oq}) ]
    - This process is not as easy as in linear or autoregression as the erros
      are not observable, so how do we do it?
     
      well, It is not so straighforward, However, the general gitst is that the
    'autocorrelation values for each lag are directly related to their cofficients'
     therefore, once yu know the 'autocorrelation' of a 'forecast error' you can
     work bakwards to acquire its coefficient.
                                      O1
                          C1 = -----------------
                                  1 + O1^2

    # NOTE:---------------------------------------------------------------------
    # - The weight terms for (MA) are different (in way of estimation) than    |
    #   from (AR) model, this is they key point you must REMEMBER.             |
    # --------------------------------------------------------------------------

  > [ Walkthrough In Python ]
    1|> Make you time series 'stationary'  
        - Use differencing to 'de-trend' the model 
        - Use box-cox or log transformation to 'stabilize the variance' 
        from scipy.stats import boxcox

        data['feature_transformed'], lam = boxcox(data['feature'])
    2|> Apply 'Auto-Correlation function (ACF)' to determine the order we are 
        going to use 'q'.

    3|> Use ARIMA Model
        from statsmodels.tsa.arima.model import ARIMA

        model = ARIMA(train['feature'], order=(0, 0, 13)).fit()
        # ERROR:>> Make a Search about Order

    4|> Create New Forecasts
    5|> Make inverse_transformations and differencing 

    # INFO:---------------------------------------------------------------------
    # - The blue region in the 'ACF' plot represent where the statistically 
    #   significant (so anything in that region we dont really want, we want
    #   the ones outside the blue region)
    # - Even if you can clearly (hard to define using eye) see the lags that can
    #   be used to the model it is a 'GOOD WAY TO BRUT FORCE APREACH ALL
    #   POSSIBLE LAGS TO USE'.
    # CONCLUSION:--------------------------------------------------------------
    # - This model does not give a good results comparted to 'Auto-Regressor'
    #   model, but its power comes win it gets combined with (AR) Model
    #   (AR) + (MA)
    
# CONCLUSION:-------------------------------------------------------------------
# - Autoregression (AR): Forecasting based on previous time series observations 
# - Moving Average (MV): Forecasting based on previous 'errors' (estimating
#   errors)
# - These models required a 'stationary' time series dataset. 

===[ ARIMA Model ]===
* We have covered 'autoregression (AR)' and 'moving-average (MA)' models,
  However do you know what is better than these two models?
  => A single model that 'combines them!'
  - Autoregressive Integrated Moving Average better know as 'ARIMA', is
    probably the most used time series forecasting model.
  > 'Autoregression (AR)': it is a model that forecast values using a 'linear'
    'combination of the previously observed values'.
    # NOTE:> (AR) model works perfectly when the time series is staionary

  > 'Moving Average (MA)': This model learns from the 'errors' not from the past 
    observations.

      [ ARIMA = (AR) + (I) + (MA) ]

    - (I) differencing, this makes the series 'stationary', meaning patterns
      and correlations are stable this stability makes (AR) and (MA) reliable
      - The (I) part is not about finding trends, It is about 'removing trends'
      'so (AR) and (MA) can do their job properly'.
      >> In short 'Integration means differencing the data' substracting the 
         previous value from the current one. 
         - Example: Sales = [100, 105, 110, 115] 
         . differences [5, 5, 5]
         |=> now the trend is gone, it is just a flat line of '5'
         |=> (AR) and (MA) ARE THEN APPLIED TO THIS DIFFERENCED SERIES.
      >> After forecasting, the model 'integrate back' (adds the differences
         up) to return predictions in the original scale.

    # INFO:[ Differncing Outside ML and ARIMA Model ]---------------------------
    # - It is simply looking at the 'change between two values instead of
    #   the values themselves' 
    # - I have noticed that differncing also describe growth but in a  
    #   'discrete way (step by step)' in the same sequence otherwise
    #   derivative describe the growth in a function (relation between 
    #   multi-variables)
    # __________________________________________________________________________
    # | Aspect      | Differencing                      | Derivative           |
    # |-------------|-----------------------------------|----------------------|
    # | Domain      | Discrete data (steps, sequences)  | Continuous functions |
    # | Change type | Step-to-step change               | Instantaneous change |
    # | Precision   | Approximate (depends on step size)| Exact (as step ‚Üí 0)  |
    # |-------------|-----------------------------------|----------------------|
    # | Example     | Sales went from 100 ‚Üí 120 ‚Üí       | Growth rate of sales | 
    # |             | change = +20                      | curve at day         |
    # |             |                                   | 5 = slope of tangent |
    # |_____________|___________________________________|______________________|

      # NOTE:>----------------------------------------------------------------
      # - In calculus, differencing is the 'discrete' version of 'derivatives'
      # - Both concepts are related (both measure change) but differencing
      #   is for 'discrete jumps' while derivatives are for 'smooth, continous'
      #   'changes'.

  # INFO:[ ARIMA Components Analogy ]-----------------------------------------
  # Think of ARIMA Like riding a bike on a hill:
  #   . (AR) = your balance (keeps track of past moves)
  #   . (MA) = correction when you wobble
  #   . (I) = flattening the hill first, so you balance actualy works!
  # _________________________________________________________________________
  # [ If the hill is too steep (trend), you will topple over no matter      ] 
  # [ how good your balance is.                                             ]
  # - (AR) model does not directly handle 'trend'.


  # INFO:[ ARIMA | Analogy ]--------------------------------------------------
  # - 'ARIMA' is like a smart friend who remembers 'past patterns (past
  #   observations<AR>)', notices 'trends', and learns from 'previous mistakes
  #   (errors)<MA>'

  > [ How to find number of Past Observation `p` ]
    1|> AutoRegression (AR) Model
        - We simply use the 'partial Autocorrelation function' to find the 
          number of 'past observed time series' to use linearly in the question 
    2|> like rebut or bruth forece let your model goes over all order and gives
        you the best parameters that gives the best fitting

  > [ How to find the number past errors `q` ]
    1|> Moving Average (MA)
    2|> like rebut or bruth forece let your model goes over all order and gives
        you the best parameters that gives the best fitting (using AIC OR BIC)

    # NOTE:> the 2 method is the simpliest one.

  > [ How to define the `d` order of Differencing ]
    - the order 'd' as this can be verified by carrying out a statistical test 
      for stationarity. The most popular one is the 'Augmented'
      'Dickey-Fuller (ADF)', where the null hypothesis is that time series is
      not stationary (differencing -> check stationarity (if not) ->
      differencing -> check stationarity ...)

  > [ How to do estimation (find {O1, O2, ..., Oq}) ]
    - As the stationary data elogn to some distribution (frequently the normal
      distribution), we can estimate the coefficients using Maximum likelihood  
      Estimation (MLE).
      . MLE: deduces the optimal values of the coefficients that produce the  
        highest probabilty of obtaining that data.
      . MLE for normally distributed data, is the same result as carrying
        ordinary least squares, therefore least squares is also frequently used
        for this exact reason.

  > [ ARIMA Model in Python ]
    1|> differencing the data to stabilize the mean (trend)
    2|> transform time series using (box-cox || log) to stabilize seasonality
        (variance)  
    3|> apply PACF and ACF to find:
        . 'p': number of orders (past time series values used by (AR) model)
        . 'q': ACF to find the number 'errors' used by (MA) model. 
    4|> Train the ARIMA Model 
        model = ARIMA(train['feature'], order=(p, d, q)).fit()
    5|> make forecasts and inverse transformation using (inv_boxcox()) 

# CONCLUSION:-------------------------------------------------------------------
# - `ARIMA` is classical model that covers around 80% from the concept of time 
#   series, some concept here are used for 'modern time series forecasting' even
#   in the deep learning, they are some concepts here that are very important. 
# - There some limitation for 'ARIMA' model 
#   . It does not handle 'seasonality'
#   . It does not include external factors
# - SARIMA = ARIMA + Seasonality
# - SARIMAX = ARIMA + Seasonality + External factors 

===[ SARIMA Model ]===
* This is the seasonal version of the 'ARIMA Model', So 'SARIMA' is an extension
  of the regular ARIMA model that adds as seasonality component to the model,
  this allows us to better capture seasonal affects that the regular ARIMA model 
  does not permit.
  - The classic ARIMA model has three componenets: Autoregressive, Integrated
    (differencing), and Moving-Average, These are then linearly combined to form
    the model

    [ Y'(t) = C + (AR) + (MA) ]

    . Y': differenced time series, the number of differencing applied is noted
          as "d". 

  # INFO:[ My Analysis ]------------------------------------------------------- 
  # - As you can see ARIMA Model does not handle 'seasonality', because we
  #   make the data stationary before training the model, that is why SARIMA 
  #   extension comes up.
  # - SARIMA Model adds the seasonality componenets, but how ? 
  #   . The idea is simple, since the (AR) Model use a number 'p' of historical  
  #     observations, we can simplay add other observation in another season
  #     Y(t - m), Y(t - 2 - m), ..., Y(t - P - m), so we add a number 'P' of 
  #     seasonal historical data. 
  #   . We apply the same idea on (MA), we add a number 'Q' of seasonal error
  #     terms to our (MA) Model

  > [ Order Selection ]
    - After the time series is stationary, we then need to deduce the best
      orders (p, d, q) and (P, D, Q)m for our model 
      . The simplest one to calculate is the season D, and regular differencing
        d, by using 'Augmented Dickey-Fuller (ADF)' statistical test that 
        deduces wheter a time series is stationary or not.
      . (p, q, P, Q) can be computed by analysing the partial autocorrelation
        function (PACF) and (ACF)

  > [ Estimation ]
    - The final step is to compute the corresponding coefficients for these
      orders. The most common method is to use 'Maximum likelihood Estimation'
      '(MLE)'.

  > [ Python Walkthrough ]


===[ SARIMAX Model ]===
* 'SARIMAX' model is just 'SARIMA model' + 'External Factors' 
  - 'X' stands for 'eXogenous variables'. 
  - Exogenous variables allows us to add some new features that represent
    the external 'factors' to improve our 'model accuracy'.

  # NOTE:=======================================================================
  # - No new components is added to SARIMA >>
  # - With SARIMAX you have to be careful of exogenous variables.
  

  > [ SARIMAX in Python ]
  
    from pmdarima import auto_arima
    auto_arima() # the idea of auto_arima is to prevent overfitting


===[ statsmodels package ]===
* `statsmodels` is a 'Python library for statistics and enconometrics'
  - Think of it as:
    . 'scikit-learn' -> focuses on machine learning models (prediction)
    . 'statsmodels' -> focuses on statistical models (inference & explanation)
  - It gives you tools to 'estimate models, run hypothesis tests, and explore'
    'data with statistical rigor'.
    > Here are the big ares of 'Statsmodels':
    1|> Statistical Models
        . Linear regression (OLS, GLS, WLS)
        . Generalized linear models (Poisson, Logistic regression, etc)
        . Time Series models (AR, MA, ARIMA, SARIMA, VAR)
        . Survival models (duration analysis)
    2|> Statistical Tests 
        . t-tests, ANOVA, Chi-square
        . Normality tests (Jarque-Bera, Shapiro-Wilk)
        . Stationarity tests (ADF test)
    3|> Time Series Analysis 
        . Autocorrelation (ACF, PACF)
        . Seasonal decomposition 
        . Forecasting with ARIMA/SARIMA/VAR 
    4|>  Data Exploration
        . Descriptive statistics 
        . correlations and summary tables
        . Influence plots, residual diagnostics.

===[ Time Series Train-Test split ]===
* In 'time series forecasing', splitting data into training and testing is
  usually called 'time series cross-validation'. (or sometimes just
  train-test split for time series).
  - Time series 'cross-validation' are 'different' compared to normal 
    'ML split'
    > In 'regular ML', you can 'randomly shuffle' data and split into
      'training/testing'.
    > In time 'series', you 'cannot shuffle', because order matters (future
      depends on past), So you split based on 'time order':
      >> earlier observations -> 'training' | later ones -> 'testing'
    > The 'test set' should be the 'number of period you expect the model to'
      'predict in practice'.
    > For the 'training data' should have at least 'two whole periods' of data. 

  > [ Common Methods: (for train-test split) ]
    1|> Simple train-test split   
        . Exmaple: use first '80%' for 'training' and '20%' for 'testing'

    2|> Rolling-origin/expanding window evaluation(time series cross-validation)
        . Train on [t1, ..., tk], test on [tk + 1] 
        . then expand: train on [t1, ..., tk + 1], test on [tk + 2] 
        . This 'mimics forecasting' in real life

    3|> Sliding window evaluation 
        . Keep a fixed-size training window that slides forward through time.

    # INFO:---------------------------------------------------------------------
    # - The concept is called 'time series train-test split' and when done     |
    #   systematically across multiple folds, It is called:                    |
    #   >> Time Series Cross-Validation                                        |
    # --------------------------------------------------------------------------

    > [ Time Series Train-Split In Python ]
      - For this we are going to use the 'simple train-split method' 

      - Suppose that we want to predict the next 'sex month' for a 'weekly' 
        time series data.
      # sex months contains 18 week, this our period 
      period = 18
      train = data.loc[:-period, 0] # exclude the last 18 weeks from my dataset
      test = data.loc[-period:, 0]  # test data contains only last 18 weeks  

===[ Measurment of Accuracy (Measurment of Errors) => Metrics ]===
* The way of measuring 'errors' in 'regression' or 'time series forecasting' is
  always the 'same'.
  - Error : Delta between 'actual values (Y-hat)' and the 'predicted values' 

  1|> MAE (Mean Absolute Error)

          sum(| Y-hat - Y|) 
    MAE = ------------------- => average distance (errors)
                  N

  2|> RMSE (Root Mean Squared Error)
                  sum((y-hat - y)^2)
    RMSE = sqrt( -------------------- )  => It handles outliers
                         N

  3|> MAPE (Mean Absolute Persentage Error)
      
            sum(|Y - Y-hat| / Y)
    MAPE = ---------------------- => This one is the most interpretable metric
                    N                IT Gives 'Percentage'.

===[ Time Series Model in Production ]===
* The 'final model' must be 'trained on all the data', and the 'errors' can't
  be measured, but later


===[ Augmented Dicky-Fullter Test ]===
* This is a 'statistical method' to define if a 'serie' is 'stationary' or 'not'.
  - It tests 'contancy' of 'mean', 'variance', and 'covariance' over time.

  from statsmodels.tsa.stattools import adfuller 

  results = adfuller(df['feature'])

  # p-value is the a statistical method to define if distribution is stationary
  # or not, it help us define if the values is statistically significant or not 
  p_value = result[1]

  if p_value < 0.05:
     print("evidence suggest that the time series is stationary")
  else:
     print("evidence suggest that the time series is not stationary")

===[ AIC & BIC ]===
* 'AIC (Akaike Information Criterion)' & 'BIC (Bayesian Information Criterion)'
  are numbers that help you 'choose the best statistical model' when you have
  multiple options.
  - Think of them as 'score' for "your models": They take into accout 2 things
    . 'goodness of fit' (how well the model explains your data) 
    . number of parameters used - 'simplicity' (fewer parameters are better).
  - 'Lower score' -> 'better model'

  - 'AIC' is all about 'balance' (seeks a well-fitting model without execessive
    parameters).
  - 'BIC' penalizes models with more parameters, stronger aversion to model
    complexity.

  - auto.arima 'tests' many ARIMA models (combinition) and calculate 'AIC' and
    'BIC' scores for them.
    |==> The model with 'lowest AIC & BIC = Best Model'

  # INFO:[ CONCLUSION ]=========================================================
  # - Auto.arima explores different p, d, and q combinations, calculating AIC  |
  #   and BIC for each model.                                                  |
  # - They are not perfect but give a really solid start, because we should    |
  #   mostly focus on error (MSE, MAPE ...)                                    |
  # ============================================================================

  > [ Pros ]
    - data driven
    - Penalty for Complexity
    - Flexibility (it can be used with different models)

  > [ Cons ]
    - Relative measures 
    - Busness Focus
    - Information loss

  # INFO:[ Analogy ]============================================================
  # * Imagine you want a backpack for hiking:                                  |
  #   - You could get a huge backpack with lots of compartmenets (fits         |
  #     everything perfectly).                                                 |
  #   - or smaller, simpler backpack (fits most essentials, easier to carry)   | 
  #   - AIC/BIC are like a backpack rating:                                    |
  #     . They 'penalize overly complicated backpacks' (models with too many   |
  #       parameters) even if they "fit" everything.                           |
  #     . They reward 'models that are simple but still work well'.            |
  # ============================================================================
 
  > [ Diff between AIC & BIC ]
    ____________________________________________________________________________
    | Criterion | Penalty for complexity | Use case                            |
    |-----------|------------------------|-------------------------------------|
    | AIC       | Mild penalty           | Good for prediction accuracy;       |
    |           |                        | favors models that explain the data |
    |           |                        | well even if slightly complex       |
    |-----------|------------------------|-------------------------------------|
    | BIC       | Strong penalty         | Good for finding the true underlying|
    |           |                        | model; favors simpler models if data|
    |           |                        | is limited                          |
    |___________|________________________|_____________________________________|

===[ SARIMA Model ]===
* 'SARIMA' model is nothing but a 'ARIMA' model that has a 'seasonal component'
  it adds the extra layer of the 'real world' that we need in order to forecast 
  in heighest accuracy.
  - We need 'sarima' because most of real-world data set has a seasonal patterns
  - Seasonal components or parameters are
    . 'P' = seasonal autoregressive order
    . 'D' = seasonal differencing order
    . 'Q' = seasonal moving average order
    . 'm' = the number of periods in each season

===[ 'Parameter Tuning' or 'hyprparameter Tuning' ]===
* Fine tuning parameter is the key to go from a good forecasting to another one. 
  - fine-tuning paramters means choosing the right settings (lags, differencing,
    seasonality, smoothing weigths, etc) so your model properly capture 'trend'
    + 'seasonality' + 'noise' and produces accurate forecasts.
  - Tuning models allows tailoring the model to your problem and optimizes accuracy

  [ Parameters ] => Run the model => Measure Error => [ Save ] 


      |----------------[ Types of parameters ]----------------------|
      |                                                             |
[ Model Parameters ]                                      [ Hyperparameters ]
- Learned automatically during training             - set by you before training
> Example: In a linear regression model               and these are what you  
  , the 'slop' and 'intercept' are                    'fine-tune'
  parameters the model learns by itself             > Example: optimal order in
                                                      ARIMA models, learning
                                                      rate. 
> [ Example In time Series ]
  >> ARIMA ('p', 'd', 'q'):
    - 'p': how many past observations to use (autoregression).
    - 'd': how many times to difference to remove trend.
    - 'q': how many past forecast errors to include.
    > Fine-tuning = testing different values of ('p', 'd', 'q') to see which
      combination fits best.

  >> SARIMA ('P', 'D', 'Q', 's'):
    - Adds seasonal parameters.
    - Example: ice cream sales with a yearly cycle ‚Üí you fine-tune s=365
      (daily data with annual seasonality).
    - Exponential Smoothing (Œ±, Œ≤, Œ≥):
    - Œ±: how much weight you give to recent values.
    - Œ≤: how much weight you give to the trend.
    - Œ≥: how much weight you give to seasonality.
    > Fine-tuning means adjusting these until your model reacts ‚Äújust right‚Äù
      to changes in the data.

  >> Prophet:
    - Has hyperparameters like seasonality strength, changepoint flexibility.
    > Fine-tuning decides how flexible the model is in detecting sudden changes
      or seasonal patterns.

  -> [ How it‚Äôs done ]
    - Manually: try different parameter values and check error metrics (MAE,
      RMSE, MAPE).
    - Automatically: use search strategies like: 
      . Grid Search
      . Random Search
      . Auto-ARIMA (tests many paramter combincations for you)

Automatically: use search strategies like Grid Search, Random Search, or
Auto-ARIMA that test many parameter combinations for you.
> [ How are they used? ]
  - You start with some values (like a first guess).
  - You train your model.
  - If the results aren‚Äôt good, you adjust the hyperparameters and try again
  - This process is repeated until you find a combination that works best
  >> This is called "hyperparameter tuning" or "fine-tuning"

  > [ Python Walkthrough ]
    from sklearn.model_selection import ParameterGrid

    # Creating a Hyperparameter Grid
    param_grid = {
      'p': [1, 2, 3],
      'd': [0, 1],
      'q': [1, 2, 3],
      'P': [1, 2],
      'D': [0],
      'Q': [1,2]
    }
    
    grid = ParameterGrid(param_grid)
    
    # Creating paramter tuning loop
    for params in grid:
        # build model with 'params'
        # cross validation
        # Evaluation using 'Cross validation' => get the score
        # store the error
