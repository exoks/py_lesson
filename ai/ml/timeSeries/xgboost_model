#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£§‚£¶‚£¥‚£∂‚£æ‚£ø‚£∂‚£∂‚£∂‚£∂‚£¶‚£§‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢†‚°∂‚†ª‚†õ‚†ü‚†ã‚†â‚†Ä‚†à‚†§‚†¥‚†∂‚†∂‚¢æ‚£ø‚£ø‚£ø‚£∑‚£¶‚†Ñ‚†Ä‚†Ä‚†Ä           ìêì  xgboost_model ìêî           
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚†î‚†ã‚†Ä‚†Ä‚†§‚†í‚†í‚¢≤‚†Ä‚†Ä‚†Ä‚¢Ä‚£†‚£§‚£§‚£¨‚£Ω‚£ø‚£ø‚£ø‚£∑‚£Ñ‚†Ä‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚£Ä‚£é‚¢§‚£∂‚£æ‚†Ö‚†Ä‚†Ä‚¢Ä‚°§‚†è‚†Ä‚†Ä‚†Ä‚††‚£Ñ‚£à‚°ô‚†ª‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚†Ä      Eng: oussama <oussama.ezzaou@gmail.com> 
#  ‚¢Ä‚†î‚†â‚†Ä‚†ä‚†ø‚†ø‚£ø‚†Ç‚††‚†¢‚£§‚†§‚£§‚£º‚£ø‚£∂‚£∂‚£§‚£ù‚£ª‚£∑‚£¶‚£ç‚°ª‚£ø‚£ø‚£ø‚£ø‚°Ä                                              
#  ‚¢æ‚£æ‚£Ü‚£§‚£§‚£Ñ‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†â‚¢ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á                                              
#  ‚†Ä‚†à‚¢ã‚¢π‚†ã‚†â‚†ô‚¢¶‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£º‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á       Created: 2025/10/31 14:30:49 by oussama
#  ‚†Ä‚†Ä‚†Ä‚†ë‚†Ä‚†Ä‚†Ä‚†à‚°á‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†á       Updated: 2025/11/02 15:51:08 by oussama
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°á‚†Ä‚†Ä‚¢Ä‚£æ‚£ø‚£ø‚†ø‚†ü‚†õ‚†ã‚†õ‚¢ø‚£ø‚£ø‚†ª‚£ø‚£ø‚£ø‚£ø‚°ø‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚†á‚†Ä‚¢†‚£ø‚£ü‚£≠‚£§‚£∂‚£¶‚£Ñ‚°Ä‚†Ä‚†Ä‚†à‚†ª‚†Ä‚†ò‚£ø‚£ø‚£ø‚†á‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†±‚†§‚†ä‚†Ä‚¢Ä‚£ø‚°ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚£ø‚†è‚†Ä‚†Ä                             ìÜ©‚ôïìÜ™      
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°Ñ‚†Ä‚†Ä‚†Ä‚†ò‚¢ß‚°Ä‚†Ä‚†Ä‚†∏‚£ø‚£ø‚£ø‚†ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ê‚†ã‚†Ä‚†Ä‚†Ä                     ìÑÇ oussama ezzaouìÜÉ  
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚†Ñ‚£Ä‚°Ä‚†∏‚†ì‚†Ä‚†Ä‚†Ä‚††‚†ü‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä                                              

===[ Index ]====================================================================
1|> Decision Trees 
2|> Random Forest
3|> Gradient Boosting
4|> Extreme Gradient Boosting

===[ XGboost Model ]===
# REVISION:---------------------------------------------------------------------
          |-------------------[ Time Series Model ]-------------------|
          |                                                           |
[ Time Series Traditional Model ]                           [ ML Models ]
          |                                                 - Regression with  
  |-------|-------------------------|                         lagged feature 
  |                                 |                       - Tree-based models 
[ Univariate Models ]           [ Multi-variate Model ]       (Random Forest, 
- ARIMA model                   - Vector Autoregression       (XGBoost, trained
- SARIMA (season ARIMA Model)     Model                       on time lagged 
  . It takes in the account                                   features.
    seasanlity                                              - Neural Networks
- Prophet                                                   - Hybrid models  
- Neural Prophet (neural network version of 'prophet')        combine ARIMA +
                                                              ML for residual
                                                              modeling.

[1]|> ===[ Decision Trees ]===

- A 'Decision Tree' is just a 'series of `if-then` rules' that split data into
  smaller and smaller groups until each group is as `pure` (similar) as possible
  
  # INFO:[ Analogy ]------------------------------------------------------------
  # - Think of it like how a person makes decisions step by step:
  #   . "Is it raining?" -> is yes, take an umbrella
  #   . if no -> "Is it cloudy" -> Maybe bring a light jacket
  # -> That's exactly how a `Decision Tree` works, it keeps asking questions
  #   that best separate the data.

  - A Decision Tree looks like this:

               [Root Node: Is temperature > 25¬∞C?]            | Root Node
                        /                     \
                     Yes                       No
            [Is humidity > 60%?]         [Is windy?]          | Decision Nodes
                /         \                 /      \
             Yes          No            Yes        No
           "No"          "Yes"        "No"        "Yes"       | Leafs Nodes

      - 'Root Node': The first and most important question 
      - 'Branches': possible answers ('yes', 'No', '< value', '> value') 
      - 'Leaves': final decisions or predictions

  # QUESTION:[ What the tree is trying to do? ]--------------------------------|
  # - At each step, the tree tries to ask the 'most informative quesiton'      |
  #   possible, the one that 'best split' the data into groups that are more   | 
  #   consistent (or 'pure') in terms of the target variable.                  |
  # - Example:                                                                 |
  #   If we are predicting whether someone will 'buy ice cream', a good first  |
  #   question might be 'is the temperature > 25C?' because that seperates     | 
  #   'likely yes' vs 'likely no' quite well.                                  |
  # QUESTION:[ How the tree chooses questions? (splits) ]----------------------|
  # The algorithm test every feature (e.g: temerature, humidity, age, etc.) and|
  # finds the 'split that gives the biggest improvement' in purity.            |
  # - Fore Classification:                                                     |
  #   . [ Gini Impurity ] - measures how mixed a group is.                     |
  #   . [ Entropy (Information Gain) ] - measures disorder (borrowed from      |
  #     infromation theory).                                                   |
  # - For regression:                                                          |
  #   . Uses 'variance reduction' (how much the split reduces the spread of    |
  #     values).                                                               |
  # > The tree chooses the question that makes the children nodes more 'pure'  |
  #   than the parent node.                                                    |
  # QUESTION:[ How the tree grows? ]-------------------------------------------|
  # - 'It keeps splitting' the data recursively, each time choosing the best   | 
  #   question.                                                                |
  # - It 'stops' when:                                                         |
  #   . The group is pure enough (e.g: all `yes` or all `No`) or               |
  #   . The tree reaches a max depth, or                                       |
  #   . There's too little data left to split meaningfully.                    | 
  # ---------------------------------------------------------------------------|

# ===[ NOTES ]===
- Decision Tree is a 'supervised machine learning algorithm' used for both 
  'classification' and 'regression' tasks.
  - It works by 'splitting data into branches' based on 'conditions of the'
    'input features'
  > essentially forming a 'tree-like structure of if-then rules'.

> [ Linear Regression & Logistic regression VS Decision tree ]===
  _____________________________________________________________________________
  | Aspect        | Linear Regression | Logistic Regression | Decision Tree   |
  |---------------|-------------------|---------------------|-----------------|
  | Type of Model | Parametric        | Parametric          | Non-parametric  |
  |---------------|-------------------|---------------------|-----------------|
  | Prediction    | Linear mathematical Logistic (sigmoid)  | Hierarchical    |
  | Basis         | equation          | function            | if‚Äìthen rules   |
  |---------------|-------------------|---------------------|-----------------|
  | Assumes       | Linear            | Linear in log-odds  | Non-linear and  |
  | Relationships | (straight-line)   |                     | flexible        | 
  | Are           |                   |                     |                 |
  |---------------|-------------------|---------------------|-----------------|
  | Interpret-    | Coefficients show | Coefficients show   | Very intuitive  |
  | ability       | feature impact    | impact on log-odds  | (visual,        | 
  |               |                   |                     |     rule-based) |
  |---------------|-------------------|---------------------|-----------------|
  | Data Handling | Requires scaling, | Similar to linear   | Handles both    | 
  |               |                   |                     | numeric &       | 
  |               | handles numeric   | regression          | categorical data| 
  |               | data well         |                     | easily          |
  |---------------|-------------------|---------------------|-----------------|
  | Overfitting   | Moderate          | Moderate            | High (unless    |
  |     Risk      |                   |                     | pruned or       |
  |               |                   |                     | regularized)    |
  |_______________|___________________|_____________________|_________________|

  - 'Regression models': (linear, logistic) learn 'mathematical equations' that
    fit data:
            [   y = w1 * x1 + w2 * x2 + ... + b   ]
    . They find the 'best-fitting line (or curve)' by optimizing coefficients 
      to minimize error.
  - 'Decision trees', on the other hand, do 'no equation-fitting', they
    repeatedly 'partition the data' based on feature thresholds that maximize
    '<purity>' (using metrics like 'Gini impurity' or 'information gain').
  >> The final prediction is based on 'majority vote (classification)' or
     'average value (regression)' of the samples in a leaf node
 
> [ Strenghts of Decision Trees ]
  - Easy To 'visualize and interpret' ('I can see the rules!')
  - Can handle 'numeric and categorical' data
  - Requires 'little preprocessing' (no scaling or normalization)
  - works for both 'classification and regression'.
  - can model 'nonlinear relationships' easily

# NOTE:------------------------------------------------------------------------
# - All boosting and bagging methods (including Random Forest and XGBoost) are
#   build on decision trees.

===[ Decision Tree Learning ]===
* There are 'two main questions' must be asked before building any decision tree:
1|> How to choose what feature to split on at each node (root/decision node) ?
      - The general rule says: "shoose the feature that maximize the purity"
        "or minimize the umpurity?"

  > [ Purity & Impurity ]
    - When a decision tree 'splits' data, it tries to create groups (branches)
      that are as 'pure' as possible meaning, the examples inside each group are
      'mostly of one kind'.
    
    # INFO:[ ANALOGY ]==========================================================
    # > Imagine you want to group similar items together, all apples in one
    #   basket, all oranges in another.
    #   - if a basket has only apples, it's 'pure'
    #   - if it has a mix of apples and orages, it's impure.
    # >> The goal of the decision tree is 'to find questions (splits)" that make
    #    these groups as pure as possible.
    
    - At each step, the decision tree asks:
        [ Whic feature (question) best splits my data into groups that are more
          a like inside themselves? ]
    - If the split results in:
      . Groups with mostly one label -> 'high purity'
      . Groups with mixed lables 'low purity'

    > For example:
      - 'Question': is the fruit color = red ?
        . Yes -> 90% apples 
        . No  -> 80% oranges
        -> Great! That's a pretty pure split.

    # NOTE:[ VERY IMPORTANT ]===================================================
    # - Purity tells the decision tree 'how good its splits are'
    # - The tree wants each final leaf (end node) to be as pure as possible
    #   because:
    #   . Pure leaves -> clear, confident predictions
    #   . Impure leaves -> uncertain predictions (the tree is not sure) 
    # - In training, the tree measures impurity using math (like 'Gini impurity'
    #   or 'entropy'), but conceptually, it's all about 'how mixed' or 'cleanly'
    #   'separated' the data is.

2|> When do you stop splitting?
  - When a node is 100% one class
  - When splitting a node will result in the tree exceeding a maximum depth
  - When improvements in 'purity score' are 'below a threshold' 
  - When 'number of examples in' a node is below a 'threshold'

===[ Measuring Purity: Entropy & Information Gain ]============================
# REVISION:---------------------------------------------------------------
# - Purity is about how similar the items in a group are after a split 
#   . A 'pure' group means all items inside it belong to the same category
#   . An 'impure' group has a mix
# > So the goal from decision tree is to keep splitting the data until 
#   the groups are as pure as possible.


                |===[ How to Mesaure Purity ]===|
                |                               |
      [ Gini Impurity ]                     [ Entropy ]
- Think of it as the chance           - 'Entropy', Similar idea, but  
  of picking two different              it measures disorder 
  items from the same group             . All one type = low disorder(pure)

  # NOTE:=================================================================
  # - Don't worry about the math, both are just ways to score how mixed or 
  #   unmixed a group is. 
  # - when building a decistion tree, the algorithm looks for the question 
  #   (or 'split') that makes the resulting groups 'more pure' than before


===[ Entropy ]===
* 'Entropy' is just a way to 'measure how mixed or uncertain' a group of data is
  - If a group has 'only one kind of thing', its "pure -> low entropy (or zero)"
  - If it has 'a mix of different kinds', its 'impure -> high entropy'

  # INFO:[ Uncertain ]==========================================================
  # > `Uncertain`: (adj), not knowing what to do or believe, or not able to
  #                decide about something.
  # ============================================================================

# INFO:[ Analogy ]===============================
# * Think of entropy as a 'messiness' score:    |
#   . 'More mixed' = more messy = higher entropy|
#   . 'More uniform' = cleaner = lower entropy  |
# ===============================================

- The basic fomula for Entropy: 

    Entropy = - Sum (pi * log2(pi))

    . 'pi': the proportion (probability) of each class in the group.
    . 

> [ Why using log base 2 ]
  - Entropy comes from infromation theory The idea of entropy comes from
    "Claude Shannon's infromation Theory" (1948) 
  - They wanted a way to measure how much 'information' or 'uncertainty' is in a
    message or dataset. 
  - In information theory, the 'amount of information' in a event depends on how
    surprising it is. 

    > if something is 'very predictable', it carries 'little information'.
      ex: "the sun rieses in the east" -> not surprising
    > if something is 'unlikely', it carries 'more information' 
      ex: "it will snow in the desert today" -> surprising!

  Shannon defined 'information' as:

      [ Information(x) = - log2(p(x)) ]

  - That's why "logarithms" come into play, they translate probabilities
    (like 0,5, 0,25, etc) into "information content".

# QUESTION:[ Why specifically log base 2 ]======================================
# - Because we measure information in 'bits', the basic unit of digital
#   information. 
#   . Each 'bit represents' a (yes/no | 1 or 0) question, like flipping a coin
#     or answering a binray question.
# - So using 'log base 2' means we are measuring:
#   "How many binary questions (yes/no decisions) would it take to identify"
#   "the correct class?"
# - For Example:
#   . if p = 0.5, then -log2(0.5) = 1bit -> one yes/no question
#   . if p = 0.25, then -log2(0.25) = 2 bits -> two questions (more uncertainty)
# > That's why 'base 2' makes sense, decision trees make binray decisions at
# each step (`yes` or `no`), just like bits!.
# INFO:[ EXAMPLE ]==============================================================
# - Imagine guessing what fruit someone picked:
# - If there are 2 equally likely fruits (apple or orange), it takes about 1
#   yes/no question (‚ÄúIs it an apple?‚Äù).
# - If there are 4 equally likely fruits (apple, orange, banana, mango), it
#   might take about 2 yes/no questions (‚ÄúIs it an apple?‚Äù ‚Üí No ‚Üí ‚ÄúIs it an
#   orange?‚Äù ‚Üí etc.)
# - So the log base 2 gives a measure of how many yes/no questions are needed,
#   which fits perfectly with how decision trees split data!
# ____________________________________________________________________________
# | Concept     | Meaning                                                    |
# |-------------|------------------------------------------------------------|
# | Logarithm   | Turns probabilities into ‚Äúinformation content.‚Äù            |
# | Base 2      | Measures that information in bits (yes/no decisions).      |
# | Why use it? | Because decision trees are made of binary splits,          |
# |             | it‚Äôs a natural match!                                      |
# |_____________|____________________________________________________________|
# - So, we use log‚ÇÇ because entropy is all about measuring uncertainty in bits,
#   and decision trees reduce that uncertainty step by step ‚Äî like answering a
#   series of yes/no questions.
# - Entropy measures how missness the group after splitting, the general formula 
#     [ entropy = -sum(log2(pi)) ]
# - log2(pi) measures the uncertainty in a group (or how many questions/decisions
#   of yes/no)
# ==============================================================================

[2]|> ===[ Random Forest ]===

[3]|> ===[ Gradient Boosting ]===

[4]|> ===[ Extreme Gradient Boosting ]===
