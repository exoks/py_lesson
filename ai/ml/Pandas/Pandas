#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£§‚£¶‚£¥‚£∂‚£æ‚£ø‚£∂‚£∂‚£∂‚£∂‚£¶‚£§‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢†‚°∂‚†ª‚†õ‚†ü‚†ã‚†â‚†Ä‚†à‚†§‚†¥‚†∂‚†∂‚¢æ‚£ø‚£ø‚£ø‚£∑‚£¶‚†Ñ‚†Ä‚†Ä‚†Ä                  ìêì  Pandas ìêî           
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚†î‚†ã‚†Ä‚†Ä‚†§‚†í‚†í‚¢≤‚†Ä‚†Ä‚†Ä‚¢Ä‚£†‚£§‚£§‚£¨‚£Ω‚£ø‚£ø‚£ø‚£∑‚£Ñ‚†Ä‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚£Ä‚£é‚¢§‚£∂‚£æ‚†Ö‚†Ä‚†Ä‚¢Ä‚°§‚†è‚†Ä‚†Ä‚†Ä‚††‚£Ñ‚£à‚°ô‚†ª‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚†Ä       Dev: oezzaou <oussama.ezzaou@gmail.com> 
#  ‚¢Ä‚†î‚†â‚†Ä‚†ä‚†ø‚†ø‚£ø‚†Ç‚††‚†¢‚£§‚†§‚£§‚£º‚£ø‚£∂‚£∂‚£§‚£ù‚£ª‚£∑‚£¶‚£ç‚°ª‚£ø‚£ø‚£ø‚£ø‚°Ä                                              
#  ‚¢æ‚£æ‚£Ü‚£§‚£§‚£Ñ‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†â‚¢ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á                                              
#  ‚†Ä‚†à‚¢ã‚¢π‚†ã‚†â‚†ô‚¢¶‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£º‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á       Created: 2025/06/20 12:43:36 by oezzaou
#  ‚†Ä‚†Ä‚†Ä‚†ë‚†Ä‚†Ä‚†Ä‚†à‚°á‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†á       Updated: 2025/07/07 23:06:42 by oezzaou
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°á‚†Ä‚†Ä‚¢Ä‚£æ‚£ø‚£ø‚†ø‚†ü‚†õ‚†ã‚†õ‚¢ø‚£ø‚£ø‚†ª‚£ø‚£ø‚£ø‚£ø‚°ø‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚†á‚†Ä‚¢†‚£ø‚£ü‚£≠‚£§‚£∂‚£¶‚£Ñ‚°Ä‚†Ä‚†Ä‚†à‚†ª‚†Ä‚†ò‚£ø‚£ø‚£ø‚†á‚†Ä                                              
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†±‚†§‚†ä‚†Ä‚¢Ä‚£ø‚°ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚£ø‚†è‚†Ä‚†Ä                             ìÜ©‚ôïìÜ™      
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°Ñ‚†Ä‚†Ä‚†Ä‚†ò‚¢ß‚°Ä‚†Ä‚†Ä‚†∏‚£ø‚£ø‚£ø‚†ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ê‚†ã‚†Ä‚†Ä‚†Ä                     ìÑÇ oussama ezzaouìÜÉ  
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚†Ñ‚£Ä‚°Ä‚†∏‚†ì‚†Ä‚†Ä‚†Ä‚††‚†ü‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä                                              

===[ Index: ]===================================================================
1|> Data Sience Live Cycle
    . Data Collection 
    . Data Cleaning
    . EDA (Exploratry Data Analysis)
    . Data Transformation (Encoding & Feature Scaling)
    . Feature Engineering 
2|> Pandas General Infos 
3|> Reading Data Files 
4|> Data Manipulations 
    . Label Managment/Index Manipulation  
    . Filtering (Conditional Selection) & Ordering
      - Filtering: 'Boolean Indexing', '.isin()', '.filter()', '.str.contains()' 
      - Ordering: '.sort_values()', '.sort_index()'
    . Indexing: Selection ('loc[]' and 'iloc[]')
      - loc[]: 'Label-Based Selection'
      - iloc[]: 'Index-Based Selection'
    . Grouping & Aggregation
      - Grouping: 'groupby()'
      - Aggregation: (Statistics Operations) 'mean()', 'std()', 'max()', etc 
    . Maping: '.map()', '.apply' and '.replace()'
    . Combining Operations
      - '.merge()'  => Column-Based Joining / Relational Joins (SQL Style)
      - '.join()'   => Index-Based Joining 
      - '.contat()' => DataFrames Stacking
5|> Data Visualization '.plot()'
6|> Data Understading 
7|> Data Cleaning
8|> Exploratry Data Analysis (EDA)

===[ Data Science Live Cycle: ]=================================================
* In 'Data Science before training a model', The data typically goes through
  'serveral key stages' in a pipeline, These steps are crucial for ensuring the
  model receives 'clean', 'structured', and 'meaningful data':

             ||==> Identify Problem => Data mining==||
             ||                                     ||
Data visualization          Data Sience        Data Cleaning
             ||             Live Cycle              ||
Predective modeling                            EDA (Data Exploration Analysis)
             ||                                or Exploratry Data Analysis 
             ||                                     ||
             ||========Feature engineering <========|| 

1|> [ Data Collection ]
    * 'Gather' raw data from various sources:
        - Databases (SQL, NoSQL)
        - APIs
        - CSV/Excel files
        - Web scraping
        - Sensors, logs, etc.

2|> [ Data Cleaning ]
    * 'Fix' or 'remove' 'corrupted', 'duplicate', or 'missing data':
        1. 'Understand' the data (basic dataset check)
        2. 'Remove duplicates'
        3. Handle 'missing values' (e.g., fill, drop, interpolate)
        4. 'Fix inconsistent formats' (e.g., capitalization, date formats)

3|> [ Exploratory Data Analysis (EDA) ]
    * 'Analyze' and 'visualize' data to uncover insights:
        - Summary statistics (mean, median, std)
        - Histograms, boxplots, scatter plots
        - Correlation matrix
        - Identify patterns, trends, and anomalies
        - 'Detect outliers'

4|> [ Data Transformation (Preprocessing) ]
    * Convert data into a suitable format for modeling:
        - 'Encoding categorical' variables (e.g., one-hot, label encoding)
        - 'Feature scaling' (e.g., normalization, standardization)
        - Handle 'outliers' or reduce 'noise'
    > [ Feature Engineering ]
    * 'Create' new features or transform existing ones to improve model
      performance:
        - Polynomial features
        - Interaction terms
        - Log transformations
        - Aggregated features (e.g., group-based mean, sum)

5|> [ Data Splitting: ]
    * Split data into subsets: 
        - 'Training set': to train the model
        - 'Validation set': to tune parameters (optional)
        - 'Test set': to evaluate model performance

# INFO:-------------------------------------------------------------------------
# - As you can see the boundaries between these stages are not always strict:
# * Case: 1 
#   Data Collection => Data Cleaning  => modeling
# * Case: 2 
#   Data Collection => Data Cleaning (early step) => Data Transforamtions
#   (main stage of preprocessing EDA) => Feature engineering (advanced
#   preprocessing) 
# * Case: 3
#   Data Collection => Data Cleaning => EDA => data Transformation (feature
#   engineering is part from data Transformation) => modeling  
# ...

===[ Summary: ]===

  Data Collection              
       ‚Üì
  Data Cleaning 
       ‚Üì
Exploratory Data Analysis (EDA)
       ‚Üì
  Data Transformation
(Feature scaling & Encoding) 
       ‚Üì 
  Feature Engineering 
       ‚Üì
  Model Training

================================================================================

===[ Pandas: ]==================================================================
* 'Pandas' is library 'built on top of Numpy', Pandas is optimized for 'data'
  'manipulation and analysis', especially 'tabular (row/column)' or
  'labeled data', Pandas is used:
    - Data cleaning and preprocessing
    - Data transformation and reshaping
    - Exploratory Data Analysis (EDA) 
    - Time series analysis
    - Importing/exporting real-world data (CSV, Excel, SQL, etc) 
    # NOTE:---------------------------------------------------------------------
    # - Pandas is not a 'replacement' for Numpy, It 'extends' it.
    # - It brings 'structure', 'labels', and 'data-aware operations' that are
    #   crucial for real-world data analysis 
    # - 'ndarray' is great for 'nemerical computation'; Pandas is optimized for
    #   'data manipulation', 'cleaning', and exploration. 
  - The term 'Pandas' is term derived from 'Panel Data', a term used in
    'econometrices' to describe 'multidimensional structured data' 
      . 'Panel Data' = data that involves 'observations over multiple time'
        'periods' for the same individuals (e.g time series + cross-sectional
         data)
      . The 'library name' reflects its focus on providing powerful and flexible 
        'data structures' for working with such 'complex structured datasets'
  # CONCLUSION:-----------------------------------------------------------------
  # while 'Pandas' might sound like the animal üêº, it actually has a technical
  # origin rooted in 'data analysis'.

# QUESTION:[ Why learn Pandas ? ]===============================================
# 1|> Higher-level Abstraction: Pandas provides an intuitive way to work with
#     structured data, eliminating the need for complex loops and index tracking
# 2|> Faster Development: Cleaner and shorter code for common data analysis
#     tasks
# 3|> Real-World Data Ready: Most real-word data is 'tabular', with missing
#     values, labels, and mixed data types. (Pandas handles this gracefully)
# 4|> Seamless Integration: Works well with other data science tools, (like
#     Numpy, Matplotlib, Scikit-learn, etc) 
# >> [ Numpy VS Pandas ]
# ______________________________________________________________________________ 
# | Aspect            | NumPy                 | Pandas                         |
# |-------------------|-----------------------|--------------------------------|
# | Focus             | Numerical computation | Data manipulation and analysis |
# | Data Type Support | Homogeneous arrays    | Heterogeneous tables           |
# | Primary Structure | ndarray               | Series and DataFrame           |
# | Use Case          | Scientific computing  | Real-world data analysis       |
# |-Learning Need     |-Essential for         |-Essential for high-level data  | 
# |                   | low-level operations  | analysis                       | 
# |___________________|_______________________|________________________________|
#
# TIP:==========================================================================
# - Use NumPy for efficient computation and Pandas for efficient data handling.
#   In most data science projects, you'll use both, but Pandas is your go-to
#   for structured, real-world datasets.

===[ The Core Motivation behind Pandas ]===
- The 'main goal' of 'creating pandas' was to add 'labels and metadata' to 
  to numerical data structures (like NumPy arrays), enabling more 'flexible' and
  'human-readable' data analysis workflows.
=> 'Labels' (especially column lables in a 'DataFrame') typically represent
   'input feature' in machine learning and data analysis.
=> [ Terminology: ]
  > 'Columns' -> represent 'features' (with column labels)
  > 'Rows'    -> represent 'records/observations/samples' (with row labels
    or index) 

# QUESTION:[ Why Labels Matter? ]-----------------------------------------------
# 1|> Contextual Understanding
#     - Labels like column names ("age", "price") and row indices ("user_001",
#       "2023-01-01") provide semantic meaning to raw data.
# 2|> Ease of Access & Manipulation
#     - Instead of arr[0, 1], you can do df.loc["user_001", "price"].
#     - Enables powerful slicing, filtering, and grouping based on names, not
#       just positions.
# 3|> Automatic Alignment
#     - Labeled operations (e.g., combining datasets with different indices) are
#       handled automatically.
#     - NumPy can't align data across mismatched indices ‚Äî Pandas can.
# 4|> Missing Data Support
#     - Real-world datasets often have incomplete entries ‚Äî Pandas uses NaN,
#       .isna(), .fillna(), etc., to manage this elegantly.
# QUESTION:[ How Pandas Built on Numpy ?]---------------------------------------
# - Uses 'NumPy (ndarray) under the hood' for fast computation 
# - Wraps it with: 
#   . `Index`: to assign labels
#   . `Series`: labeled 1D array 
#   . `DataFrame`: labeled 2D table 
# - Adds high-level data manipulation capabilities.
# CONCLUSION:===================================================================
# - multiple Series (each representing an input feature) can be combined into a
#   DataFrame ‚Äî a 2D data matrix ‚Äî to visualize and analyze both the features
#   (columns) and the cases (rows) easily.
# - DataFrame object represent my matrix table that i have learnt in 
#   'math_statistics'.

===[ Series ]===
* A Series is essentially a 'one-dimensional labeled array' than can hold any
  data type

--> [ Internal Components: (Attribues) ]
    - 'values': Stores the actual data as a 'NumPy''ndarray' (or sometimes
                'ExtensionArray', like for 'pd.StringDtype', 'Categorical', etc)
    - 'index': Stores 'labels' for each element (row labels). It is an
               'instance' of the 'Index' class. 
    - 'dtype': Stores the data type of the elements 

--> [ Examples: ]
    Import pandas as pd

    s = pd.Series([10, 20, 30], index=["a", "b", "c"])
    # values -> array([10, 20, 30]) 
    # index  -> Index(['a', 'b', c]) => Labels

# CONCLUSION:-------------------------------------------------------------------
# - A series is, in essence, a single column of a DataFrame, So you can assign
#   row labels to the Series the same way as before, using an index parameter,
#   However, a Series does not have a column name, it only has one overall name. 

===[ DataFrame: ]===
* A 'DataFrame' is a 'two-dimensional labeled data structure' (like a table or
  spreadsheet).

# INFO:[ Origin Of the Term 'DataFrame' ]=======================================
# - The name `DataFrame` comes from the 'R programming language' which used it
#   to describe a 'tabular data structure' that is: 
#     . 2-dimensional (like a matrix)
#     . Has 'Labeled axes' (rows and columns)
#     . Can store 'heterogeneous data types' (different types in different
#     columns) 
# - 'Wes Mckinney', the creator of Pandas, was inspired by R when he designed
#   Pandas, He browwed the name and concept of the 'DataFrame'
# QUESTION:[ Why Not `DataMatrix` name for example ? ]--------------------------
# - A 'matrix' is a mathematical concept that implies: 
#   ____________________________________________________________________________
#   | Matrix Characteristic | DataFrame Equivalent | Why Not Enough?           |
#   |-----------------------|----------------------|---------------------------|
#   |-Purely numerical      |-Columns can be, ints |-DataFrames can store      |
#   |                       | strings, etc         | mixed types               |
#   |-Only 2D and           |-DataFrames are 2D    |-Matrices are not flexible | 
#   | homogeneous           | but heterogeneous    | for real-world data       |
#   |-Indexed by position   |-DataFrames are       |-Real data often needs     | 
#   | only                  | labeled              | named columns/index       |
#   |_______________________|______________________|___________________________|
# - 'DataMatrix' would be misleading because:
#  . It suggest only 'numeric, homogeneous' data (like in NumPy)
#  . It ignores 'labels' which are contral to pandas 
#  . It does not reflect the 'flexibility' needed for real-world datasets
#    (think CSV files, Databases, etc)
# - 'DataFrame' make Sense because: 
#  > 'Frame' suggest a 'structure or container', just like a photo frame
#    contains multiple photos.
#  > 'Data' represent the 'contents', actual values, columns rows
#  > Together, 'DataFrame' means: 'A structured, labeled container for'
#    'your data'

--> [ Internal Components: (Attributes) ]
    - '_data': Under the hood, stores data in a 'block manager' which manages
               a dictionary of 'ndarray' or 'ExtensionArray' columns grouped by
               dtype. 
    - 'columns': Labels for columns (an 'Index' object) 
    - 'index': 'Labels' for 'rows' (also an 'Index' object (samples, observation
               cases <term in statistics>)
    - Each column is a 'Series' with the same index

--> [ Example: ]
    Import pandas as pd

    df = pd.DataFrame({
      "feature1": ["oussama", "ezzaou"]
      "feature2": [25, 30]
    }, index=["case1", "case2"])

    # >> Internally:
    # name -> Series(['oussama', 'ezzaou', dtype=object])
    # age -> Series([25, 30], dtype=int)
    # >> Data Matrix (Matrix Table)
    #         _______________________
    #         | Feature1 | Feature2 |
    # |-------|----------|----------|
    # | case1 | oussama  |  25      | 
    # |-------|----------|----------|
    # | case2 | ezzaou   |  30      |
    # |_______|__________|__________|

--> [ Pandas Core Class UML: ]
    +------------------------------------------------+
    |                DataFrame                       |
    +------------------------------------------------+
    | ‚Äì _data: BlockManager                          |
    | ‚Äì index: Index                                 |
    | ‚Äì columns: Index                               |
    +------------------------------------------------+
    | + __init__(...)                                |
    | + locate/iloc/select etc.                      |
    | + groupby(), merge(), pivot(), to_csv(), ...   |
    +------------------------------------------------+
                       ‚ñ≤         ‚ñ≤
                       |         |
         contains      |         | contains
                       |         |
        +----------------+   +----------------+
        |    Series      |   |     Index      |
        +----------------+   +----------------+
        | ‚Äì values: ndarray/ExtensionArray    |
        | ‚Äì index: Index                      |
        | ‚Äì name: Optional[str]               |
        +----------------+   +----------------+
        | + __init__(...)                     |
        | + arithmetic methods (add, mean, ..)|
        | + isna(), plot(), ‚Ä¶                 |
        +----------------+   +----------------+


===[ Reading Data Files: ]======================================================
* Being able to create a DataFrame or Series by hand is handy. But, most of
  the time, we won't actually be creating our own data by hand. Instead,
  we will be working with data that alreasdy exist
  - Data can be stored in any of a number of different forms and formats. By far
    the most basic of these is the humbles CSV file, when you open as `CSV` file
    you get something that looks like this: 

   | Product A,Product B,Product C, |
   | 30,21,9,                       |
   | 35,34,1,                       |
   | 41,11,11                       |
   
  - So a CSV file is a table of values separated by commas, Hence the name
    "comma-Seprated Values", or CSV

  1|> "pd.read_csv(r'file_path')" this function is used to read in 'csv files' 
 
    . header=: (optional) It is used for input feature labels 
              > Ex: 'header=None' removes the labels and use index instead
    . names=[inputfeature1, inputfeature2, ...], this function can be used also 
    . index_col=X: you can define the index labels of samples/observations 
                     cases.
                   - X: can be and index of column as 'integer' or
                        'column name' as a string. 

   |> [ df.to_csv('saved_file.csv', index=True) ]: is used to 'save' a DataFrame
      as a 'CSV file'.

    # REVISION:----------------------------------------------------------------
    # - '\', ":",  are still special characthere until you till python to deal
    #   with them as a row string
    # - What makes 'DataFrame' different is having labels 

    # TIP:----------------------------------------------------
    # - df is the standard name of the dataset red from a file

    . sep='separator': defines the separator between input features. 
    . index_col='0': You can define the index col instead of [0, 1, 3, ...]

  2|> 'pandas.read_table(data.txt)' this function is used to read a txt files 

  3|> 'pandas.read_json('data.json')': used to read json file into a DataFrame 

  4|> 'pandas.read_excel(data.xlsx, sheet_name='')': used for excel files


  # INFO:-----------------------------------------------------------------------
  # - pd.set_option('display.max.rows', nbr/None): is used to display a nbr of
  #   rows
  # - pd.set_option('display.max_columns', nbr/None) is used to display a nbr
  #   of columns (None to display all columns)

===[ Data Manipulation: ]=======================================================

===[ Data Manipulation: Index Manipulation/Label Management ]===
  1|> 'df.set_index()': is used to 'change the index' of a DataFrame to one or
      more existing columns.

      > Ex:
          df.set_index('x'): 'x' is the input feature label

  2|> 'df.reset_index()' is used to 'reset the index' of a 'DataFrame' back to
      the default (0, 1, 2, ...) 
      > [ Use cases: ]
        . Your index is no longer meaningful (ex: after filtering or grouping)
        . You want to convert the index back into a column
        . You want a clean DataFrame with the default integer index.

        >>> [ df.reset_index(level=None, drop=False, inplace=False) ] <<<

        . 'level': (opt) specify index level(s) to reset for 'multiIndex'
        . 'drop': If 'True, do not insert the index' into DataFrame as a column
          . Default is 'False'
        . 'inplace': If 'True', modify the DataFrame in-place. Default is
                     'False' > (simply define if you want to change the object
                     itself or not)

===[ Data Manipulation: Filtering: Conditional Selection ]===
  - 1|> "df[df['x'] > y]": (boolean indexing) returns all the the 'cases/'
        'samples/observations' that have an input feature 'x' greater (`>`) than
        'y'.

    # NOTE:---------------------------------------------------------------------
    # - '>' were used as an example, you CAN USE ANY OTHER COMPARISON operator.
    # - It is almost the same idea in 'NumPy' 
    # - df['x'] > y: is called boolean indexing technique that generates a
    #   boolean mask. 
    # --------------------------------------------------------------------------
  
  2|> "df.isin()": is used to 'filter data' by checking whether each element in
      DataFrame or Series 'exist in a list or another set values'. It returns a
      'Boolean Mask', `True` if the element 'is in' the given list, else `False`

      # NOTE:--------------------------
      # It does generate a boolean mask
    
  3|> "df['x'].str.contains()": method is used to 'check if a string column'
      'contains a certain substring or pattern' (using regex by default)

      # NOTE:--------------------------
      # It does generate a boolean mask

  4|> 'df.filter()': is used to 'select specific rows or columns' based on
      'labels (names)', 'not by values'.

      df.filter(items=[label1, label2, label3, ...], like="regrex", axis=)
        . 'axis=': 0 represent the samples/observation labels 
                   axis=1: represetn the input feature labels
        . 'items=': get a list of items (row labels or column labels)
        . 'like=': like="x", brings all labels that contains 'x' as part from
                   the names. 

  5|> 'isnull()' and 'notnull()': let's you high light values which are
                                  (or are not) empty (NaN).
                                  - Missing Data Selection.

      # NOTE:--------------------------
      # It does generate a boolean mask

  # CONCLUSION:=================================================================
  # - We can do filtering (conditional Selection) using two methods: 
  #   1|> Boolean Indexing: Use Camparison and Logical Operators 
  #   2|> Built-in functions: 'filter()', 'str.contains()', '.isin()'  
  #                           'isnull()', 'notnull()', etc

  > [ Summary: ]
      _______________________________________________________________________
      | Method            | Returns | Uses     | Type of Filtering          |
      |                   | Boolean | Boolean  |                            |
      |                   | Mask    | Indexing |                            |
      |-------------------|---------|----------|----------------------------|
      | `.isin()`         | ‚úÖ Yes  | ‚úÖ Yes   | Value-based filtering      |
      | `.str.contains()` | ‚úÖ Yes  | ‚úÖ Yes   | String pattern filtering   |
      | `.isnull()`       | ‚úÖ Yes  | ‚úÖ Yes   | Missing value filtering    |
      | `.notnull()`      | ‚úÖ Yes  | ‚úÖ Yes   | Non-missing value filtering|
      | `.filter()`       | ‚ùå No   | ‚ùå No    | Label-based (column/row)   |
      |                   |         |          | filtering                  |
      |___________________|_________|__________|____________________________|


  # TIP:[ fd.loc[] & df.iloc[] as filters ]-------------------------------------
  # - fd.loc[] and df.iloc[] are indexer but they can be used as filters if you 
  #   combine it with boolean indexing
  #   > Ex:
  #     df.loc[ fd['x'] > y]
  #     
  #     # fd['x'] > y: is a boolean indexing technique that generates a boolean
  #     # mask that can be used with loc[] or iloc[]
  #
  #     # NOTE:-----------------------------------------------------------------
  #     # - boolean mask tells the loc wich columns or rows to keep
  #     # - You can't use iloc directly with boolean mask. use iloc[] instead
  #     #  TIP: convert boolean mask to positions (integers) to use it with iloc

===[ Data Manipulation: Ordering: ]===
  5|> "df.sort_values(by=['label1', ...], ascending=[True, ...])", Is used to
    'sort a DataFrame or Series by one or more columns (or by its values)'

[df.sort_values(by, axis=0, ascending=True, inplace=False, na_position='last') ]

  6|> 'df.sort_index()': is used to 'sort a DataFrame or Series by its index'
      'labels, not by column values'


===[ Data Manipulation: Indexing ]===
* 'Index': is an object that stores all axis labels for all pandas objects

  1|> "df.loc['x']": is a powerful method for 'label-based selection' in pandas 
                   It allows you to 'access rows and columns by their labels',
                   not by position (use iloc for that)

      >>> [ df.loc[row_label, column_label] ] <<<

      . You can pass single labels, lists, slices or boolean masks
        exclusive slicing)

        df.loc[start_row_lebale:end_row_label, start_col_label: end_col_label]

        # NOTE:-------------------------------------- 
        # - end_row_label/end_col_label are Inclusive

  2|> "df.iloc[]": is used for 'label-based selection', It lets you select rows
                   and columns by 'integer location', just like you would with
                   python lists.

    # NOTE:--------------------------------------------------------------------
    # - 'loc' and 'iloc' can be used for filtering, but only when combined with
    #   'boolean indexing'.
    # - end_row_index/end_col_index are exclusive

      # INFO:------------------------------
      # - loc: stands for LOCation
      # - iloc: stands for Integer LOCation

      >> [ df.iloc[start_row_idx: end_row_idx, start_col_idx: end_row_idx] ] <<

  # NOTE:----------------------------------------------------------------------
  # loc[] and iloc[] are Similar techniques to indexing and slicing in NumPy

  . You can pass 'integers', 'lists of integers', 'slices', or 'boolean masks'.

# QUESTION:[ What is the diff between indexing & filtering in pandas ?]========
# Filtering: Select rows based on 'condition (boolean)'
#   |=> result type: Filtered subset
# Indexing: Selects specific rows/columns by position or label
#   |=> Result Type: Subset of original 
# * [ Filtering: Which Rows to keep ]
#   - Filtering uses a 'boolean condition' to keep only certain rows.
#   > Ex:
#     df[df[x] > y]: filtering rows based on a condition 
# * [ Indexing: what to access ]
#  - Indexing is how you access specific parts of the data by 'label (loc)'
#    or 'position (iloc)'
#   > Ex:
#     df.loc['city'] 
#     df.iloc[1]
# CONCLUSION:-------------------------------------------------------------------
# * Indexing = direct 'selection' (by label or position)
# * Filtering = 'Conditional' selection (based on a rule)
# - 'loc' and 'iloc' are 'indexers', but they can perform 'filtering' when given
#   a 'boolean condition' for the 'row selector'.
# ______________________________________________________________________________
# | Feature    | Filtering                         | Indexing                  |
# |------------|-----------------------------------|---------------------------|
# | Definition | Selects rows based on a condition | Selects rows or           |
# |            |                                   | columns by label/position |
# |------------|-----------------------------------|---------------------------|
# | Syntax     | `df[condition]`                   | `df.loc[]`, `df.iloc[]`,  |
# |            |                                   | `df['col']`               |
# |------------|-----------------------------------|---------------------------|
# | Returns    | A filtered DataFrame              | A DataFrame, Series, or   | 
# |            | (subset of rows)                  | scalar                    |
# |------------|-----------------------------------|---------------------------|
# | Input      | Boolean Series (e.g.,             | Label(s), position(s),    | 
# |            | `df['age'] > 25`)                 | or slices                 |
# |------------|-----------------------------------|---------------------------|
# | Output     | Rows where condition is True      | Specific rows/columns by  |
# | Example    |                                   | name/position             | 
# |------------|-----------------------------------|---------------------------|
# | Purpose    | Keep only rows that match         | Access specific part(s)   |
# |            | condition                         | of data                   |
# |____________|___________________________________|___________________________|

===[ Data Manipulation: Grouping & Aggregation ]===
* 'groupby()' function in Pandas is used to 'split data' into 'groups based'
  '(subgroups)' on 'one or more keys (column values)', 'apply' a function (e.g
  'sum', 'mean', 'count') and then 'combine' the results into a 'new DataFrame',
  This process is known as 'the Split-Apply-Combine' strategy.
  - Instead of 'applying operations' on 'column or rows', you can 'apply' it
    on group 'based on keys (column values)'.

  - 'groupby()' return a special object 'DataFrameGroupBy', It is a
    'Lazy object'

  # NOTE:-----------------------------------------------------------------
  # This is a personal Note, When grouping by any keys/columns, it becomes
  # the index of the DataFrame

  # INFO:> grouping can be classified under a 'data manipulation category' 
  
  > [ Aggregation methods: ]
    _________________________________________________
    | Method     | Description                      |
    |------------|----------------------------------|
    | `.sum()`   | Total per group                  |
    | `.mean()`  | Average per group                |
    | `.count()` | Count non-null entries           |
    | `.min()`   | Minimum per group                |
    | `.max()`   | Maximum per group                |
    | `.size()`  | Total number of rows (incl.NaN)  |
    | `.agg()`   | Custom or multiple aggregations  |
    |____________|__________________________________|

    # INFO:-----------------------------------------------------------------
    # - .size() in the context of aggregation returns the number of rows for 
    # each splited group
    # - It is like telling pandas: 
    # [ "Hey Pandas, split my DataFrame into groups, and tell me how many" ]
    # [ "rows are in each group."                                          ]
    # ERROR:[ size attr & .size() are different ]===========================
    # size: attr return the full size of DataFrame [ rows * cols ]
    # .size(): returns the number of rows for each subgroup
    # ----------------------------------------------------------------------

  |> 'pd.agg()': is used to apply 'one or more aggregation functions' to a
      'DataFrame or Series'

    # INFO:> aggregation can be classifed under 'Data summarization/analysis'

      >> [ df.agg(func, axis=0) ] <<
        . `func`: a function (or list/dict of functions) like 'mean', 'sum', etc
        . `axis=0`: (defautl) apply to 'columns'
        . `axis=1`: (defautl) apply to 'rows'

      > [ Example: ]

      import pandas as pd
      import numpy as np

      df = pd.DataFrame({
        'A': [1, 2, 3, 4],
        'B': [5, 6, 7, 8]
      })
      # Single aggregation on all columns
      df.agg('mean')
      # A    2.5
      # B    6.5
      # Multiple aggregations on all columns
      df.agg(['mean', 'sum'])
      # Different aggregations per column
      df.agg({
          'A': ['min', 'max'],
          'B': ['mean', 'std']
      })

  # INFO:[ Use cases: ]-------------------------------
  # - Custom summary statistics
  # - aggregating different columns differently
  # - grouped aggregations (used with groupby() often)

  |> 'df.describe()': returns 'a quick overview of statistics' for numeric
     columns

    >> [ df.describe(include=None, exclude=None, percentiles=None) ] <<
      . include='all': include non-numeric columns
      . exclude='number': exclude numeric
      . percentiles: specify percentiles to show (e.g. [.25, .5, .75])

    > [ Exmaple ]
      # describing the distribution (center tendency (mode, median, mean),
      # dispersion <range, IQR, std variance, etc)
      df.describe()

      # return example
      #         | A    | B    |
      # |-------|------|------|
      # | count | 4.0  | 4.0  |
      # | mean  | 2.5  | 6.5  |
      # | std   | 1.29 | 1.29 |
      # | min   | 1.0  | 5.0  |
      # | 25%   | 1.75 | 5.75 |
      # | 50%   | 2.5  | 6.5  |
      # | 75%   | 3.25 | 7.25 |
      # | max   | 4.0  | 8.0  |

  # INFO:[ Use cases: ]-------------------------------
  # - Quick look at data distributions 
  # - Exploratory Data Analysis (EDA)

  # INFO:-----------------------------------------------------------------------
  # - Aggregation: (noun) the process of combining things or amounts into a
  #   single group or total:
  #   . ex: Some animals avoid being eaten by aggregation in groups such as
  #         schools or fish
  # - In Context of Pandas means applying a 'summary function' to groups of
  #   data to get a 'single value' per group (like 'sum', 'mean', 'min', 'max'
  #   or 'count')
  # CONCLUSION:-----------------------------------------------------------------
  # - Aggregation is a method to perform a group of statistical operations
  #   (like sum, mean, count, etc.) on a DataFrame or Series.
  # ----------------------------------------------------------------------------

# QUESTION:[ Boolean indexing VS boolean Mask ]---------------------------------
# - 'Boolean indexing' is a way to 'filter data' in a DataFrame or series by
#   using a 'boolean condition' to select rows or columns.
# - 'Boolean mask' is a 'Series or array of True/False values' used to filter 
#   data, it's called a 'mask' because it reveals only the elements where the 
#   condition is true

===[ Data Manipulation: Maping ]===
* A 'map' is a term, borrowed from mathematics for a 'function' that
  'takes on set' of values and 'maps' them to 'another set' of values, In data
  science we often have a need for creating a new representations from an
  (feature engineering) existing data, or make 'feature scaling', 'Maps' are
  what handle this work. 

  1|> 'df[x].map(func)': Apply a function or mapping to 'each element' in a
      'Series' (Usually one column)

      . 'func': is function that accept one element

    # NOTE:-------------------------------------------------
    '.map()': - Element-wise transformation on series 

    > Examples:
      df['gender'] = df['gender'].map({'M': 'Male', 'F': 'Female'})
      df['score'] = df['score'].map(lambda x: x**2)

  2|> 'DataFrame/Serie.apply(func, axis=)': Apply a custom function 'along a'
      'row or column' of a 'DataFrame' or 'Series'. 

      . 'func': accept 'rows or column' based on 'axis value'
      . 'axis=0': apply 'func' on 'columns' 
      . 'axis=1': apply 'func' on 'rows'


    # NOTE:-------------------------------------------------
    '.apply()': - Flexible row/column-size operation 

    > Example:
      # On Series
      [ df['tax'] = df['income'].apply(lambda x: x * 0.2) ]
      # On DataFrame
      df['total'] = df.apply(lambda row: row['price'] * row['quantity'], axis=1)

    # INFO:---------------------------------------------------------------------
    # - .map() and .apply() return a new transformed DataFrame Or Serie, They
    #   modify the original one.

  3|> 'DataFrame/Serie.Replace()': 'Replace specific values' with others in
      'Series' or 'DataFrame'. 

    > Examples:  
      [ df['status'] = df['status'].replace({'P': 'Pending', 'C': 'Complete'}) ]

    # NOTE:---------------------------------------------------
    '.replace()': - Value substituation in Series or DataFrame

  4|> 'df.fillna()': is used to map only `NaN` values. int the context of
                     handle/maping missing data

# TIP:==========================================================================
# |> Use .map() when you‚Äôre dealing with a 'single column' and want
#   'element-wise transformation'
# |> Use .apply() when you need 'row-level logic' or more 'flexibility'.
# |> Use .replace() for straightforward 'substitution' (like correcting
#    categorical values).

> [ Summary: ]
________________________________________________________________________________
| Method       | Scope            | Function Type   | Use Case Example         |
|--------------|------------------|-----------------|--------------------------|
| `.map()`     | Series only      | Element-wise    |-Mapping codes to labels, |
|              |                  |                 | simple transformations   |
| `.apply()`   | Series/DataFrame | Row/col-wise or |-Custom functions across  |
|              |                  | element-wise    | columns/rows             |
| `.replace()` | Series/DataFrame | Value           |-Replace values for       |
|              |                  | substitution    | cleaning or relabeling   |
|______________|__________________|_________________|__________________________|

===[ Data Manipulation: Combining Operations ]===

===[ Mergin: 'pandas.merge()']===
* 'pandas.merge()' is used to 'combine two dataframes' based on one or more
  'keys (columns or indices)', similar to SQL joins

  # TIP:--------------------------------------------------------------------
  # Think of it as a way to 'merge relational tables' based on common fileds

  > [ Use Cases: ]
    . Complex joins (on multiple columns, different column names)
    . SQL-style operations ('inner', 'outer', 'left', 'right')
    . Merging on 'columns' or 'index + columns'
    # CONCLUSION:> COLUMN-BASED JOINS == RELATIONAL JOINS

      |-------------------[ Types of Merges: ]-------------------|
      |                   |                  |                   |
      |                   |                  |                   |
[ Inner Merge ]    [ Left Merge ]     [Right Merge]      [ Outer Merge ]

  1|> [ Inner Merge: (default) ]
      * Only rows with 'matching keys' in both DataFrames, It is an
        'Intersection' between two DataFrames based on join key(s)

      >> [ pd.merge(df1, df2, how='inner', one='key', on='x') ] <<

          . how='type of mergin'
          . one='key': mering based on key
          . on='x': which columns to apply mering on 

  2|> [ Outer Merge: ]
      * 'All Rows' from both, NaNs where there's no match, It is an
        'Union' between two dataFrames based on key(s)
      
      >> [ pd.merge(df1, df2, how='outer', one='key') ] <<

  3|> [ Left Merge: ]
      * All rows from 'left' matching rows from right,
        . Preserve left + Intersection (inner)

      >> [ pd.merge(df1, df2, how='left', one='key') ] <<

  4|> [ Right Merge: ]
      * All rows from 'right' matching rows from right
        . Preserve Right + Intersection (inner)
      
      >> [ pd.merge(df1, df2, how='right', one='key') ] <<

# NOTE:--------------------------------------------------------
# - You can either use pandas.merge(df1, df2) or df1.merge(df2)
# - There is also how='cross', read about it if you need to

===[ Joining: 'pandas.join()']===
* '.join()' method is used to 'combine two DataFrames' based on their 'index'
  or a key column, It is similar to SQL joins but works primarily on 'indexes'
  by default

  > [ Use Cases: ]
    . Simple joins based on 'index'
    . When you want to join two DataFrames quickly using their index
    # CONCLUSION:> INDEX-BASED JOINS

    >> [ DataFrame.join(other, on=None, how='left', lsuffix='',
                                          rsuffix='', sort=False) ] <<

    ________________________________________________________________
    | Feature        | `join()`             | `merge()`            |
    |----------------|----------------------|----------------------|
    |-Joins on       |-Indexes (by default) |-Columns (like SQL)   |
    |                |                      | index or both        |
    |-Supports `on=` |-‚úÖ Yes (for joining  |-‚úÖ Yes               |
    |                | on columns)          |                      |
    |-Default `how`  |-`'left'`             |-`'inner'`            |
    |-More flexible  |-‚ùå Less flexible     |-‚úÖ More powerful and |
    |                |                      | SQL-like             |
    |-Best used for  |-Joining DataFrames   |-Joining on one or    |
    |                | on index             | more columns         |
    |-Simpler syntax |-‚ùå No                |-‚úÖ yes               | 
    |________________|______________________|______________________|
 

===[ Concatenation: 'np.concat()' ]===
* 'pandas.concat()' is used for Stacking DataFrames

  > [ Use Cases: ]
    . 'Combining DataFrames' along a 'particular axis' (vertically or
      horizontally)
    . Use when you want to 'stack' DataFrames (like np.'concatenate')
    . Useful for adding rows or columns 'without needing keys/joins'
    # CONCLUSION:> STACKING DATAFRAMES (Vertical/Horizontal Stacking)

    >> [ pd.concat(objs, axis=0, join='outer', ignore_index=False) ] <<

      . 'objs': list of DataFrames
      . 'axis=': 0 stack rows (default)          |-vertical stack (like
                                                 | np.vstack())
                 1 adds columns (side-by-side)   |-horizontal stack
                                                 | (like np.hstack()) 

      . 'join=inner': stack/concatenate only 'intersection' columns 
                    - 'outer (default)' concatenate all columns 
                    ...

# CONCLUSION:===================================================================
# - `pandas.merge()`: COLUMN-BASED JOINS (RELATIONAL JOINS)
# - `pandas.join()`: INDEX-BASED JOINS  
# - `pandas.concat()`: DATAFRAMES STACKING 
# ______________________________________________________________________________
# | Feature  | `concat()`    | `merge()`                  | `join()`           |
# |----------|---------------|----------------------------|--------------------|
# |-Combines |-Based on axis |-Based on key columns/index |-Based on index (or |
# |          | (rows or      | (like SQL)                 | column using `on`) |
# |          | columns)      |                            |                    |
# |-Handles  |-‚ùå No key     |-‚úÖYes (columns/index)      |‚úÖYes (mostly index)|
# | keys     | matching      |                            |                    |
# |-Best for |-Stacking      |-Relational joins           |-Index-based joins  |
# |          | DataFrames    |                            |                    |
# |-Flexible |-‚úÖ Yes        |-‚ùå No                      |-‚ùå No              |
# | on axis  |               |                            |                    |
# |-SQL-style|-‚ùå No         |-‚úÖ Yes                     |-‚ùå No              |
# | join     |               |                            |                    |
# |__________|_______________|____________________________|____________________|

===[ Pandas Visualization: ]====================================================
# REVISION:---------------------------------------------------------------
# - Graphs and plots are used in describtive statistics and data sience to
#   summarize, describe (distribution and contral tendecy), and understand
#   relationships or correlation between varibales. 

* Pandas has 'built-in support for plotting', and under the hood, it uses  
  'Matplotlib' to generate the visualizations.

  > [ Integration of Matplotlib with Pandas: ] 
    - You can call '.plot()' directly on a 'DataFrame' or 'Serie'
    - By default, this uses 'Matplotlib' as the backend 
    - You can 'customize' the 'plot' using 'Matplotlib functions' after plotting

    >>  [ df.plot(kin='line') ] <<

        . 'kind=line': (default) defines the 'plot type' 
        . 'subplot=True': breaking up or spiltting the 'plot' into 'subplots'
        . "title='x'": used to add a title to the plot
        . "xlables='x'": sets the label (title) for the 'X-axis' 
        . "ylabels='y'": sets the label (title) for the 'Y-axis'

  > [ Plot Types: ] 
    _______________________________________________
    | `kind`      | Plot type                     |
    |-------------|-------------------------------|
    | `'line'`    | Line plot (default)           |
    | `'bar'`     | Vertical bar plot             |
    | `'barh'`    | Horizontal bar plot           |
    | `'hist'`    | Histogram                     |
    | `'box'`     | Box plot (for statistics)     |
    | `'kde'`     | Kernel density estimate       |
    | `'area'`    | Area plot                     |
    | `'pie'`     | Pie chart                     |
    | `'scatter'` | Scatter plot (DataFrame only) |
    | `'hexbin'`  | Hexbin plot (DataFrame only)  |
    |_____________|_______________________________|

  > [ Line Plot: ]

# QUESTION:[ What is the difference btween 'scatter plot' and 'line plot' ]=====
#  'Line Plot': connects data points with lines, typically showing
#   'how variable changes over time' or in response to another continuous
#   variable. 
#   > [ Use cases: ]  
#     . Points are connected
#     . Emphasizers 'trends', 'progression' or 'patterns'
#     . Common in 'time series Data'
# - A 'Scatter plot': displays individual data points as 'dots' on a 2D plan
#   without connecting them, It is used to show the 'realtionship or
#   correlation' between two varibales
#   > [ Use Cases: ]
#     . No lines between points.
#     . Highlights 'distribution', 'clusters' and 'correlation'
#     . Common in 'regression analysis' and 'pattern detection'
# CONCLUSION:-------------------------------------------------------------------
# |> Use a 'line plot' when you are showing data over time (e.g, temperature
#    changes), When the data points have a 'logical or chronological order'.
# |> Use a 'scatter plot' when you want to explore the 'relationship betwee
#    two variables' (e.g, height vs weight) 
# ------------------------------------------------------------------------------
#
  > [ Bar Plot: ]

  # REVISION:-------------------------------------------------------------------
  # Bar Graph and Pie Chart are used to 'summarize a distibution' of categorical
  # variables (level of measurment <nominal>)

      >> [ pandas.plot(kind='bar', stacked=False) ] <<  

          . kind='bar': vertical bar graph 
          . stacked=False: If 'True' the bars will displayed stacked

  # NOTE:> fd.plot.barh() is used to display an horizontal bar

  > [ Scatter Plot: ]

  # REVISION:-------------------------------------------------------------------
  # Scatter plot is used to show the relationship between two
  # numerical/quentitive varibales.

    >> [ df.plot(kind='scatter', x='independent', y='dependent', s=, c=) ]
        . x='x': represent the 'indepent variable' 
        . y='y': represent the 'dependet variable' 
        . s=nbr: represent the size of the plot dots 
        . c='color': the 'color' of the 'dots'

    # NOTE:----------------------------------------------------------
    # Histograms are really good of showing distribution of variables

  > [ Histogram: ]

  # REVISION:-------------------------------------------------------------------
  # A histogram is a type of graph that shows the 'distribution of a dataset',
  # It looks similar to a 'bar graph' but it is used for 'continuous'
  # numerical data rather than categories.

    >> [ df.plot(kind='hist', bins=10) ]

        . "kind='hist'": Specifies a histogram plot
        . 'bins=10' (default): Number of intervals to divide the data into 

  > [ Box Plot: ]

  # REVISION:-------------------------------------------------------------------
  # - 'Box plot' is a powerful way to visualize 'distribution', 'spread', and
  #   'outliers' in dataset
  # - 'box plot' is a graphical summary of data that shows:
  #   . 'The center' (median)
  #   . 'The spread' (range and interquartile range) 
  #   . 'the shape' (symmetry or skewness)
  #   . 'Outliers' 

    >> [ df.plot(kind='box') ]

  > [ Area: ]
    # ERROR:> READ About if you need to 

  > [ Pie Chart: ]

  # REVISION:-------------------------------------------------------------------
  # - Pie Chart are used to 'summarize a distibution' of categorical variables 
  #   (level of measurment <nominal>)
  # - Bar Grpah is used in case of large data 
  # > Purpose:
  #   . Showing 'percentage'
  #   . Highlighting difference in quantity across groups.
  #   . When the goal is to                      
  #   . visualize 'how a whole is'
  #   . 'divided' into parts

    >> [ df.plot(kind='pie', y='y', figsize=(nbr)) ] <<

        . y='y': 'y' is the input feature/column to chart 
        . figsize=(nbr): defines chart size


# TIP:-----------------------------------------
# - You can style visuals (plt.sytle.available)
# - Read About styling when you need to 
# ---------------------------------------------

===[ Data Understanding:  Quick Overview ]======================================
    - 'df.info()': used to get a 'concist summary' of a 'Pandas DataFrame', It
      is very helpful for 'understanding the structure' of your dataset quickly.
    - 'df.head(nbr=5)': used to display the 'nbr' from the 'head' of 'DataFrame' 
    - 'df.tail(nbr=5)': used to display the 'nbr' from the 'tail (end)' of
                        'DataFrame' 
    - 'df.shape': shape of the 'DataFrame (dimensions)' 
    - 'df.describe()': This function generates a 'high-level' summary of the
      attributes of the given column, It is 'type-aware', meaning that its
      output changes based on the data type of the input. The output above
      only makes sense for 'numerical data' for string data here's what we get: 

      >>[ df.'x_column'.describe() ]<<

    > Example Output:

      count:    10000
      unique:   20
      top:      'oussama'
      freq:     3003
    - "df.tester_name.unique()": gives a list of unique values

    - 'df.x.value_counts()': This function gives a count of unique values 

    # NOTE:=====================================================================
    # We can name these functions by Summary Functions (is not an official name)
    # ==========================================================================

===[ Data Cleaning: ]===========================================================
  1|> [ Understand the data: ]
      * Do a 'Basic data check' and use function that 'do not depend on clean'
        'types'
        . df.head()
        . fd.tail()
        . df.colomuns()
        . df.shape
        . df.dtype

      * It will give and idea about the data that you are dealing with:
        . Samples/observations/cases and input features/labels/variables
        . Input Feature info (level of measurment categorical, numerical) 

  2|> [ Remove Duplicates: ]
      - Use drop_duplicates() function to remove duplicates. 
      >> [ df.drop_duplicates() ] <<

  3|> [ Handle Missings: ]
      - Handling missings data means 'detecting', 'fixing', or 'removing'
        'incomplete' or 'null' values in your dataset
      'Remove' or 'Drop' useful 'columns/input features'
      - In pandas, missing values are usually represented as:
        . 'NaN' (Not a number)
        . 'None'
        . Custom strings like "N/A", "null", "?"
      - Detecting Missing Values:
          df.isna()         # return True for NaN
          df.isnull()       # Same as isna()
          df.isna().sum()   # count missing values in each column
      - Common Ways to Handale Missing Data.
        1. Remove missing Data 
           * 'df.dropna()'   # Remove rows with any missing values
           * 'df.dropna(axis=1)' # Remove columns with any missing value
           * df[df['x'].notna()] # Removes rows where a specific column is
                                 # missing
        2. Fill in (Impute) missing Data
           * df.fillna(0)
           * fill with the mean/median/mode of the column
           * Forward-fill (use previous row) # df.fillna(method='ffill')
           * Backward-fill (use next row) # df.fillna(method='bfill')
        3. Replace placeholders with NaN
           * Sometimes missing data is represetned as strings like: "N/A",
             "missing", etc
              - You can convert them to a real 'NaN' 
                df.replace(['N/A', 'null', '?'], pd.NA, inplace=True)
              - Or directly when loading the file:
                pd.read_csv('data.csv', na_values=['N/A', 'null', '?'])

- Strategy depends on:
    _____________________________________________________________________
    | Situation                         | Strategy                      |
    |-----------------------------------|-------------------------------|
    | Data is missing at random         | Impute using mean/median/mode |
    | Few missing rows                  | Drop rows (`dropna()`)        |
    | Missing values mean "none" or "0" | Fill with constant            |
    | Time series                       | Use forward/backward fill     |
    |___________________________________|_______________________________|

- Summary: 
    ________________________________________________________
    | Method                | Description                  |
    |-----------------------|------------------------------|
    | `isna()` / `isnull()` | Detect missing values        |
    | `dropna()`            | Remove missing rows/columns  |
    | `fillna()`            | Replace missing values       |
    | `replace()`           | Turn placeholders into `NaN` |
    |_______________________|______________________________|

  4|> [ Fix Inconsistent Fromats: ]
      - 'Inconsistent formatting' refers to when 'the same kind of data' is
        written in 'different styles or formats' across a dataset.
      - Correcting it is a key part of 'data cleaning', making your data
        'uniform and standardized'

      # INFO:
      # - 'inconsistent' (adj)  ÿ∫Ÿäÿ± ŸÖÿ™ÿ≥ŸÇ
      #   ex: inconsistent person
      
      > [ Examples: ]
       _________________________________________________________________________
       | Problem Type   | Example                          | Corrected         |
       |----------------|----------------------------------|-------------------|
       |-Text case      |-`"USA"`, `"usa"`, `"Usa"`        |-`"USA"`           |
       |-Date format    |-`"01/02/2025"` vs `"2025-02-01"` |-`"2025-02-01"`    |
       |-Currency       |-`"$1,000"` vs `"1000"`           |-`1000.0`          |
       |-Whitespace     |-`"New York "` vs `" New York"`   |-`"New York"`      |
       |-Missing values |-`"N/A"`, `"null"`, `""`, `NaN`   |-`np.nan` or       |
       |                |                                  | consistent `None` |
       |-Column names   |-`"First Name"` vs `"first_name"` |-`"first_name"`    |
       |                |  vs `"firstname"`                |                   |
       |________________|__________________________________|___________________|

      > [ Tasks: ]
        ______________________________________________________________________
        | Task              | Goal                                           |
        |-------------------|------------------------------------------------|
        |-Fix text case     |-`.str.lower()`, `.str.upper()`, `.str.title()` |
        |-Strip spaces      |-`.str.strip()` (spaces and custom characters)  |
        |-Unify date format |-`pd.to_datetime()`                             |
        |-Rename columns    |-`df.columns = [...]` or `df.rename()`          |
        |-Clean symbols     |-`.str.replace()` (e.g., remove `$` or `,`)     |
        |___________________|________________________________________________|

  |> 'df.str.strip()': is a 'string method' used in pandas (and python) to
     'remove whitespace or characters from the beginning and end of strings'

    # INFO:---------------------------------------------------------------------
    # - Stripe (noun): a strip on the surface of something that is different
    #   colour from the surrounding surface:
    #   . The zebra is wild African horse with black and white stripes.

  |> 'df.str.split()': function is used to 'split strings' in a Series
      'into lists or substrings', based on a 'delimiter (separator)' 

    >> [ Series.str.split(pat=None, n=-1, expand=False) ] <<
    
      . 'pat=None': The delimiter to split on (space, comma, ...), Default
        (any white space)
      . 'n=-1': Max number of splits, default -1 => no limit
      . 'expand', If 'True' splits into 'separate columns (DataFrames)',
        otherwise return 'lists'.

    > [ Use Cases: ]
      - Splitting full names into first/last or address, etc
  |> 'df.str.replace()': function is used to 'replace characters or substrings'
     in 'string values of a Series or DataFrame column', It is useful for
     'cleaning textual data (ing inconsistent formatting)'

    >> [ df.str.replace(pat, repl, n=-1, case=None, regex=True) ] <<

    . 'pat': the 'pattern (string or regex)' to replace
    . 'repl': the replacement string
    . 'n=-1': number of replacement to make, -1 is the default => replace all
    . 'case=None': Whether to match case when 'regex=True', default 'None'
      depends on regex flags
    . 'regex=True': Whether 'pat' is regular expression (default: 'True')
  
  |> 'Series.apply()': function is used to 'apply a function to each element'
      (or row/column) of 'Series or DataFrame', It is a 'powerful and flexible'
      'tool' for transforming data. 

      [[ Series.apply(func)                 ]]
      [[ DataFrame.apply(func, axis=0 or 1) ]]
      - 'func': The function to apply.
      - 'axis': Only for DataFrames.
          . 'axis=0': apply to columns (default)

          . 'axis=1': apply to rows

    > [ Use Cases: ]
      - Cleaning or formatting text
      - Custom transformations

===[ Explotary Data Analysis: ]=================================================
* 'EDA' is a 'practical data exploration process' in 'data science' to: 

    >>[ Explore your data 'understand it', 'spot patterns', ]<< 
    >>[ 'identify problems', and 'form hypotheses'          ]<<

- 'EDA' = 'tools from descriptive statistics' + 'visualization' + 'Exploration' 

> [ Analogy: ]
  - 'Statistics is the toolbox'.
  - 'EDA is the process' where you 'open the toolbox', 'try tools' and explore 
    the data to 'understand what is inside'.
# CONCLUSION:> EDA is summarizing and analysing the data

===[ Data Transformation: (Preprocessing) ]=====================================

# NOTE:-------------------------------------------------------------------------
# Feature Scaling, Feature Engineering and Encoding are very well explained in
# the ml_model file > machine learning models theory

# QUESTION:> How to to apply Data Transforamtion in pandas (Data manipulation)
